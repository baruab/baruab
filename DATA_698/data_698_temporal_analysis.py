# -*- coding: utf-8 -*-
"""DATA_698_Temporal_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x8vr3gUPpGIPtkUxivIO8S-HZFNU9iUp
"""



!pip install google-colab geopandas plotnine statsmodels xarray
!pip install pandas google-colab geopandas plotnine statsmodels xarray

!pip install --upgrade pandas
!pip install --upgrade numpy

!pip install --upgrade pandas numpy --no-deps

!pip install --upgrade --force-reinstall numpy

!pip install --upgrade --force-reinstall pandas matplotlib scipy seaborn

!pip show numpy pandas matplotlib scipy seaborn

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
print(f"NumPy version: {np.__version__}")
print(f"Pandas version: {pd.__version__}")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
#to ignore warnings
import warnings
warnings.filterwarnings('ignore')

## In the interest of saving time, calling the API multiple times, as the information is of static nature
##   the generated file with added columns is saved and read directly to save compute time


access_log_location_url = 'https://raw.githubusercontent.com/baruab/baruab/refs/heads/main/DATA_698/tokenized_access_logs_global.csv'

# Force pandas to read all columns as strings initially
df_ac_log = pd.read_csv(access_log_location_url, dtype=str, low_memory=False)
# Add low_memory=False to handle potential memory issues

# Check the data types of each column
print(df_ac_log.dtypes)

# Inspect specific problematic columns for unusual values
# For example, if 'column_name' is suspected:
# print(df_ac_log['column_name'].unique())

# Drop uplicates
df_ac_log = df_ac_log.drop_duplicates()

df_ac_log['Dt'] = df_ac_log['Date'].str.split(' ').str[0]

# Convert the 'date' column to datetime
df_ac_log['date'] = pd.to_datetime(df_ac_log['Dt'])

# Extract the day of the week
df_ac_log['day_of_week'] = df_ac_log['date'].dt.day_name()
df_ac_log['weekday'] = df_ac_log['date'].dt.weekday

df_ac_log["AddToCart"] = df_ac_log["url"].str.contains("add_to_cart").astype(int) # str.extract("(add_to_cart)")
#df_ac_log.head()

# Reassign the IP address to IDs (make it easier later for creating edges)

ipaddrs = df_ac_log['ip'].unique()
new_ip_ids = list(range(len(df_ac_log['ip'].unique())))
map_ip = dict(zip(ipaddrs, new_ip_ids))
print(type(map_ip))

df_ac_log['ip_id'] = df_ac_log['ip'].map(map_ip)

print(len(ipaddrs))
#df_ac_log.head()



# Reassign the Product to IDs (make it easier later for creating edges)

products = df_ac_log['Product'].unique()
new_prod_ids = list(range(len(df_ac_log['Product'].unique())))
map_prod = dict(zip(products, new_prod_ids))
print(type(map_prod))

df_ac_log['Product_Id'] = df_ac_log['Product'].map(map_prod)

#### Category
# Reassign the Category to IDs

cats = df_ac_log['Category'].unique()
new_cat_ids = list(range(len(df_ac_log['Category'].unique())))
map_cat = dict(zip(cats, new_cat_ids))

df_ac_log['Category_Id'] = df_ac_log['Category'].map(map_cat)

# Reassign the Dept to IDs

depts = df_ac_log['Department'].unique()
new_dept_ids = list(range(len(df_ac_log['Department'].unique())))
map_dept = dict(zip(depts, new_dept_ids))

df_ac_log['Department_Id'] = df_ac_log['Department'].map(map_dept)

df_ac_log.info()

#Split date

df_ac_log['date_id'] = df_ac_log['Date'].str.split('/').str[1]
df_ac_log['month_id'] = df_ac_log['Date'].str.split('/').str[0]
df_ac_log['year_id'] = df_ac_log['Date'].str.split('/').str[2].str.split(' ').str[0]

df_ac_log['time'] = df_ac_log['Date'].str.split(' ').str[1]


df_ac_log['Dt'] = df_ac_log['Date'].str.split(' ').str[0]

# Convert the 'date' column to datetime
df_ac_log['date'] = pd.to_datetime(df_ac_log['Dt'])

# Extract the day of the week
df_ac_log['day_of_week'] = df_ac_log['date'].dt.day_name()
df_ac_log['weekday'] = df_ac_log['date'].dt.weekday

# Convert 'numbers' column to integer
df_ac_log['date_id'] = df_ac_log['date_id'].astype(int)
df_ac_log['month_id'] = df_ac_log['month_id'].astype(int)
df_ac_log['year_id'] = df_ac_log['year_id'].astype(int)

# Find outlier by Date - IP, if any

from datetime import datetime, timedelta
print(type(df_ac_log['date']))
df_ac_log['date1']= pd.to_datetime(df_ac_log['date'])

# group dataframe by date1 and get counts per day
date_counts = df_ac_log.groupby(['date1']).size().reset_index(name='count')
#date_counts = df_ac_log.groupby('date1').size()
print(date_counts.head(50))

# find max count in panda series date_count
max_count = date_counts['count'].max()
print(max_count)

min_count = date_counts['count'].min()
print(min_count)

# group date into week ids in df_session_log
df_ac_log['week_id'] = df_ac_log['date'].dt.isocalendar().week

# create year_month_id based on year_id and month_id
df_ac_log['year_month_id'] = df_ac_log['year_id'].astype(int)*100 + df_ac_log['month_id'].astype(int)

# remove rows with 20170914 date1 from df_ac_log due to unusual spike
df_ac_log = df_ac_log[df_ac_log['date1'] != '2017-09-14']

print(df_ac_log.shape)

# Reassign the year_month_id to snapshot_ID (make it easier later for splitting graphs)

year_months = df_ac_log['year_month_id'].unique()
new_year_months = list(range(len(df_ac_log['year_month_id'].unique())))
map_snapshot = dict(zip(year_months, new_year_months))
print(type(map_snapshot))

df_ac_log['snapshot_id'] = df_ac_log['year_month_id'].map(map_snapshot)

# Merge date and ip columns to create new column in df_ac_log
df_ac_log['date_ip_id'] = df_ac_log['date'].astype(str) + '_' + df_ac_log['ip_id'].astype(str)

access_count = df_ac_log.groupby("ip_id")["date_id"].count().rename("access_count")
print(access_count)

"""Encoding data"""

!pip install sentence-transformers

#install torch
!pip install torch

import torch
from sentence_transformers import SentenceTransformer

class SequenceEncoder:
     def __init__(self, model_name='all-MiniLM-L6-v2', device=None):
         self.device = device
         self.model = SentenceTransformer(model_name, device=device)

     @torch.no_grad()
     def __call__(self, df):
         x = self.model.encode(df.values, show_progress_bar=True,
                               convert_to_tensor=True, device=self.device)
         return x.cpu()

encoder = SequenceEncoder() # Create an instance of the SequenceEncoder class

# subset the dataframe
df_cat = df_ac_log[['Category']]
df_cat = df_cat.drop_duplicates()
print(df_cat.shape)


category_xs = [encoder(df_cat['Category'])] # Encode the 'Category' column
category_x = torch.cat(category_xs, dim=-1) # Concatenate the encoded results
print(category_x.shape)

# subset the dataframe
df_prod = df_ac_log[['Product']]
df_prod = df_prod.drop_duplicates()
print(df_prod.shape)

product_xs = [encoder(df_prod['Product'])] # Encode the 'Category' column
product_x = torch.cat(product_xs, dim=-1) # Concatenate the encoded results
print(product_x.shape)
print(product_x)

#Alternate encoder

from sklearn.preprocessing import LabelEncoder

# Label encode the 'Category' column
label_encoder = LabelEncoder()

# get unique category names
category_labels = label_encoder.fit_transform(df_ac_log['Category'].unique())
#print(category_labels)
print(len(category_labels))
category_labels_tensor = torch.tensor(category_labels, dtype=torch.long).view(-1, 1)
print(category_labels_tensor)



# group by date1 and ip to create unique session ids
df_ac_log['Session_ID'] = df_ac_log.groupby(['date1', 'ip_id']).ngroup()

#Sort the dataframe by Session_ID and time
df_ac_log = df_ac_log.sort_values(by=['Session_ID', 'time'])

df_ac_log.head()
df_ac_log.info()

df_ac_log.head()

"""# User based analysis"""

import pandas as pd

# subset df_ac_log by Session_id, time, AddToCart For Product based analysis
df_session_log = df_ac_log[['Session_ID', 'snapshot_id', 'ip_id','date', 'time', 'AddToCart', 'Product', 'Product_Id', 'Department', 'Department_Id']]

# add interaction column based on AddToCart , if 0 then 'view' else 1 then 'buy'
df_session_log['interaction'] = df_session_log['AddToCart'].apply(lambda x: 'view' if x == 0 else 'buy')

# sort dataframe by Session_ID and time
df_session_log = df_session_log.sort_values(by=['Session_ID', 'time'])

# add timestamp based on date and time
df_session_log['timestamp'] = pd.to_datetime(df_session_log['date'].astype(str) + ' ' + df_session_log['time'].astype(str))

# create timestamp using  pd.to_datetime
df_session_log['timestamp'] = pd.to_datetime(df_session_log['timestamp'])

# add time_spend based on time difference for same Session_Id
# Calculate time differences within each session
# Move the Timespent up by 1 row during calculation
import pandas as pd

# subset df_ac_log by Session_id, time, AddToCart For Product based analysis
df_session_log = df_ac_log[['Session_ID', 'snapshot_id', 'ip_id','date', 'time', 'AddToCart', 'Product', 'Product_Id', 'Department', 'Department_Id']]

# add interaction column based on AddToCart , if 0 then 'view' else 1 then 'buy'
df_session_log['interaction'] = df_session_log['AddToCart'].apply(lambda x: 'view' if x == 0 else 'buy')

# sort dataframe by Session_ID and time
df_session_log = df_session_log.sort_values(by=['Session_ID', 'time'])

# add timestamp based on date and time
df_session_log['timestamp'] = pd.to_datetime(df_session_log['date'].astype(str) + ' ' + df_session_log['time'].astype(str))

# create timestamp using  pd.to_datetime
df_session_log['timestamp'] = pd.to_datetime(df_session_log['timestamp'])

# add time_spend based on time difference for same Session_Id
# Calculate time differences within each session
# Move the Timespent up by 1 row during calculation

df_session_log['PrevTimeSpent'] = df_session_log.groupby('Session_ID')['timestamp'].diff().dt.total_seconds()
df_session_log['TimeSpent'] = df_session_log.groupby('Session_ID')['PrevTimeSpent'].shift(-1)

# Calculate user transitions
df_session_log['NextProduct_Id'] = df_session_log.groupby('Session_ID')['Product_Id'].shift(-1)
df_session_log['NextProduct'] = df_session_log.groupby('Session_ID')['Product'].shift(-1)


# Aggregate interaction sequences for each session
df_session_log['InteractionSequence'] = df_session_log.groupby('Session_ID')['interaction'].transform(lambda x: ' -> '.join(x))

df_session_log.head(15)

df_session_log['InteractionSequence'] = df_session_log.groupby('Session_ID')['interaction'].transform(lambda x: ' -> '.join(x))

df_session_log.head(15)

# Drop rows where there is no transition
transitions = df_session_log.dropna(subset=['NextProduct'])

!pip install torch_geometric
!pip install torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-2.0.0+cpu.html

"""# Create a subset"""

# Create a subset of df_temporal_subset with TimeWindow         User_ID  Product_ID Interaction
df_temporal_subset = df_session_log[['ip_id', 'Session_ID','Product_Id', 'timestamp',  'interaction']]

# sort by Session_ID and timestamp
df_temporal_subset = df_temporal_subset.sort_values(by=['Session_ID', 'timestamp'])
df_temporal_subset.head()

"""Create a dynamic graph"""

!pip install torch-geometric-temporal

!pip install --force-reinstall torch-sparse

from google.colab import files
uploaded = files.upload()  # Upload the file from your local machine

!mv tsagcn.py /usr/local/lib/python3.10/dist-packages/torch_geometric_temporal/nn/attention

import pandas as pd
from torch_geometric_temporal.signal import DynamicGraphTemporalSignal


# Create time-indexed snapshots
time_windows = sorted(df_temporal_subset['Session_ID'].unique())

# Prepare snapshots
edge_indices = []
edge_weights = []
node_features = []
labels = []  # Optional: Include labels for supervised tasks

for time in time_windows:
    snapshot = df_temporal_subset[df_temporal_subset['Session_ID'] == time]
    edge_index = snapshot[['ip_id', 'Product_Id']].values.T  # Edges (source, target)
    edge_weight = [1] * len(snapshot)  # Example: Assign weight=1 to all edges
    node_feature = pd.get_dummies(snapshot['Interaction']).values  # Example node features
    label = None  # Add labels if needed (e.g., for predictions)

    edge_indices.append(edge_index)
    edge_weights.append(edge_weight)
    node_features.append(node_feature)
    labels.append(label)

# Create dynamic graph
dynamic_graph = DynamicGraphTemporalSignal(
    edge_indices=edge_indices,
    edge_weights=edge_weights,
    features=node_features,
    targets=labels
)