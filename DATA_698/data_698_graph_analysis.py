# -*- coding: utf-8 -*-
"""DATA_698_Graph_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S_2qtQPIcCYH8-Vn1JAVVjf4XP3eGwks
"""

!pip install google-colab geopandas plotnine statsmodels xarray
!pip install pandas google-colab geopandas plotnine statsmodels xarray

"""# Load dataset"""



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
#to ignore warnings
import warnings
warnings.filterwarnings('ignore')

## In the interest of saving time, calling the API multiple times, as the information is of static nature
##   the generated file with added columns is saved and read directly to save compute time

access_log_location_url = 'https://raw.githubusercontent.com/baruab/baruab/refs/heads/main/DATA_698/tokenized_access_logs_global.csv'
df_ac_log = pd.read_csv(access_log_location_url)

# Drop uplicates
df_ac_log = df_ac_log.drop_duplicates()

"""# Feature engineering"""

df_ac_log['Dt'] = df_ac_log['Date'].str.split(' ').str[0]

# Convert the 'date' column to datetime
df_ac_log['date'] = pd.to_datetime(df_ac_log['Dt'])

# Extract the day of the week
df_ac_log['day_of_week'] = df_ac_log['date'].dt.day_name()
df_ac_log['weekday'] = df_ac_log['date'].dt.weekday



df_ac_log["AddToCart"] = df_ac_log["url"].str.contains("add_to_cart").astype(int) # str.extract("(add_to_cart)")
#df_ac_log.head()



# Reassign the IP address to IDs (make it easier later for creating edges)

ipaddrs = df_ac_log['ip'].unique()
new_ip_ids = list(range(len(df_ac_log['ip'].unique())))
map_ip = dict(zip(ipaddrs, new_ip_ids))
print(type(map_ip))

df_ac_log['ip_id'] = df_ac_log['ip'].map(map_ip)

print(len(ipaddrs))
#df_ac_log.head()



# Reassign the Product to IDs (make it easier later for creating edges)

products = df_ac_log['Product'].unique()
new_prod_ids = list(range(len(df_ac_log['Product'].unique())))
map_prod = dict(zip(products, new_prod_ids))
print(type(map_prod))

df_ac_log['Product_Id'] = df_ac_log['Product'].map(map_prod)

#### Category
# Reassign the Category to IDs

cats = df_ac_log['Category'].unique()
new_cat_ids = list(range(len(df_ac_log['Category'].unique())))
map_cat = dict(zip(cats, new_cat_ids))

df_ac_log['Category_Id'] = df_ac_log['Category'].map(map_cat)

# Reassign the Dept to IDs

depts = df_ac_log['Department'].unique()
new_dept_ids = list(range(len(df_ac_log['Department'].unique())))
map_dept = dict(zip(depts, new_dept_ids))

df_ac_log['Department_Id'] = df_ac_log['Department'].map(map_dept)

"""Split date into different parts"""

df_ac_log['date_id'] = df_ac_log['Date'].str.split('/').str[1]
df_ac_log['month_id'] = df_ac_log['Date'].str.split('/').str[0]
df_ac_log['year_id'] = df_ac_log['Date'].str.split('/').str[2].str.split(' ').str[0]

df_ac_log['time'] = df_ac_log['Date'].str.split(' ').str[1]


df_ac_log['Dt'] = df_ac_log['Date'].str.split(' ').str[0]

# Convert the 'date' column to datetime
df_ac_log['date'] = pd.to_datetime(df_ac_log['Dt'])

# Extract the day of the week
df_ac_log['day_of_week'] = df_ac_log['date'].dt.day_name()
df_ac_log['weekday'] = df_ac_log['date'].dt.weekday

# Convert 'numbers' column to integer
df_ac_log['date_id'] = df_ac_log['date_id'].astype(int)
df_ac_log['month_id'] = df_ac_log['month_id'].astype(int)
df_ac_log['year_id'] = df_ac_log['year_id'].astype(int)

# group date into week ids in df_session_log
df_ac_log['week_id'] = df_ac_log['date'].dt.isocalendar().week

# create year_month_id based on year_id and month_id
df_ac_log['year_month_id'] = df_ac_log['year_id'].astype(int)*100 + df_ac_log['month_id'].astype(int)

# Reassign the year_month_id to snapshot_ID (make it easier later for splitting graphs)

year_months = df_ac_log['year_month_id'].unique()
new_year_months = list(range(len(df_ac_log['year_month_id'].unique())))
map_snapshot = dict(zip(year_months, new_year_months))
print(type(map_snapshot))

df_ac_log['snapshot_id'] = df_ac_log['year_month_id'].map(map_snapshot)

# Merge date and ip columns to create new column in df_ac_log
df_ac_log['date_ip_id'] = df_ac_log['date'].astype(str) + '_' + df_ac_log['ip_id'].astype(str)

access_count = df_ac_log.groupby("ip_id")["date_id"].count().rename("access_count")
print(access_count)

"""Encoding data"""

!pip install sentence-transformers

#install torch
!pip install torch

import torch
from sentence_transformers import SentenceTransformer

class SequenceEncoder:
     def __init__(self, model_name='all-MiniLM-L6-v2', device=None):
         self.device = device
         self.model = SentenceTransformer(model_name, device=device)

     @torch.no_grad()
     def __call__(self, df):
         x = self.model.encode(df.values, show_progress_bar=True,
                               convert_to_tensor=True, device=self.device)
         return x.cpu()

encoder = SequenceEncoder() # Create an instance of the SequenceEncoder class

# subset the dataframe
df_cat = df_ac_log[['Category']]
df_cat = df_cat.drop_duplicates()
print(df_cat.shape)


category_xs = [encoder(df_cat['Category'])] # Encode the 'Category' column
category_x = torch.cat(category_xs, dim=-1) # Concatenate the encoded results
print(category_x.shape)

# subset the dataframe
df_prod = df_ac_log[['Product']]
df_prod = df_prod.drop_duplicates()
print(df_prod.shape)

product_xs = [encoder(df_prod['Product'])] # Encode the 'Category' column
product_x = torch.cat(product_xs, dim=-1) # Concatenate the encoded results
print(product_x.shape)
print(product_x)

"""Alternate label encoder"""

from sklearn.preprocessing import LabelEncoder

# Label encode the 'Category' column
label_encoder = LabelEncoder()

# get unique category names
category_labels = label_encoder.fit_transform(df_ac_log['Category'].unique())
#print(category_labels)
print(len(category_labels))
category_labels_tensor = torch.tensor(category_labels, dtype=torch.long).view(-1, 1)
print(category_labels_tensor)



# Find outlier by Date - IP, if any

from datetime import datetime, timedelta
print(type(df_ac_log['date']))
df_ac_log['date1']= pd.to_datetime(df_ac_log['date'])

# group dataframe by date1 and ip and get counts per day
date_counts = df_ac_log.groupby(['date1', 'ip']).size().reset_index(name='count')
#date_counts = df_ac_log.groupby('date1').size()
print(date_counts)

# find max count in panda series date_count
max_count = date_counts['count'].max()
print(max_count)

min_count = date_counts['count'].min()
print(min_count)

#df_ac_log.head()

# group by date1 and ip to create unique session ids
df_ac_log['Session_ID'] = df_ac_log.groupby(['date1', 'ip_id']).ngroup()
df_ac_log.head()
df_ac_log.info()

df_ac_log.head()

"""Subset the dataframe for Session interaction analysis"""

import pandas as pd

# subset df_ac_log by Session_id, time, AddToCart
df_session_log = df_ac_log[['Session_ID', 'ip_id','date', 'time', 'AddToCart', 'Product', 'Product_Id', 'Department', 'Department_Id']]

# add interaction column based on AddToCart , if 0 then 'view' else 1 then 'buy'
df_session_log['interaction'] = df_session_log['AddToCart'].apply(lambda x: 'view' if x == 0 else 'buy')

# sort dataframe by Session_ID and time
df_session_log = df_session_log.sort_values(by=['Session_ID', 'time'])

# add timestamp based on date and time
df_session_log['timestamp'] = pd.to_datetime(df_session_log['date'].astype(str) + ' ' + df_session_log['time'].astype(str))

# create timestamp using  pd.to_datetime
df_session_log['timestamp'] = pd.to_datetime(df_session_log['timestamp'])

# add time_spend based on time difference for same Session_Id
# Calculate time differences within each session
df_session_log['TimeSpent'] = df_session_log.groupby('Session_ID')['timestamp'].diff().dt.total_seconds()

# Aggregate interaction sequences for each session
df_session_log['InteractionSequence'] = df_session_log.groupby('Session_ID')['interaction'].transform(lambda x: ' -> '.join(x))


df_session_log.head(5)

import seaborn as sns

plt.figure(figsize=(10, 6))
sns.boxplot(x='interaction', y='TimeSpent', data=df_session_log)
plt.title("Time Spent per Interaction")
plt.ylabel("Time Spent (seconds)")
plt.xlabel("Interaction")
plt.xticks(rotation=45)
plt.show()

"""Construct a Graph based on time spent by interaction type"""

import networkx as nx

# Initialize a directed graph
G = nx.DiGraph()

# Add edges with average time spent as weights
for session_id, group in df_session_log.groupby('Session_ID'):
    for i in range(len(group) - 1):
        G.add_edge(
            group['interaction'].iloc[i],
            group['interaction'].iloc[i + 1],
            weight=group['TimeSpent'].iloc[i + 1]
        )

# Draw the graph with edge weights
pos = nx.spring_layout(G, seed=42)
nx.draw(G, pos, with_labels=True, node_size=2000, node_color='skyblue')
edge_labels = nx.get_edge_attributes(G, 'weight')
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)
plt.title("Interaction Patterns with Time Spent")
plt.show()

"""Graph based on Product interaction"""

num_sessions = df_session_log['Session_ID'].nunique()
print("Number of sessions:", num_sessions)

# take 100 sessions in a dataframe
df_session_100 = df_session_log.head(100)

# Build a graph of product interactions
G = nx.DiGraph()
for session_id, group in df_session_log.groupby("Session_ID"):
    products = group["Product"].tolist()
    for i in range(len(products) - 1):
        G.add_edge(products[i], products[i + 1])

# Compute centrality measures
degree_centrality = nx.degree_centrality(G)
betweenness_centrality = nx.betweenness_centrality(G)
closeness_centrality = nx.closeness_centrality(G)
pagerank = nx.pagerank(G)

# Display results
print("Degree Centrality:", degree_centrality)
print("Betweenness Centrality:", betweenness_centrality)
print("Closeness Centrality:", closeness_centrality)
print("PageRank:", pagerank)

# Visualize the graph
pos = nx.spring_layout(G, seed=42)
plt.figure(figsize=(8, 6))
nx.draw(G, pos, with_labels=True, node_size=2000, node_color='skyblue')
plt.title("Department Interaction Graph")
plt.show()

"""Product embedding"""

!pip install torch-geometric
!pip install torch-scatter torch-sparse torch-cluster

from torch_geometric.nn import Node2Vec
import torch
from torch_geometric.data import Data
from torch_geometric.utils import negative_sampling


# Get a mapping from product name to numerical index
product_to_index = {product: i for i, product in enumerate(G.nodes)}

# Convert NetworkX edges to PyTorch Geometric format
edge_index = [[product_to_index[u], product_to_index[v]] for u, v in G.edges]
edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()

# Initialize Node2Vec model
node2vec = Node2Vec(edge_index, embedding_dim=64, walk_length=10, context_size=5, walks_per_node=10)
optimizer = torch.optim.Adam(node2vec.parameters(), lr=0.01)

# Training Node2Vec
node2vec.train()
for epoch in range(100):
    optimizer.zero_grad()
    # Generate positive and negative random walks
    # The sample method doesn't take a batch_size argument
    # It generates all random walks during initialization
    batch = torch.randint(0, edge_index.max().item() + 1, (64,))  # Replace 64 with your desired batch size
    pos_rw, neg_rw = node2vec.sample(batch)

    # Calculate the loss
    loss = node2vec.loss(pos_rw, neg_rw)
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch + 1}, Loss: {loss.item()}")

# Extract product embeddings
product_embeddings = node2vec(torch.arange(edge_index.max().item() + 1))
print("Product Embeddings Shape:", product_embeddings.shape)

"""Analyze embedding for Product popularity using Cosine similarity"""

from sklearn.metrics.pairwise import cosine_similarity

# Compute similarity matrix
similarity_matrix = cosine_similarity(product_embeddings.detach().numpy())

# Display most similar products for a target product
target_product_index = 0  # Example product index
most_similar = similarity_matrix[target_product_index].argsort()[::-1]
print("Most similar products to product:", most_similar[:5])

# map Product_id to Product
product_id_to_name = {v: k for k, v in product_to_index.items()}
print("Most similar products to product:", [product_id_to_name[i] for i in most_similar[:5]])

"""Weighted centrality"""

# Add time spent data to centrality scores
time_spent = df_session_log.groupby("Product")["TimeSpent"].mean()
weighted_pagerank = {k: v * time_spent.get(k, 1) for k, v in pagerank.items()}

from sklearn.manifold import TSNE

# Reduce embeddings to 2D for visualization
reduced_embeddings = TSNE(n_components=2, random_state=42).fit_transform(product_embeddings.detach().numpy())

# Plot product embeddings
plt.figure(figsize=(8, 6))
plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=list(degree_centrality.values()), cmap='coolwarm', s=100)
plt.title("Product Embeddings (Colored by Degree Centrality)")
plt.colorbar(label="Degree Centrality")
plt.show()

"""Encode sequences"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.sequence import pad_sequences


# Encode interactions
interaction_encoder = LabelEncoder()
df_session_log['InteractionEncoded'] = interaction_encoder.fit_transform(df_session_log['interaction'])

# Group by user to create sequences
sequences = df_session_log.groupby('ip_id')['InteractionEncoded'].apply(list).reset_index(name='Sequence')

# Add purchase labels (1 if a purchase exists in the sequence)
sequences['AddToCart'] = df_session_log.groupby('ip_id')['AddToCart'].max().reset_index(drop=True)

# Pad sequences to fixed length
MAX_SEQ_LENGTH = 5
padded_sequences = pad_sequences(sequences['Sequence'], maxlen=MAX_SEQ_LENGTH, padding='post')
print(padded_sequences)

"""Use RNN to process"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout

# Define model
model = Sequential([
    Embedding(input_dim=len(interaction_encoder.classes_), output_dim=16, input_length=MAX_SEQ_LENGTH),
    LSTM(32, return_sequences=False),
    Dropout(0.5),
    Dense(16, activation='relu'),
    Dense(1, activation='sigmoid')  # Sigmoid for binary classification
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
print(model.summary())

# Example: Splitting data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(padded_sequences, sequences['AddToCart'], test_size=0.2, random_state=42)

# Train model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# Evaluate model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy:.4f}")

# Combine user and product features with sequences
df_session_log['UserFeature'] = df_session_log['ip_id'].map(user_features)
df_session_log['ProductFeature'] = df_session_log['interaction'].map(product_features)

"""Evaluate the model"""

from sklearn.metrics import classification_report, roc_auc_score

y_pred = (model.predict(X_test) > 0.5).astype(int)
print(classification_report(y_test, y_pred))
print(f"AUC-ROC: {roc_auc_score(y_test, y_pred):.4f}")

"""Visualize predictions"""

import matplotlib.pyplot as plt

# Predict probabilities
probs = model.predict(X_test).flatten()

# Plot histogram
plt.hist(probs, bins=20, color='skyblue', edgecolor='black')
plt.title("Distribution of Predicted Conversion Probabilities")
plt.xlabel("Probability of Purchase")
plt.ylabel("Frequency")
plt.show()

"""Generate Traffic Heatmap"""

# Aggregate by time windows and products
df_session_log['TimeWindow'] = df_session_log['timestamp'].dt.floor('H')  # Group by hourly windows
traffic = df_session_log.groupby(['TimeWindow', 'Product_Id']).size().reset_index(name='InteractionCount')

# Create pivot table
heatmap_data = traffic.pivot(index='TimeWindow', columns='Product_Id', values='InteractionCount').fillna(0)
print(heatmap_data)

traffic.head()

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
sns.heatmap(
    heatmap_data.T,  # Transpose to display products on rows
    annot=True,      # Display interaction counts
    fmt="g",         # Format annotation as integer
    cmap="YlGnBu",   # Color map
    cbar_kws={'label': 'Interaction Count'}
)

plt.title("Traffic Heatmap: Product Interactions Over Time")
plt.ylabel("Product ID")
plt.xlabel("Time Window")
plt.xticks(rotation=45)
plt.show()

# Aggregate separately by interaction type
traffic_by_type = df_session_log.groupby(['TimeWindow', 'Product_Id', 'interaction']).size().reset_index(name='InteractionCount')

heatmap_data = heatmap_data.div(heatmap_data.sum(axis=0), axis=1)  # Normalize by column

import plotly.express as px

fig = px.imshow(
    heatmap_data.T,
    labels=dict(x="Time Window", y="Product ID", color="Interaction Count"),
    x=heatmap_data.index.strftime('%Y-%m-%d %H:%M:%S'),  # Format time
    y=heatmap_data.columns,
    color_continuous_scale="YlGnBu"
)
fig.update_layout(title="Interactive Traffic Heatmap")
fig.show()

# Create transitions
df_session_log['NextInteraction'] = df_session_log.groupby('ip_id')['interaction'].shift(-1)
transitions = df_session_log.dropna().groupby(['interaction', 'NextInteraction']).size().reset_index(name='Count')
print(transitions)

"""Visualize pathways"""

import networkx as nx
import matplotlib.pyplot as plt

# Create a directed graph
G = nx.DiGraph()

# Add edges with weights
for _, row in transitions.iterrows():
    G.add_edge(row['interaction'], row['NextInteraction'], weight=row['Count'])

# Draw the graph
pos = nx.spring_layout(G, seed=42)  # Layout for better visualization
plt.figure(figsize=(8, 6))
nx.draw(G, pos, with_labels=True, node_size=3000, node_color="skyblue", font_size=10, font_weight="bold")
labels = nx.get_edge_attributes(G, 'weight')
nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)
plt.title("Common Pathway: view to AddToCart")
plt.show()

import plotly.graph_objects as go

# Prepare data for Sankey diagram
source = transitions['interaction'].map({'view': 0, 'AddToCart': 1}).tolist()
target = transitions['NextInteraction'].map({'view': 0, 'AddToCart': 1}).tolist()
value = transitions['Count'].tolist()

# Create Sankey diagram
fig = go.Figure(go.Sankey(
    node=dict(
        pad=15,
        thickness=20,
        line=dict(color="black", width=0.5),
        label=["view", "AddToCart"]
    ),
    link=dict(
        source=source,
        target=target,
        value=value
    )
))

fig.update_layout(title_text="Pathway from view to AddToCart", font_size=10)
fig.show()

from matplotlib.sankey import Sankey

# Create Sankey diagram
fig = plt.figure(figsize=(10, 6))
ax = fig.add_subplot(1, 1, 1, xticks=[], yticks=[])
sankey = Sankey(ax=ax, unit=None)
for _, row in transitions.iterrows():
    sankey.add(flows=[row['Count'], -row['Count']],
               labels=[row['interaction'], row['NextInteraction']],
               orientations=[0, 0])
sankey.finish()
plt.title("Flow Diagram: view to AddToCart")
plt.show()