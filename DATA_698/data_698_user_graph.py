# -*- coding: utf-8 -*-
"""DATA_698_User_Graph.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZFUTKxiktmxJeEavG6VGZX47kH_5xFBV
"""

!pip install google-colab geopandas plotnine statsmodels xarray
!pip install pandas google-colab geopandas plotnine statsmodels xarray

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
#to ignore warnings
import warnings
warnings.filterwarnings('ignore')

## In the interest of saving time, calling the API multiple times, as the information is of static nature
##   the generated file with added columns is saved and read directly to save compute time

access_log_location_url = 'https://raw.githubusercontent.com/baruab/baruab/refs/heads/main/DATA_698/tokenized_access_logs_global.csv'
df_ac_log = pd.read_csv(access_log_location_url)

# Drop uplicates
df_ac_log = df_ac_log.drop_duplicates()

df_ac_log['Dt'] = df_ac_log['Date'].str.split(' ').str[0]

# Convert the 'date' column to datetime
df_ac_log['date'] = pd.to_datetime(df_ac_log['Dt'])

# Extract the day of the week
df_ac_log['day_of_week'] = df_ac_log['date'].dt.day_name()
df_ac_log['weekday'] = df_ac_log['date'].dt.weekday

df_ac_log["AddToCart"] = df_ac_log["url"].str.contains("add_to_cart").astype(int) # str.extract("(add_to_cart)")
#df_ac_log.head()

# Reassign the IP address to IDs (make it easier later for creating edges)

ipaddrs = df_ac_log['ip'].unique()
new_ip_ids = list(range(len(df_ac_log['ip'].unique())))
map_ip = dict(zip(ipaddrs, new_ip_ids))
print(type(map_ip))

df_ac_log['ip_id'] = df_ac_log['ip'].map(map_ip)

print(len(ipaddrs))
#df_ac_log.head()



# Reassign the Product to IDs (make it easier later for creating edges)

products = df_ac_log['Product'].unique()
new_prod_ids = list(range(len(df_ac_log['Product'].unique())))
map_prod = dict(zip(products, new_prod_ids))
print(type(map_prod))

df_ac_log['Product_Id'] = df_ac_log['Product'].map(map_prod)

#### Category
# Reassign the Category to IDs

cats = df_ac_log['Category'].unique()
new_cat_ids = list(range(len(df_ac_log['Category'].unique())))
map_cat = dict(zip(cats, new_cat_ids))

df_ac_log['Category_Id'] = df_ac_log['Category'].map(map_cat)

# Reassign the Dept to IDs

depts = df_ac_log['Department'].unique()
new_dept_ids = list(range(len(df_ac_log['Department'].unique())))
map_dept = dict(zip(depts, new_dept_ids))

df_ac_log['Department_Id'] = df_ac_log['Department'].map(map_dept)

df_ac_log.info()

#Split date

df_ac_log['date_id'] = df_ac_log['Date'].str.split('/').str[1]
df_ac_log['month_id'] = df_ac_log['Date'].str.split('/').str[0]
df_ac_log['year_id'] = df_ac_log['Date'].str.split('/').str[2].str.split(' ').str[0]

df_ac_log['time'] = df_ac_log['Date'].str.split(' ').str[1]


df_ac_log['Dt'] = df_ac_log['Date'].str.split(' ').str[0]

# Convert the 'date' column to datetime
df_ac_log['date'] = pd.to_datetime(df_ac_log['Dt'])

# Extract the day of the week
df_ac_log['day_of_week'] = df_ac_log['date'].dt.day_name()
df_ac_log['weekday'] = df_ac_log['date'].dt.weekday

# Convert 'numbers' column to integer
df_ac_log['date_id'] = df_ac_log['date_id'].astype(int)
df_ac_log['month_id'] = df_ac_log['month_id'].astype(int)
df_ac_log['year_id'] = df_ac_log['year_id'].astype(int)

# Find outlier by Date - IP, if any

from datetime import datetime, timedelta
print(type(df_ac_log['date']))
df_ac_log['date1']= pd.to_datetime(df_ac_log['date'])

# group dataframe by date1 and get counts per day
date_counts = df_ac_log.groupby(['date1']).size().reset_index(name='count')
#date_counts = df_ac_log.groupby('date1').size()
print(date_counts.head(50))

# find max count in panda series date_count
max_count = date_counts['count'].max()
print(max_count)

min_count = date_counts['count'].min()
print(min_count)

# group date into week ids in df_session_log
df_ac_log['week_id'] = df_ac_log['date'].dt.isocalendar().week

# create year_month_id based on year_id and month_id
df_ac_log['year_month_id'] = df_ac_log['year_id'].astype(int)*100 + df_ac_log['month_id'].astype(int)

# remove rows with 20170914 date1 from df_ac_log due to unusual spike
df_ac_log = df_ac_log[df_ac_log['date1'] != '2017-09-14']

print(df_ac_log.shape)

# Reassign the year_month_id to snapshot_ID (make it easier later for splitting graphs)

year_months = df_ac_log['year_month_id'].unique()
new_year_months = list(range(len(df_ac_log['year_month_id'].unique())))
map_snapshot = dict(zip(year_months, new_year_months))
print(type(map_snapshot))

df_ac_log['snapshot_id'] = df_ac_log['year_month_id'].map(map_snapshot)

# Merge date and ip columns to create new column in df_ac_log
df_ac_log['date_ip_id'] = df_ac_log['date'].astype(str) + '_' + df_ac_log['ip_id'].astype(str)

access_count = df_ac_log.groupby("ip_id")["date_id"].count().rename("access_count")
print(access_count)

"""Encoding data"""

!pip install sentence-transformers

#install torch
!pip install torch

import torch
from sentence_transformers import SentenceTransformer

class SequenceEncoder:
     def __init__(self, model_name='all-MiniLM-L6-v2', device=None):
         self.device = device
         self.model = SentenceTransformer(model_name, device=device)

     @torch.no_grad()
     def __call__(self, df):
         x = self.model.encode(df.values, show_progress_bar=True,
                               convert_to_tensor=True, device=self.device)
         return x.cpu()

encoder = SequenceEncoder() # Create an instance of the SequenceEncoder class

# subset the dataframe
df_cat = df_ac_log[['Category']]
df_cat = df_cat.drop_duplicates()
print(df_cat.shape)


category_xs = [encoder(df_cat['Category'])] # Encode the 'Category' column
category_x = torch.cat(category_xs, dim=-1) # Concatenate the encoded results
print(category_x.shape)

# subset the dataframe
df_prod = df_ac_log[['Product']]
df_prod = df_prod.drop_duplicates()
print(df_prod.shape)

product_xs = [encoder(df_prod['Product'])] # Encode the 'Category' column
product_x = torch.cat(product_xs, dim=-1) # Concatenate the encoded results
print(product_x.shape)
print(product_x)

#Alternate encoder

from sklearn.preprocessing import LabelEncoder

# Label encode the 'Category' column
label_encoder = LabelEncoder()

# get unique category names
category_labels = label_encoder.fit_transform(df_ac_log['Category'].unique())
#print(category_labels)
print(len(category_labels))
category_labels_tensor = torch.tensor(category_labels, dtype=torch.long).view(-1, 1)
print(category_labels_tensor)



# group by date1 and ip to create unique session ids
df_ac_log['Session_ID'] = df_ac_log.groupby(['date1', 'ip_id']).ngroup()

#Sort the dataframe by Session_ID and time
df_ac_log = df_ac_log.sort_values(by=['Session_ID', 'time'])

df_ac_log.head()
df_ac_log.info()

df_ac_log.head()

"""# User based analysis"""

import pandas as pd

# subset df_ac_log by Session_id, time, AddToCart For Product based analysis
df_session_log = df_ac_log[['Session_ID', 'snapshot_id', 'ip_id','date', 'time', 'AddToCart', 'Product', 'Product_Id', 'Department', 'Department_Id']]

# add interaction column based on AddToCart , if 0 then 'view' else 1 then 'buy'
df_session_log['interaction'] = df_session_log['AddToCart'].apply(lambda x: 'view' if x == 0 else 'buy')

# sort dataframe by Session_ID and time
df_session_log = df_session_log.sort_values(by=['Session_ID', 'time'])

# add timestamp based on date and time
df_session_log['timestamp'] = pd.to_datetime(df_session_log['date'].astype(str) + ' ' + df_session_log['time'].astype(str))

# create timestamp using  pd.to_datetime
df_session_log['timestamp'] = pd.to_datetime(df_session_log['timestamp'])

# add time_spend based on time difference for same Session_Id
# Calculate time differences within each session
# Move the Timespent up by 1 row during calculation
import pandas as pd

# subset df_ac_log by Session_id, time, AddToCart For Product based analysis
df_session_log = df_ac_log[['Session_ID', 'snapshot_id', 'ip_id','date', 'time', 'AddToCart', 'Product', 'Product_Id', 'Department', 'Department_Id']]

# add interaction column based on AddToCart , if 0 then 'view' else 1 then 'buy'
df_session_log['interaction'] = df_session_log['AddToCart'].apply(lambda x: 'view' if x == 0 else 'buy')

# sort dataframe by Session_ID and time
df_session_log = df_session_log.sort_values(by=['Session_ID', 'time'])

# add timestamp based on date and time
df_session_log['timestamp'] = pd.to_datetime(df_session_log['date'].astype(str) + ' ' + df_session_log['time'].astype(str))

# create timestamp using  pd.to_datetime
df_session_log['timestamp'] = pd.to_datetime(df_session_log['timestamp'])

# add time_spend based on time difference for same Session_Id
# Calculate time differences within each session
# Move the Timespent up by 1 row during calculation

df_session_log['PrevTimeSpent'] = df_session_log.groupby('Session_ID')['timestamp'].diff().dt.total_seconds()
df_session_log['TimeSpent'] = df_session_log.groupby('Session_ID')['PrevTimeSpent'].shift(-1)

# Calculate user transitions
df_session_log['NextProduct_Id'] = df_session_log.groupby('Session_ID')['Product_Id'].shift(-1)
df_session_log['NextProduct'] = df_session_log.groupby('Session_ID')['Product'].shift(-1)


# Aggregate interaction sequences for each session
df_session_log['InteractionSequence'] = df_session_log.groupby('Session_ID')['interaction'].transform(lambda x: ' -> '.join(x))
df_session_log.head(15)

# Drop rows where there is no transition
transitions = df_session_log.dropna(subset=['NextProduct'])

"""Collapse the dataframe to unique sessions with num of requests, number of buys"""

# create a subset dataframe with unique session_ID, sum AddToCart and number of requests per session
# create a subset dataframe with Session_ID as group

# Group by Session_ID and aggregate
session_df = df_session_log.groupby('Session_ID').agg(
    ip_id = ('ip_id', 'first'),
    snapshot_id = ('snapshot_id', 'first'),
    num_buy=('AddToCart', 'sum'),          # Sum of AddToCart per session
    num_request=('Session_ID', 'count'),         # Number of requests per session
    avg_TimeSpent=('TimeSpent', 'mean')
).reset_index()

# absolute value for avg_timespent column
session_df['avg_TimeSpent'] = session_df['avg_TimeSpent'].abs()

# Add a column to flag drop-offs (AddToCart_Sum = 0 means drop-off)
session_df['Drop_Off'] = (session_df['num_buy'] == 0).astype(int)

session_df.head()

"""# Create the User classification model"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score

# Define features and target
X = session_df[['num_request', 'avg_TimeSpent', 'num_buy']]
y = session_df['Drop_Off']

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest Classifier
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]  # Probability of drop-off

print(classification_report(y_test, y_pred))
print(f"AUC-ROC: {roc_auc_score(y_test, y_proba):.2f}")

"""Analyze User Behavior with Graph-Based Models"""

import networkx as nx

# Create a bipartite graph (User-Product)
G = nx.Graph()
for _, row in session_df.iterrows():
    G.add_node(row['ip_id'], bipartite=0)  # User nodes
    # add node features
    G.nodes[row['ip_id']]['num_request'] = row['num_request']
    G.nodes[row['ip_id']]['avg_TimeSpent'] = row['avg_TimeSpent']
    G.nodes[row['ip_id']]['num_buy'] = row['num_buy']
    G.nodes[row['ip_id']]['Drop_Off'] = row['Drop_Off']
    G.add_node(row['Session_ID'], bipartite=1)  # Session nodes
    G.add_edge(row['ip_id'], row['Session_ID'], weight=row['avg_TimeSpent'])  # Edge with weight

print("Nodes:", G.nodes())
# num nodes
print("Number of nodes:", G.number_of_nodes())
# num edges
print("Number of edges:", G.number_of_edges())
print("Edges:", G.edges(data=True))

!pip install torch_geometric
!pip install torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-2.0.0+cpu.html

import torch
from torch_geometric.nn import SAGEConv
from torch_geometric.data import Data

# Convert the graph to PyTorch Geometric format
edge_index = torch.tensor(list(G.edges()), dtype=torch.long).t().contiguous() # Change the dtype to torch.long
edge_weight = torch.tensor([G[u][v]['weight'] for u, v in G.edges()])


# Define node features (e.g., dummy features for simplicity)
# add node features
#x = torch.tensor([[row['num_request'], row['avg_TimeSpent'], row['num_buy'], row['num_buy']] for _, row in session_df.iterrows()], dtype=torch.float)
x = torch.randn((len(G.nodes), 4))  # Random node features
y = torch.tensor(session_df['Drop_Off'].tolist())  # Labels

# Create a PyTorch Geometric Data object
graph_data = Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=y)

# Define GraphSAGE model
class GraphSAGE(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super(GraphSAGE, self).__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, out_channels)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index).relu()
        x = self.conv2(x, edge_index)
        return x

# Train the GNN
model = GraphSAGE(in_channels=4, hidden_channels=16, out_channels=2)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = torch.nn.CrossEntropyLoss()

model.train()
for epoch in range(100):
    optimizer.zero_grad()
    out = model(graph_data.x, graph_data.edge_index)
    loss = criterion(out, graph_data.y)
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss.item()}")

"""Evaluate User groups to identity user drop-offs"""

from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# Use node embeddings (or features) for clustering
node_embeddings = out.detach().numpy()

# Replace NaN with 0
node_embeddings = np.nan_to_num(node_embeddings)

# Reduce dimensions for visualization
pca = PCA(n_components=2)
reduced_embeddings = pca.fit_transform(node_embeddings)

# Cluster users
kmeans = KMeans(n_clusters=2, random_state=42)
clusters = kmeans.fit_predict(reduced_embeddings)

# Visualize clusters
plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=clusters, cmap='coolwarm')
plt.title("User Clusters (Buying vs Drop-offs)")

# add legend
plt.legend(['Buying User', 'Drop-offs'])
plt.show()

"""# Create a subset of df_session_log with ip_id, Product_Id, City, AddToCart"""

# Create a subset of df_session_log with ip_id, Product_Id, City, AddToCart
df_log_subset = df_ac_log[['Session_ID' ,'ip_id', 'Product_Id', 'City', 'AddToCart']]

# filter AddToCart ==1
df_user_segmentation = df_log_subset[df_log_subset['AddToCart'] ==1]

df_user_segmentation.head()

print(df_user_segmentation.shape)

# print unique ip_id
print(df_user_segmentation['ip_id'].nunique())

# print unique Product_Id
print(df_user_segmentation['Product_Id'].nunique())

# print unique session_ID
print(df_session_log['Session_ID'].nunique())

# print where drop_off == 0
print(session_df[session_df['Drop_Off'] == 0]['Session_ID'].nunique())

# print where drop_off == 0
print(session_df[session_df['Drop_Off'] == 1]['Session_ID'].nunique())

print(df_user_segmentation.shape)

df_user_5kseg = df_user_segmentation.head(5000)

print(df_user_5kseg.head(5))

# city df_user_5kseg
city_labels = pd.Series(df_user_5kseg['City'], index=df_user_5kseg['ip_id'])

print(city_labels.shape)

# Map cities to unique colors
# unique city from df_user_5kseg
city_labels = df_user_5kseg['City']
# unique city
unique_cities = city_labels.unique()
print(city_labels.shape)
print(unique_cities.shape)

city_to_color = {city: color for city, color in zip(unique_cities, plt.cm.tab10.colors)}
colors = city_labels.map(city_to_color)
print(colors.shape)
print(colors)

# Initialize a graph
G = nx.Graph()

# Add nodes for each user
for _, row in df_user_5kseg.iterrows():
    G.add_node(row['ip_id'], city=row['City'])

# Add edges based on shared purchases
for product, group in df_user_5kseg.groupby('Product_Id'):
    users = group['ip_id'].tolist()

    #print len users
    print("len(users): ",len(users))

    for i in range(len(users)):
        for j in range(i + 1, len(users)):
            G.add_edge(users[i], users[j], type='shared_purchase', weight=1)

#print len df_user_5kseg
print("len(df_user_5kseg): ", len(df_user_5kseg))

# Add edges based on geographic proximity
for i, user1 in df_user_5kseg.iterrows():
    for j, user2 in df_user_5kseg.iterrows():
        if i < j:
            # Calculate geographic distance
            loc1 = user1['City']
            loc2 = user2['City']

            if loc1 == loc2:  # Same city
                G.add_edge(user1['ip_id'], user2['ip_id'], type='geo_proximity', weight=1)


# Print graph nodes and edges
print("Nodes:", G.nodes())
print("Edges:", G.edges(data=True))
# num nodes
print("Number of nodes:", G.number_of_nodes())
# num edges
print("Number of edges:", G.number_of_edges())

"""Learn Node Representation"""

!pip install node2vec

from node2vec import Node2Vec

# Generate embeddings using Node2Vec
node2vec = Node2Vec(G, dimensions=16, walk_length=10, num_walks=100, workers=4)
model = node2vec.fit(window=5, min_count=1, batch_words=4)

# Get embeddings as a DataFrame
embeddings = pd.DataFrame([model.wv[str(node)] for node in G.nodes], index=G.nodes)
print(embeddings.head())

"""Cluster User based on Node learnings (embeddings)"""

from sklearn.cluster import KMeans

# Cluster embeddings
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(embeddings)

# Add cluster labels to the DataFrame
clustered_users = pd.DataFrame({'User_ID': embeddings.index, 'Cluster': clusters})
print(clustered_users)

"""Visualize the clusters"""

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Reduce dimensions for visualization
pca = PCA(n_components=2)
reduced_embeddings = pca.fit_transform(embeddings)

# Plot the clusters
plt.figure(figsize=(10, 6))
plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=clusters, cmap='coolwarm', alpha=0.8)
for i, user in enumerate(embeddings.index):
    plt.annotate(user, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]))
plt.title("User Segments Based on Shared Purchases and Same City")
plt.xlabel("PCA Dimension 1")
plt.ylabel("PCA Dimension 2")
plt.show()

# Plot the scatter plot
# Handle NaN values in 'colors'

colors_subset = []
for user in embeddings.index:
    city = df_user_5kseg.loc[df_user_5kseg['ip_id'] == user, 'City'].iloc[0]
    # Check if the city is in the dictionary, otherwise use a default color
    color = city_to_color.get(city, 'gray')  # Use 'gray' as default color
    colors_subset.append(color)

# Convert RGB tuples to valid color format (if necessary)
colors_subset = [tuple(c) if isinstance(c, (tuple, list)) and len(c) in (3, 4) else c for c in colors_subset]

plt.figure(figsize=(10, 6))
scatter = plt.scatter(
    reduced_embeddings[:, 0],
    reduced_embeddings[:, 1],
    c=colors_subset,  # Use city-based colors for the subset of users
    alpha=0.8,
    s=100
)

# Add annotations for users
for i, user in enumerate(embeddings.index):
    plt.annotate(user, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), fontsize=9)

# Add a legend for cities
legend_elements = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10, label=city)
                   for city, color in city_to_color.items()]
plt.legend(handles=legend_elements, title="Cities", loc='best')

# Add titles and labels
plt.title("User Segments by City buying same Products")
plt.xlabel("PCA Dimension 1")
plt.ylabel("PCA Dimension 2")
plt.show()

from sklearn.cluster import KMeans
cluster_centers = pca.transform(kmeans.cluster_centers_)
plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='black', marker='x', label='Centroids')
plt.legend()

import plotly.express as px
fig = px.scatter(x=reduced_embeddings[:, 0], y=reduced_embeddings[:, 1],
                 color=clusters, text=embeddings.index)
fig.show()