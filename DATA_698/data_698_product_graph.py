# -*- coding: utf-8 -*-
"""DATA_698_Product_Graph.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rls0WFZ8J5VPBCzbu_fARwCmQeFS36Bi
"""



!pip install google-colab geopandas plotnine statsmodels xarray
!pip install pandas google-colab geopandas plotnine statsmodels xarray

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
#to ignore warnings
import warnings
warnings.filterwarnings('ignore')

## In the interest of saving time, calling the API multiple times, as the information is of static nature
##   the generated file with added columns is saved and read directly to save compute time

access_log_location_url = 'https://raw.githubusercontent.com/baruab/baruab/refs/heads/main/DATA_698/tokenized_access_logs_global.csv'
df_ac_log = pd.read_csv(access_log_location_url)

# Drop uplicates
df_ac_log = df_ac_log.drop_duplicates()

df_ac_log['Dt'] = df_ac_log['Date'].str.split(' ').str[0]

# Convert the 'date' column to datetime
df_ac_log['date'] = pd.to_datetime(df_ac_log['Dt'])

# Extract the day of the week
df_ac_log['day_of_week'] = df_ac_log['date'].dt.day_name()
df_ac_log['weekday'] = df_ac_log['date'].dt.weekday

df_ac_log["AddToCart"] = df_ac_log["url"].str.contains("add_to_cart").astype(int) # str.extract("(add_to_cart)")
#df_ac_log.head()

# Reassign the IP address to IDs (make it easier later for creating edges)

ipaddrs = df_ac_log['ip'].unique()
new_ip_ids = list(range(len(df_ac_log['ip'].unique())))
map_ip = dict(zip(ipaddrs, new_ip_ids))
print(type(map_ip))

df_ac_log['ip_id'] = df_ac_log['ip'].map(map_ip)

print(len(ipaddrs))
#df_ac_log.head()



# Reassign the Product to IDs (make it easier later for creating edges)

products = df_ac_log['Product'].unique()
new_prod_ids = list(range(len(df_ac_log['Product'].unique())))
map_prod = dict(zip(products, new_prod_ids))
print(type(map_prod))

df_ac_log['Product_Id'] = df_ac_log['Product'].map(map_prod)

#### Category
# Reassign the Category to IDs

cats = df_ac_log['Category'].unique()
new_cat_ids = list(range(len(df_ac_log['Category'].unique())))
map_cat = dict(zip(cats, new_cat_ids))

df_ac_log['Category_Id'] = df_ac_log['Category'].map(map_cat)

# Reassign the Dept to IDs

depts = df_ac_log['Department'].unique()
new_dept_ids = list(range(len(df_ac_log['Department'].unique())))
map_dept = dict(zip(depts, new_dept_ids))

df_ac_log['Department_Id'] = df_ac_log['Department'].map(map_dept)

#Split date

df_ac_log['date_id'] = df_ac_log['Date'].str.split('/').str[1]
df_ac_log['month_id'] = df_ac_log['Date'].str.split('/').str[0]
df_ac_log['year_id'] = df_ac_log['Date'].str.split('/').str[2].str.split(' ').str[0]

df_ac_log['time'] = df_ac_log['Date'].str.split(' ').str[1]


df_ac_log['Dt'] = df_ac_log['Date'].str.split(' ').str[0]

# Convert the 'date' column to datetime
df_ac_log['date'] = pd.to_datetime(df_ac_log['Dt'])

# Extract the day of the week
df_ac_log['day_of_week'] = df_ac_log['date'].dt.day_name()
df_ac_log['weekday'] = df_ac_log['date'].dt.weekday

# Convert 'numbers' column to integer
df_ac_log['date_id'] = df_ac_log['date_id'].astype(int)
df_ac_log['month_id'] = df_ac_log['month_id'].astype(int)
df_ac_log['year_id'] = df_ac_log['year_id'].astype(int)

# Find outlier by Date - IP, if any

from datetime import datetime, timedelta
print(type(df_ac_log['date']))
df_ac_log['date1']= pd.to_datetime(df_ac_log['date'])

# group dataframe by date1 and get counts per day
date_counts = df_ac_log.groupby(['date1']).size().reset_index(name='count')
#date_counts = df_ac_log.groupby('date1').size()
print(date_counts.head(50))

# find max count in panda series date_count
max_count = date_counts['count'].max()
print(max_count)

min_count = date_counts['count'].min()
print(min_count)

# group date into week ids in df_session_log
df_ac_log['week_id'] = df_ac_log['date'].dt.isocalendar().week

# create year_month_id based on year_id and month_id
df_ac_log['year_month_id'] = df_ac_log['year_id'].astype(int)*100 + df_ac_log['month_id'].astype(int)

# remove rows with 20170914 date1 from df_ac_log due to unusual spike
df_ac_log = df_ac_log[df_ac_log['date1'] != '2017-09-14']

print(df_ac_log.shape)

# Reassign the year_month_id to snapshot_ID (make it easier later for splitting graphs)

year_months = df_ac_log['year_month_id'].unique()
new_year_months = list(range(len(df_ac_log['year_month_id'].unique())))
map_snapshot = dict(zip(year_months, new_year_months))
print(type(map_snapshot))

df_ac_log['snapshot_id'] = df_ac_log['year_month_id'].map(map_snapshot)

# Merge date and ip columns to create new column in df_ac_log
df_ac_log['date_ip_id'] = df_ac_log['date'].astype(str) + '_' + df_ac_log['ip_id'].astype(str)

access_count = df_ac_log.groupby("ip_id")["date_id"].count().rename("access_count")
print(access_count)

"""Encoding data"""

!pip install sentence-transformers

#install torch
!pip install torch

import torch
from sentence_transformers import SentenceTransformer

class SequenceEncoder:
     def __init__(self, model_name='all-MiniLM-L6-v2', device=None):
         self.device = device
         self.model = SentenceTransformer(model_name, device=device)

     @torch.no_grad()
     def __call__(self, df):
         x = self.model.encode(df.values, show_progress_bar=True,
                               convert_to_tensor=True, device=self.device)
         return x.cpu()

encoder = SequenceEncoder() # Create an instance of the SequenceEncoder class

# subset the dataframe
df_cat = df_ac_log[['Category']]
df_cat = df_cat.drop_duplicates()
print(df_cat.shape)


category_xs = [encoder(df_cat['Category'])] # Encode the 'Category' column
category_x = torch.cat(category_xs, dim=-1) # Concatenate the encoded results
print(category_x.shape)

# subset the dataframe
df_prod = df_ac_log[['Product']]
df_prod = df_prod.drop_duplicates()
print(df_prod.shape)

product_xs = [encoder(df_prod['Product'])] # Encode the 'Category' column
product_x = torch.cat(product_xs, dim=-1) # Concatenate the encoded results
print(product_x.shape)
print(product_x)

#Alternate encoder

from sklearn.preprocessing import LabelEncoder

# Label encode the 'Category' column
label_encoder = LabelEncoder()

# get unique category names
category_labels = label_encoder.fit_transform(df_ac_log['Category'].unique())
#print(category_labels)
print(len(category_labels))
category_labels_tensor = torch.tensor(category_labels, dtype=torch.long).view(-1, 1)
print(category_labels_tensor)



# group by date1 and ip to create unique session ids
df_ac_log['Session_ID'] = df_ac_log.groupby(['date1', 'ip_id']).ngroup()

#Sort the dataframe by Session_ID and time
df_ac_log = df_ac_log.sort_values(by=['Session_ID', 'time'])

df_ac_log.head()
df_ac_log.info()



df_ac_log.head()

"""# Product based analysis"""

import pandas as pd

# subset df_ac_log by Session_id, time, AddToCart For Product based analysis
df_session_log = df_ac_log[['Session_ID', 'snapshot_id', 'ip_id','date', 'time', 'AddToCart', 'Product', 'Product_Id', 'Department', 'Department_Id']]

# add interaction column based on AddToCart , if 0 then 'view' else 1 then 'buy'
df_session_log['interaction'] = df_session_log['AddToCart'].apply(lambda x: 'view' if x == 0 else 'buy')

# sort dataframe by Session_ID and time
df_session_log = df_session_log.sort_values(by=['Session_ID', 'time'])

# add timestamp based on date and time
df_session_log['timestamp'] = pd.to_datetime(df_session_log['date'].astype(str) + ' ' + df_session_log['time'].astype(str))

# create timestamp using  pd.to_datetime
df_session_log['timestamp'] = pd.to_datetime(df_session_log['timestamp'])

# add time_spend based on time difference for same Session_Id
# Calculate time differences within each session
# Move the Timespent up by 1 row during calculation
import pandas as pd

# subset df_ac_log by Session_id, time, AddToCart For Product based analysis
df_session_log = df_ac_log[['Session_ID', 'snapshot_id', 'ip_id','date', 'time', 'AddToCart', 'Product', 'Product_Id', 'Department', 'Department_Id']]

# add interaction column based on AddToCart , if 0 then 'view' else 1 then 'buy'
df_session_log['interaction'] = df_session_log['AddToCart'].apply(lambda x: 'view' if x == 0 else 'buy')

# sort dataframe by Session_ID and time
df_session_log = df_session_log.sort_values(by=['Session_ID', 'time'])

# add timestamp based on date and time
df_session_log['timestamp'] = pd.to_datetime(df_session_log['date'].astype(str) + ' ' + df_session_log['time'].astype(str))

# create timestamp using  pd.to_datetime
df_session_log['timestamp'] = pd.to_datetime(df_session_log['timestamp'])

# add time_spend based on time difference for same Session_Id
# Calculate time differences within each session
# Move the Timespent up by 1 row during calculation

df_session_log['PrevTimeSpent'] = df_session_log.groupby('Session_ID')['timestamp'].diff().dt.total_seconds()
df_session_log['TimeSpent'] = df_session_log.groupby('Session_ID')['PrevTimeSpent'].shift(-1)

# Calculate user transitions
df_session_log['NextProduct_Id'] = df_session_log.groupby('Session_ID')['Product_Id'].shift(-1)
df_session_log['NextProduct'] = df_session_log.groupby('Session_ID')['Product'].shift(-1)


# Aggregate interaction sequences for each session
df_session_log['InteractionSequence'] = df_session_log.groupby('Session_ID')['interaction'].transform(lambda x: ' -> '.join(x))

df_session_log.head(15)

df_session_log['InteractionSequence'] = df_session_log.groupby('Session_ID')['interaction'].transform(lambda x: ' -> '.join(x))

df_session_log.head(15)

# Drop rows where there is no transition
transitions = df_session_log.dropna(subset=['NextProduct'])

"""# Create the Product graph"""

#import networkX
import networkx as nx
import matplotlib.pyplot as plt

# Create the graph
G = nx.DiGraph()  # Use a directed graph

# Add edges with weights
for _, row in transitions.iterrows():
    G.add_edge(
        row['Product_Id'],
        row['NextProduct'],
        weight=row['TimeSpent']
    )

# Print graph info
print(G)

#print node
print(G.nodes())

# print source in G
print(G.nodes())
print(type(G.nodes()))
# num nodes
print(G.number_of_nodes())

# Check if nodes '7' and '60' exist
if 'Garmin Forerunner 910XT GPS Watch' in G.nodes and 'SOLE E25 Elliptical' in G.nodes:
    print("Both nodes exist.")
else:
    print("One or both nodes are missing.")

"""
for node in G.nodes():
    print(node)
    print(type(node))

"""

# Define node positions
pos = nx.spring_layout(G, seed=42)  # Generate layout

# Draw nodes
nx.draw_networkx_nodes(G, pos, node_size=3000, node_color='skyblue', alpha=0.8)

# Draw edges
nx.draw_networkx_edges(G, pos, edgelist=G.edges(), arrowstyle='->', arrowsize=20, edge_color='gray')

# Draw labels for nodes and edges
nx.draw_networkx_labels(G, pos, font_size=10, font_color="black", font_weight="bold")
edge_labels = nx.get_edge_attributes(G, 'weight')
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color="red")

# Title and plot
plt.title("Product Transition Graph with Time Spent")
plt.axis("off")
plt.show()

# print connecting nodes
print(G.edges())

"""Centrality Measure"""

# Degree centrality (popularity of products)
degree_centrality = nx.degree_centrality(G)
print("Degree Centrality:", degree_centrality)

# Weighted in-degree (total time spent coming into a product)
weighted_in_degree = {node: sum(weight for _, _, weight in G.in_edges(node, data='weight')) for node in G.nodes()}
print("Weighted In-Degree:", weighted_in_degree)

# Sort edges by weight
sorted_edges = sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)
print("Most Common Transitions:", sorted_edges[:5])

"""Path analysis"""

# Shortest path between two products (if meaningful)
# Check if a path exists before trying to find the shortest path
if nx.has_path(G, source="Nike Men's Dri-FIT Victory Golf Polo", target="Garmin Forerunner 910XT GPS Watch"):
    shortest_path = nx.shortest_path(G, source="Nike Men's Dri-FIT Victory Golf Polo", target="Garmin Forerunner 910XT GPS Watch", weight='weight')
    print("Shortest Path:", shortest_path)
else:
    print("No path exists between the specified products.")

"""Co-View Products in the same session"""

!pip install torch_geometric

import torch
from torch_geometric.utils.convert import from_networkx  # Import from_networkx
from torch_geometric.data import Data  # Import Data class

# Create a co-purchase/view graph
G = nx.Graph()

# Group by session and create edges between products in the same session
for _, group in df_session_log.groupby('Session_ID'):
    products = list(group['Product_Id'])
    for i in range(len(products)):
        for j in range(i + 1, len(products)):
            if G.has_edge(products[i], products[j]):
                G[products[i]][products[j]]['weight'] += 1  # Increment weight
            else:
                G.add_edge(products[i], products[j], weight=1)

# Convert NetworkX graph to PyTorch Geometric graph
data_pg = from_networkx(G)

# Check if the graph has edges
if data_pg.edge_index is None or data_pg.edge_index.size(1) == 0:
    # If no edges, handle the case appropriately:
    # Instead of creating an empty edge_index, add a self-loop to each node.
    # This ensures that edge_index is not empty and allows train_test_split_edges to function.
    num_nodes = len(G.nodes())
    data_pg.edge_index = torch.arange(num_nodes).repeat(2, 1) # Create self-loops for all nodes
    # Create dummy edge attributes for the self-loops
    data_pg.edge_attr = torch.ones(num_nodes, dtype=torch.float)

# Existing code for edge_attr population:
# This part needs modification to handle the added self-loops.
# We will concatenate the existing edge attributes with the dummy attributes for self-loops.
edge_attr = []
for i in range(data_pg.edge_index.shape[1]): #Iterate over number of edges in edge_index
    u, v = data_pg.edge_index[0, i].item(), data_pg.edge_index[1, i].item()
    # Check if the edge is a self-loop (u == v) and use the dummy attribute if so
    if u == v:
        edge_attr.append(1.0)  # Dummy attribute for self-loops
    else:
        edge_attr.append(G[u][v]['weight']) #Append weight from networkx graph

data_pg.edge_attr = torch.tensor(edge_attr, dtype=torch.float) #Assign edge attributes to data_pg

print(data_pg.edge_index)
print(data_pg.edge_attr)

"""GAE Encoder"""

from torch_geometric.nn import GCNConv
from torch_geometric.nn import GAE

class GCNEncoder(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super(GCNEncoder, self).__init__()
        self.conv1 = GCNConv(in_channels, 16)  # Hidden layer
        self.conv2 = GCNConv(16, out_channels)  # Latent space

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index).relu()
        x = self.conv2(x, edge_index)
        return x

# Initialize the GAE model
# If data_pg.x doesn't exist or is None, set in_channels to 1
in_channels = data_pg.x.size(1) if data_pg.x is not None else 1
encoder = GCNEncoder(in_channels=in_channels, out_channels=2)
model = GAE(encoder)

import torch
# Get the number of nodes in the graph.
num_nodes = data_pg.num_nodes
# If data_pg.x is None, create a tensor of ones with shape [num_nodes, in_channels].
x = torch.ones((num_nodes, in_channels), dtype=torch.float) if data_pg.x is None else data_pg.x

import torch
from torch_geometric.loader import DataLoader
from torch_geometric.utils import train_test_split_edges
from torch_geometric.transforms import RandomLinkSplit

# Check if edge_index exists before applying RandomLinkSplit:
if not hasattr(data_pg, 'edge_index') or data_pg.edge_index is None:
    # If edge_index is missing, create it using the existing edges
    edge_index = torch.tensor(list(G.edges()), dtype=torch.long).t().contiguous()
    data_pg.edge_index = edge_index

# Apply RandomLinkSplit
transform = RandomLinkSplit(is_undirected=True,
                                   num_val=0.1, num_test=0.1,
                                   add_negative_train_samples=False,
                                   neg_sampling_ratio=1.0)

train_data, val_data, test_data = transform(data_pg)
# Access the training edges directly using 'train_pos_edge_index'
train_pos_edge_index = train_data.edge_index  # Positive training edges

# Get the number of nodes in the graph.
num_nodes = train_data.num_nodes  # Use num_nodes from train_data

# If data_pg.x is None, create a tensor of ones with shape [num_nodes, in_channels].
# Assuming in_channels is determined as before
in_channels = train_data.x.size(1) if train_data.x is not None else 1
x = torch.ones((num_nodes, in_channels), dtype=torch.float) if train_data.x is None else train_data.x


# Define optimizer and loss function
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Training loop
model.train()
for epoch in range(100):  # Number of epochs
    optimizer.zero_grad()
    # Encode the graph using training edges
    z = model.encode(x, train_pos_edge_index)
    # Calculate reconstruction loss using training edges
    loss = model.recon_loss(z, train_pos_edge_index)
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss.item()}")

"""Extract and Analyze Embeddings"""

model.eval()
with torch.no_grad():
    # Use the same x you used during training if data_pg.x is None
    x = data_pg.x if data_pg.x is not None else torch.ones((data_pg.num_nodes, in_channels), dtype=torch.float)
    z = model.encode(x, data_pg.edge_index) #  Use data_pg.edge_index instead of data_pg.train_pos_edge_index

# Convert embeddings to a DataFrame for analysis
embeddings = pd.DataFrame(z.numpy(), index=list(G.nodes()))
print(embeddings)

# Identify high-performance products
# Example: Products with high norm in latent space
embeddings['norm'] = embeddings.apply(lambda row: (row**2).sum()**0.5, axis=1)
high_performance = embeddings.sort_values('norm', ascending=False)
print("High-Performance Products:", high_performance)

"""Visualize Product relationship"""

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# Reduce embeddings to 2D
tsne_embeddings = TSNE(n_components=2, random_state=42).fit_transform(z.numpy())

# Plot the embeddings
plt.figure(figsize=(10, 6))
plt.scatter(tsne_embeddings[:, 0], tsne_embeddings[:, 1], s=50, alpha=0.8)
for i, product in enumerate(G.nodes()):
    plt.annotate(product, (tsne_embeddings[i, 0], tsne_embeddings[i, 1]))
plt.title("Product Embeddings")
plt.xlabel("Dimension 1")
plt.ylabel("Dimension 2")
plt.show()