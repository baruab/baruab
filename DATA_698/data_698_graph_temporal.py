# -*- coding: utf-8 -*-
"""DATA_698_Graph_temporal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/150EboMj4CwXBQ2w5SP_j39z9m7baXjxk

Construct multiple views of graph
"""

#!pip uninstall pandas
#!pip show pandas
#!pip install pandas==1.3.5

#!pip install --upgrade pandas
!pip install google-colab geopandas plotnine statsmodels xarray
!pip install pandas google-colab geopandas plotnine statsmodels xarray

"""# Load the dataset"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
#to ignore warnings
import warnings
warnings.filterwarnings('ignore')

## In the interest of saving time, calling the API multiple times, as the information is of static nature
##   the generated file with added columns is saved and read directly to save compute time

access_log_location_url = 'https://raw.githubusercontent.com/baruab/baruab/refs/heads/main/DATA_698/tokenized_access_logs_global.csv'
df_ac_log = pd.read_csv(access_log_location_url)

print("Pandas version:", pd.__version__)

# Drop uplicates
df_ac_log = df_ac_log.drop_duplicates()

"""# Adding Features to the data"""

df_ac_log['Dt'] = df_ac_log['Date'].str.split(' ').str[0]

# Convert the 'date' column to datetime
df_ac_log['date'] = pd.to_datetime(df_ac_log['Dt'])

# Extract the day of the week
df_ac_log['day_of_week'] = df_ac_log['date'].dt.day_name()
df_ac_log['weekday'] = df_ac_log['date'].dt.weekday

df_ac_log["AddToCart"] = df_ac_log["url"].str.contains("add_to_cart").astype(int) # str.extract("(add_to_cart)")
#df_ac_log.head()

# Reassign the IP address to IDs (make it easier later for creating edges)

ipaddrs = df_ac_log['ip'].unique()
new_ip_ids = list(range(len(df_ac_log['ip'].unique())))
map_ip = dict(zip(ipaddrs, new_ip_ids))
print(type(map_ip))

df_ac_log['ip_id'] = df_ac_log['ip'].map(map_ip)

print(len(ipaddrs))
#df_ac_log.head()

# Reassign the Product to IDs (make it easier later for creating edges)

products = df_ac_log['Product'].unique()
new_prod_ids = list(range(len(df_ac_log['Product'].unique())))
map_prod = dict(zip(products, new_prod_ids))
print(type(map_prod))

df_ac_log['Product_Id'] = df_ac_log['Product'].map(map_prod)

#### Category
# Reassign the Category to IDs

cats = df_ac_log['Category'].unique()
new_cat_ids = list(range(len(df_ac_log['Category'].unique())))
map_cat = dict(zip(cats, new_cat_ids))

df_ac_log['Category_Id'] = df_ac_log['Category'].map(map_cat)

# Reassign the Dept to IDs

depts = df_ac_log['Department'].unique()
new_dept_ids = list(range(len(df_ac_log['Department'].unique())))
map_dept = dict(zip(depts, new_dept_ids))

df_ac_log['Department_Id'] = df_ac_log['Department'].map(map_dept)

#mapping = {index: i for i, index in enumerate(df_ac_cat_subset.index.unique())}

"""**Split the date time string into date & time represented as numbers**"""

df_ac_log['date_id'] = df_ac_log['Date'].str.split('/').str[1]
df_ac_log['month_id'] = df_ac_log['Date'].str.split('/').str[0]
df_ac_log['year_id'] = df_ac_log['Date'].str.split('/').str[2].str.split(' ').str[0]

df_ac_log['time'] = df_ac_log['Date'].str.split(' ').str[1]


df_ac_log['Dt'] = df_ac_log['Date'].str.split(' ').str[0]

# Convert the 'date' column to datetime
df_ac_log['date'] = pd.to_datetime(df_ac_log['Dt'])

# Extract the day of the week
df_ac_log['day_of_week'] = df_ac_log['date'].dt.day_name()
df_ac_log['weekday'] = df_ac_log['date'].dt.weekday

# Convert 'numbers' column to integer
df_ac_log['date_id'] = df_ac_log['date_id'].astype(int)
df_ac_log['month_id'] = df_ac_log['month_id'].astype(int)
df_ac_log['year_id'] = df_ac_log['year_id'].astype(int)

# group date into week ids in df_session_log
df_ac_log['week_id'] = df_ac_log['date'].dt.isocalendar().week

# create year_month_id based on year_id and month_id
df_ac_log['year_month_id'] = df_ac_log['year_id'].astype(int)*100 + df_ac_log['month_id'].astype(int)

# Reassign the year_month_id to snapshot_ID (make it easier later for splitting graphs)

year_months = df_ac_log['year_month_id'].unique()
new_year_months = list(range(len(df_ac_log['year_month_id'].unique())))
map_snapshot = dict(zip(year_months, new_year_months))
print(type(map_snapshot))

df_ac_log['snapshot_id'] = df_ac_log['year_month_id'].map(map_snapshot)

# Merge date and ip columns to create new column in df_ac_log
df_ac_log['date_ip_id'] = df_ac_log['date'].astype(str) + '_' + df_ac_log['ip_id'].astype(str)

access_count = df_ac_log.groupby("ip_id")["date_id"].count().rename("access_count")
print(access_count)

"""Create Encoders"""

!pip install sentence-transformers

#install torch
!pip install torch

import torch
from sentence_transformers import SentenceTransformer

class SequenceEncoder:
     def __init__(self, model_name='all-MiniLM-L6-v2', device=None):
         self.device = device
         self.model = SentenceTransformer(model_name, device=device)

     @torch.no_grad()
     def __call__(self, df):
         x = self.model.encode(df.values, show_progress_bar=True,
                               convert_to_tensor=True, device=self.device)
         return x.cpu()

encoder = SequenceEncoder() # Create an instance of the SequenceEncoder class

# subset the dataframe
df_cat = df_ac_log[['Category']]
df_cat = df_cat.drop_duplicates()
print(df_cat.shape)


category_xs = [encoder(df_cat['Category'])] # Encode the 'Category' column
category_x = torch.cat(category_xs, dim=-1) # Concatenate the encoded results
print(category_x.shape)

# subset the dataframe
df_prod = df_ac_log[['Product']]
df_prod = df_prod.drop_duplicates()
print(df_prod.shape)

product_xs = [encoder(df_prod['Product'])] # Encode the 'Category' column
product_x = torch.cat(product_xs, dim=-1) # Concatenate the encoded results
print(product_x.shape)
print(product_x)

print(category_x)
print(category_x.shape)

"""**Alternate Label encoder**"""

from sklearn.preprocessing import LabelEncoder

# Label encode the 'Category' column
label_encoder = LabelEncoder()

# get unique category names
category_labels = label_encoder.fit_transform(df_ac_log['Category'].unique())
#print(category_labels)
print(len(category_labels))
category_labels_tensor = torch.tensor(category_labels, dtype=torch.long).view(-1, 1)
print(category_labels_tensor)

"""# Define the node features"""

#Let's create the User Node, create a subset dataframe.
# Add user city, access count and buy intent count

access_count = df_ac_log.groupby("ip_id")["ip_id"].count().rename("access_count")
buy_count = df_ac_log[df_ac_log["AddToCart"] == 1].groupby("ip_id")["AddToCart"].count().rename("buy_count")
user_node_features = pd.concat([access_count, buy_count], axis=1)

# Remap user ID
user_node_features = user_node_features.reset_index(drop=False)
user_node_features.head()
user_id_mapping = user_node_features['ip_id']

# Only keep user features
#user_node_features = user_node_features.drop('ip_id', axis=1)
user_node_features.head()


df_ip_city = df_ac_log[['ip_id', 'City', 'State', 'Country']]
df_ip_city = df_ip_city.drop_duplicates()
df_ip_city.head()

# merge user_node_features and df_ip_city by ip_id
df_user_features = pd.merge(user_node_features, df_ip_city, on='ip_id')
df_user_features = df_user_features.drop_duplicates()

# Only keep user features
#df_user_features = df_user_features.drop('ip_id', axis=1)
#df_user_features.head()
print(df_user_features.shape)

# torch cat the features
#torch access_count
access_cnt = df_user_features['access_count']
access_cnt = torch.tensor(access_cnt, dtype=torch.float).view(-1, 1)
print(access_cnt.shape)

#torch city
df_user_features['City'] = df_user_features['City'].astype(str) # Convert cities to a list of strings
df_user_features['State'] = df_user_features['State'].astype(str) # Convert cities to a list of strings

# torch cities
city_xs =  [encoder(df_user_features['City'])]
city_x = torch.cat(city_xs, dim=-1) # Concatenate the encoded results
print(city_x.shape)

# torch states
state_xs =  [encoder(df_user_features['State'])]
state_x = torch.cat(state_xs, dim=-1) # Concatenate the encoded results
print(state_x.shape)

user_features = torch.cat([city_x, state_x], dim=1)
print(user_features.shape)

"""Create Product features"""

# get unique department names

df_product_depts = df_ac_log[['Product_Id', 'Department_Id']]
df_product_depts = df_product_depts.drop_duplicates()
df_product_depts.head()

# convert x into tensor
product_features = torch.tensor(df_product_depts['Product_Id'], dtype=torch.float).view(-1,1)
print(product_features.shape)

#dept_labels = label_encoder.fit_transform(df_product_depts['Department_Id'])

dept_labels = df_product_depts['Department_Id']
#print(dept_labels)
#print(len(dept_labels))

dept_labels_tensor = torch.tensor(dept_labels, dtype=torch.float).view(-1, 1)
product_y = dept_labels_tensor

#print(product_y)
print(product_y.shape)

!pip install torch-geometric torch-sparse torch-scatter

# Import the necessary library
from torch_geometric.data import HeteroData

user_ids = torch.tensor(df_user_features['ip_id'].values, dtype=torch.long)
user_y = torch.tensor(df_user_features['buy_count'].values, dtype=torch.long).view(-1,1)

hdata = HeteroData()
hdata.snapshot_id = torch.tensor(df_ac_log['snapshot_id'].unique(), dtype=torch.long)
hdata.year_month_id = torch.tensor(df_ac_log['year_month_id'].unique(), dtype=torch.long)
hdata.week_id = torch.tensor(df_ac_log['week_id'].unique(), dtype=torch.long)

hdata['user'].num_nodes = len(user_node_features)
hdata['user'].x = city_x
hdata['user'].ids = user_ids
hdata['user'].y = user_y   # buy_count

product_ids = torch.tensor(df_product_depts['Product_Id'].values, dtype=torch.long)
hdata['product'].num_nodes = len(df_product_depts)
hdata['product'].x = product_x
hdata['product'].ids = product_ids
hdata['product'].y = product_y   # dept label

print(hdata)
print(hdata.snapshot_id)
print(hdata.year_month_id)
print(hdata.week_id)

"""# Create Temporal Datasets"""

from google.colab import files
uploaded = files.upload()  # Upload the file from your local machine

!mv tsagcn.py /usr/local/lib/python3.10/dist-packages/torch_geometric_temporal/nn/attention

#######
#create a two dimensional array

!pip install torch-geometric-temporal

import pandas as pd
import torch
import torch_geometric
#import torch_geometric_temporal

print("Pandas version:", pd.__version__)
print("Torch version:", torch.__version__)
print("PyTorch Geometric version:", torch_geometric.__version__)
#print("PyTorch Geometric Temporal version:", torch_geometric_temporal.__version__)

"""
from torch_geometric_temporal.signal import temporal_signal_split, DynamicGraphTemporalSignal
import torch

# 1. Prepare temporal snapshots
# Example: Assume you have processed your data into time-wise snapshots
num_time_steps = 10  # Number of temporal snapshots
node_features = []   # List to store node features for each time step
edge_indices = []    # List to store edge indices for each time step
edge_attrs = []      # List to store edge attributes (optional)
labels = []          # List to store target labels for each time step

for t in range(num_time_steps):
    # Extract features, edges, etc., for time t from HeteroData
    node_features.append({
        'user': hdata['user'].x,        # User features for time t
        'product': hdata['product'].x   # Product features for time t
    })
    edge_indices.append({
        ('user', 'buy', 'product'): hdata[('user', 'buy', 'product')].edge_index,  # Edges
        ('product', 'rev_buy', 'user'): hdata[('product', 'rev_buy', 'user')].edge_index
    })
    # Optional: edge attributes or labels
    edge_attrs.append({
        ('user', 'buy', 'product'): hdata[('user', 'buy', 'product')].edge_attr,
        ('product', 'rev_buy', 'user'): hdata[('product', 'rev_buy', 'user')].edge_attr
    })
    labels.append(hdata['user'].y)  # Assume labels are user-focused

# 2. Create a DynamicGraphTemporalSignal object
temporal_data = DynamicGraphTemporalSignal(
    edge_indices=edge_indices,
    edge_features=edge_attrs,  # Optional
    node_features=node_features,
    targets=labels  # Optional, for supervised tasks
)

# 3. Split into train and test datasets
train_dataset, test_dataset = temporal_signal_split(temporal_data, train_ratio=0.8)

# Print stats
print(f"Train snapshots: {len(train_dataset)}")
print(f"Test snapshots: {len(test_dataset)}")

# 4. Access data for each time step during training
for snapshot in train_dataset:
    x_t = snapshot.node_features  # Node features at time t
    edge_index_t = snapshot.edge_index  # Edge indices at time t
    edge_attr_t = snapshot.edge_features  # Edge attributes at time t (if available)
    y_t = snapshot.target  # Target labels at time t (if available)
"""

#from google.colab import files
#uploaded = files.upload()  # Upload the file from your local machine

#!mv tsagcn.py /usr/local/lib/python3.10/dist-packages/torch_geometric_temporal/nn/attention

import torch # Import the torch module
from datetime import datetime # Import the datetime module from the datetime library

# Seperate by buy vs view using AddToCart flag
buy_edge_index=[]
buy_timestamp=[]

#Iterate the dataframe
for index, row in df_ac_log.iterrows():

  timestamp = datetime.strptime(row["Date"],'%m/%d/%Y %H:%M')
  ts_unix = int(timestamp.timestamp())
  buy_edge_index.append([row["ip_id"], row["Product_Id"]])
  buy_timestamp.append(ts_unix)

# Convert to tensor and add to HeteroData
buy_edge_index = torch.tensor(buy_edge_index, dtype=torch.long).t().contiguous()
buy_timestamp = torch.tensor(buy_timestamp, dtype=torch.long).view(-1, 1)
hdata['user', 'buy', 'product'].edge_index = buy_edge_index
### add edge_type attribute
hdata['user', 'buy', 'product'].edge_type = 'buy'
hdata['user', 'buy', 'product'].edge_time = buy_timestamp

# torch AddToCart from df_ac_log
add_to_cart_labels = torch.tensor(df_ac_log['AddToCart'].values, dtype=torch.long)
hdata['user', 'buy', 'product'].edge_label = add_to_cart_labels


# Add reverse relation edge index for 'product' -> 'user'
hdata['product', 'rev_buy', 'user'].edge_index = hdata['user', 'buy', 'product'].edge_index.flip(0)

# Optionally, add edge attributes for the reverse relation
# If `edge_attr` is present for 'buys' edges, you can reuse it for 'rev_buys'
if 'edge_attr' in hdata['user', 'buy', 'product']:
    hdata['product', 'rev_buy', 'user'].edge_attr = hdata['user', 'buy', 'product'].edge_attr

print(hdata)
print(buy_edge_index)
print(buy_edge_index.shape)

print(hdata)
print(buy_edge_index)
print(buy_edge_index.shape)

"""Prepare temporal snapshots"""

# Split the dataframe by session id, date and ip address simulating temporal behaviour of the users on the website

""" DON'T DELETE will uncomment later  """
def split_dataframe_by_column(df, column_name):
    dataframes = []
    for value in df[column_name].unique():
        dataframes.append(df[df[column_name] == value])
    return dataframes

split_dfs = split_dataframe_by_column(df_ac_log, 'date_ip_id')
print(len(split_dfs))

print(split_dfs[0]['date_id'])

print(split_dfs[0].info())

# get unique department names

df_product_depts = df_ac_log[['Product_Id', 'Department_Id']]
df_product_depts = df_product_depts.drop_duplicates()
df_product_depts.head()

# convert x into tensor
product_features = torch.tensor(df_product_depts['Product_Id'], dtype=torch.float).view(-1,1)
print(product_features.shape)

#dept_labels = label_encoder.fit_transform(df_product_depts['Department_Id'])

dept_labels = df_product_depts['Department_Id']
#print(dept_labels)
#print(len(dept_labels))

dept_labels_tensor = torch.tensor(dept_labels, dtype=torch.float).view(-1, 1)
product_y = dept_labels_tensor

#print(product_y)
print(product_y.shape)

"""# Create DynamicGraphTemporalSignal object"""

# Initialize buy_edge_index as an empty list outside the loop
buy_edge_index = []
session_hdata_list = []

node_features=[]
edge_indices=[]
edge_weights=[]
edge_labels=[]
labels=[]

# Iterate the Split_df list
#for i in range(len(split_dfs)):
#  46556 max number of sessions

for i in range(1,50):
  buy_edge_index = []

  split_dfs[i] = split_dfs[i].sort_values(by='time')
  # Convert 'time' column to datetime objects
  split_dfs[i]['time'] = pd.to_datetime(split_dfs[i]['time'])
  split_dfs[i]['avg_req_duration'] = (max(split_dfs[i]['time']) - min(split_dfs[i]['time'])) / len(split_dfs[i])
  split_dfs[i]['num_requests'] = len(split_dfs[i])
  # num buys based on AddToCart
  split_dfs[i]['num_buys'] = split_dfs[i]['AddToCart'].sum()
  # num views is num_requests minus num_buys
  split_dfs[i]['num_views'] = split_dfs[i]['num_requests'] - split_dfs[i]['num_buys']

  split_dfs[i]['exit_product_id'] = split_dfs[i]['Product_Id'].tail(1).values[0]
  split_dfs[i]['exit_buy'] = split_dfs[i]['AddToCart'].tail(1).values[0]
  split_dfs[i]['entry_product_id'] = split_dfs[i]['Product_Id'].head(1).values[0]
  split_dfs[i]['entry_buy'] = split_dfs[i]['AddToCart'].head(1).values[0]

  #### Let's start with the Product Node, create a subset dataframe.
  # It will have Product_Id, Category, Department, url
  df_temp_product_nodes = split_dfs[i][['Product_Id', 'Category']]
  df_temp_product_nodes = df_temp_product_nodes.drop_duplicates()
  df_temp_product_nodes = df_temp_product_nodes.reset_index(drop=True)
  #print("!!! df_temp_product_nodes !!!")
  #print(df_temp_product_nodes)

  # Filter df_product_features by Product_Id
  df_temp_product_features = df_product_depts[df_product_depts['Product_Id'].isin(df_temp_product_nodes['Product_Id'])]
  #print(df_temp_product_features)

  # Create the user node and features
  df_temp_user_node = split_dfs[i][['date_ip_id']]
  df_temp_user_node = df_temp_user_node.drop_duplicates()
  df_temp_user_node = df_temp_user_node.reset_index(drop=True)
  # print("!!!! df_temp_user_node !!!")
  # print(df_temp_user_node)


  avg_req_duration = split_dfs[i]['avg_req_duration'].mean()
  num_requests = split_dfs[i]['num_requests'].mean()
  num_buys = split_dfs[i]['num_buys'].mean()
  num_views = split_dfs[i]['num_views'].mean()

  exit_product_id = split_dfs[i]['exit_product_id'].mean()

  # Convert avg_req_duration to a numeric representation before concatenation
  avg_req_duration_seconds = avg_req_duration.total_seconds()

  # Create a DataFrame from the individual features
  df_temp_user_features = pd.DataFrame({
    'avg_req_duration': [avg_req_duration_seconds],
    'num_requests': [num_requests],
    'num_buys': [num_buys],
    'num_views': [num_views],
    'exit_product_id': [exit_product_id]
  })
  #print("!!!! df_temp_user_features !!!")
  #print(df_temp_user_features)


  # Edge / edge index
  #temp_edge_index = split_dfs[i][["AddToCart"]].values.transpose()
  #temp_edge_index = torch.tensor(temp_edge_index, dtype=torch.float)
  #print("!!!! temp_edge_index !!!")
  #print(temp_edge_index)
  #print(temp_edge_index.shape)

  num_edges = len(split_dfs[i])
  for j in range(num_edges):
    # if AddToCart == 1 then add the edge
    if split_dfs[i]["AddToCart"].iloc[j] == 1:
      buy_edge_index.append([split_dfs[i]["ip_id"].iloc[j], split_dfs[i]["Product_Id"].iloc[j]]) # Access element by index j

  num_buys_in_session = len(buy_edge_index)
  temp_edge_index = torch.tensor(buy_edge_index, dtype=torch.long).t().contiguous()
  print(temp_edge_index)
  print(temp_edge_index.shape)


  ##### Build the dataset
  temp_data = HeteroData()

  # Set the number of nodes for 'user' and 'product'
  temp_data.date_id = split_dfs[i]['date_id'].unique() # df_temp_user_node['ip_id']
  temp_data['user'].num_nodes = len(df_temp_user_features) # Assuming user_node_features is a list or array of user features
  temp_data['user'].id = split_dfs[i]['ip_id'].unique() # df_temp_user_node['ip_id']
  temp_data['user'].session_id = split_dfs[i]['date_ip_id'].unique() # df_temp_user_node['ip_id']
  temp_data['user'].x = df_temp_user_features
  temp_data['user'].y = num_buys


  temp_data['product'].x = df_temp_product_features
  temp_data['product'].id = split_dfs[i]['Product_Id'].unique()
  temp_data['product'].num_nodes = len(df_temp_product_features) # Assuming df_product_features is a DataFrame or array of product features
  temp_data['user', 'buy', 'product'].edge_index = temp_edge_index
  temp_data['user', 'buy', 'product'].edge_type = 'buy'
  temp_data['user', 'buy', 'product'].edge_label = num_buys_in_session
  temp_data['product', 'rev_buy', 'user'].edge_index = temp_edge_index.flip(0)
  temp_data['product', 'rev_buy', 'user'].edge_label = num_buys_in_session

  session_hdata_list.append(temp_data)

  print("~~~ num edges ~~~ ", num_edges)
  print("~~~ tempdata ~~~ ", i)
  #print(temp_data)

  node_features.append({
      'user': temp_data['user'].x,
      'product': temp_data['product'].x
  })
  edge_indices.append(temp_edge_index)
  edge_weights.append(num_buys_in_session)

  edge_labels.append({
      ('user', 'buy', 'product'): temp_data[('user', 'buy', 'product')].edge_label,
      ('product', 'rev_buy', 'user'): temp_data[('product', 'rev_buy', 'user')].edge_label
  })

  labels.append(temp_data['user'].y)
  labels = [torch.tensor([label], dtype=torch.float) for label in labels]  # Convert each label to a tensor
  print("~~~~ ", i)
#print(node_features)

# Concatenate user and product features into a single tensor for each snapshot
node_features_list = []
for snapshot_features in node_features:
    user_features = snapshot_features['user']  # Assuming 'user' key exists
    product_features = snapshot_features['product']  # Assuming 'product' key exists

    # Convert to tensors if necessary
    user_features_tensor = torch.tensor(user_features.values, dtype=torch.float) if isinstance(user_features, pd.DataFrame) else torch.tensor(user_features, dtype=torch.float)
    product_features_tensor = torch.tensor(product_features.values, dtype=torch.float) if isinstance(product_features, pd.DataFrame) else torch.tensor(product_features, dtype=torch.float)


    # If either user or product features have two dimensions, reshape to one dimension.
    user_features_tensor = user_features_tensor.reshape(-1) if user_features_tensor.dim() == 2 else user_features_tensor
    product_features_tensor = product_features_tensor.reshape(-1) if product_features_tensor.dim() == 2 else product_features_tensor

    # Concatenate user and product features
    all_features = torch.cat([user_features_tensor, product_features_tensor])

    node_features_list.append(all_features)

!pip install torch_geometric_temporal
import numpy as np
import torch
from torch_geometric_temporal.signal import temporal_signal_split, DynamicGraphTemporalSignal
import pandas as pd

from torch_geometric_temporal.signal import temporal_signal_split, DynamicGraphTemporalSignal
import torch

edge_labels_tensors = []
for edge_label_dict in edge_labels:
    # Extract edge labels for ('user', 'buy', 'product') relation
    edge_label = edge_label_dict[('user', 'buy', 'product')]
    # Convert to tensor and append
    edge_labels_tensors.append(torch.tensor([edge_label], dtype=torch.float))

temporal_data = DynamicGraphTemporalSignal(
    edge_indices=edge_indices,
    edge_weights=edge_weights,
    edge_features=edge_labels_tensors,
    features=node_features_list,
    targets=labels
)


# Split into train and test datasets
train_dataset, test_dataset = temporal_signal_split(temporal_data, train_ratio=0.8)

print(train_dataset)
print(type(train_dataset))

# Convert targets to a NumPy array
train_dataset.targets = [np.array(target) for target in train_dataset.targets]

# Ensure targets are NumPy arrays
train_dataset.targets = [np.array(target) for target in train_dataset.targets]
test_dataset.targets = [np.array(target) for target in test_dataset.targets]

# Now iterate over snapshots
for snapshot in train_dataset:
    x_t = snapshot.features  # Node features at time t
    edge_index_t = snapshot.edge_index  # Edge indices at time t
    y_t = snapshot.targets  # Targets at time t

    # Check if x_t is a tensor, and if so, get its dtype kind
    feature_kind = x_t.dtype.kind if isinstance(x_t, torch.Tensor) else None

    # Print the shape with the feature kind if it's a tensor
    print(f"Features: {x_t.shape}, Edge Indices: {edge_index_t.shape}, Targets: {y_t.shape}, Feature Kind: {feature_kind}")
"""
for snapshot in train_dataset:
    x_t = snapshot.features  # Node features at time t
    edge_index_t = snapshot.edge_index  # Edge indices at time t
    edge_weight_t = snapshot.edge_weight  # Edge weights at time t
    y_t = snapshot.target  # Target labels at time t

"""


# Print stats
#print(f"Train snapshots: {len(train_dataset.snapshots)}")
#print(f"Test snapshots: {len(test_dataset.snapshots)}")

"""Define Training and Evaluation Functions

# Create Train/Validation/Test Splits
"""

from sklearn.model_selection import train_test_split

# Split edge indices and labels
edge_indices = hdata['user', 'buy', 'product'].edge_index
labels = hdata['user', 'buy', 'product'].edge_label

# Split indices
train_idx, test_idx = train_test_split(range(edge_indices.size(1)), test_size=0.2, random_state=42)
train_idx, val_idx = train_test_split(train_idx, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2

# Create masks
train_mask = torch.zeros(edge_indices.size(1), dtype=torch.bool)
val_mask = torch.zeros(edge_indices.size(1), dtype=torch.bool)
test_mask = torch.zeros(edge_indices.size(1), dtype=torch.bool)

train_mask[train_idx] = True
val_mask[val_idx] = True
test_mask[test_idx] = True

# Assign masks and split labels
hdata['user', 'buy', 'product'].train_mask = train_mask
hdata['user', 'buy', 'product'].val_mask = val_mask
hdata['user', 'buy', 'product'].test_mask = test_mask