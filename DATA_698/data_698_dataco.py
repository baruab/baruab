# -*- coding: utf-8 -*-
"""DATA_698_DataCo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cKzkjrrp0R_u4phK8KRp2nVYKuFeE8ie

**DATA 698**


---


E-Commerce website traffic analysis using Graph Neural Networks

Data Exploration on the dataset from DataCo and related files

Importing the libraries
"""

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

import torch

def format_pytorch_version(version):
  return version.split('+')[0]

TORCH_version = torch.__version__
TORCH = format_pytorch_version(TORCH_version)

def format_cuda_version(version):
  return 'cu' + version.replace('.', '')

CUDA_version = torch.version.cuda
CUDA = format_cuda_version(CUDA_version)

print('PyTorch version:', TORCH)
print('CUDA version:', CUDA)

"""
import torch

def format_pytorch_version(version):
  return version.split('+')[0]

TORCH_version = torch.__version__
TORCH = format_pytorch_version(TORCH_version)

def format_cuda_version(version):
  return 'cu' + version.replace('.', '')

CUDA_version = torch.version.cuda
CUDA = format_cuda_version(CUDA_version)

!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html
!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html
#!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html
#!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html
!pip install torch-geometric-temporal -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html
!pip install torch-geometric
#!pip install torch-geometric==2.0.4

"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
#to ignore warnings
import warnings
warnings.filterwarnings('ignore')

"""# Import the dataset"""

# Access Log data
access_log_url = 'https://raw.githubusercontent.com/baruab/baruab/refs/heads/main/DATA_698/DataCo/tokenized_access_logs.csv'
df_ac_log = pd.read_csv(access_log_url)
df_ac_log.head()

# Supply Chain data
#sc_url = 'https://raw.githubusercontent.com/baruab/baruab/refs/heads/main/DATA_698/DataCo/DataCoSupplyChainDataset.csv'
#df_sc = pd.read_csv(sc_url , encoding='latin1')
#df_sc.head()

df_ac_log.info()

#df_sc.info()

#df_ac_log.describe()

print(min(df_ac_log['Date']))
print(max(df_ac_log['Date']))

#df_sc.describe()

"""Check for Duplication"""

df_ac_log.duplicated().sum()

# Drop uplicates
df_ac_log = df_ac_log.drop_duplicates()

df_ac_log.duplicated().sum()

"""Unique values in Access Log"""

df_ac_log.nunique()

#df_sc.nunique()

"""Check for missing values in dataset"""

df_ac_log.isnull().sum()

#df_sc.isnull().sum()

"""
Add geo features to the data based on IP address information. Map the IP address with Country, State and City."""

#!pip install ipinfo
#import ipinfo
#import json
#import requests

#json_url = 'https://raw.githubusercontent.com/baruab/baruab/refs/heads/main/DATA_698/DataCo/iso-3166-2.json'

#try:
#    response = requests.get(json_url)  # Fetch the JSON data from the URL
#    response.raise_for_status()  # Raise an exception for bad responses
#    state_abbr_data = json.loads(response.text)  # Load JSON from the response text
#except requests.exceptions.RequestException as e:
#    print(f"Error fetching JSON data: {e}")


#access_token = '8f8f1378474839'  # Optional, but added for more data  8f8f1378474839
#handler = ipinfo.getHandler(access_token)
#count = 0 # Added to tally the number

#def get_state_abbr(country_code, state_name):
#    country_data = state_abbr_data.get(country_code, {})
#    divisions = country_data.get("divisions", {})

#   for abbr, full_name in divisions.items():
#        if full_name.lower() == state_name.lower():
#            return abbr.split('-')[-1]

#    return " - "

#def get_location(ip):
#    global count
#    try:
#        count += 1
#        details = handler.getDetails(ip)

#        city = details.city if details.city else None
#        country = details.country if details.country else None  # ISO country code
#        country_name = details.country_name if details.country_name else None
#        state = details.region if details.region else None

#        if state and country:
#            state_abv = get_state_abbr(country, state)

 #       print(f"Processing IP {count}: {ip} -> City: {city}, Country: {country}, State: {state}")
 #       return city, country_name, state, country, state_abv
 #   except Exception as e:
 #       print(f"Exception for IP {ip}: {e}")
 #       return None, None, None, None, None


## In the interest of saving time, calling the API multiple times, as the information is of static nature
##   the generated file with added columns is saved and read directly to save compute time

# df_ac_log['City'], df_ac_log['Country'], df_ac_log['State'],  df_ac_log['Country_Code'], df_ac_log['State_Code'] = zip(*df_ac_log['ip'].apply(get_location))
#output_file = 'access_logs_with_state_cntry.csv'
#df_ac_log.to_csv(output_file, index=False)
#df_ac_log.head()

#access_log_location_url = 'https://raw.githubusercontent.com/baruab/baruab/refs/heads/main/DATA_698/access_logs_with_location_codes.csv'
#df_ac_log = pd.read_csv(access_log_location_url)

## In the interest of saving time, calling the API multiple times, as the information is of static nature
##   the generated file with added columns is saved and read directly to save compute time

access_log_location_url = 'https://raw.githubusercontent.com/baruab/baruab/refs/heads/main/DATA_698/tokenized_access_logs_global.csv'
df_ac_log = pd.read_csv(access_log_location_url)

df_ac_log.nunique()

df_ac_log = df_ac_log.drop_duplicates()
df_ac_log.duplicated().sum()

"""# Feature Engineering

Let's split the date time string into date & time represented as numbers to create indexes, if needed
"""

df_ac_log['date_id'] = df_ac_log['Date'].str.split('/').str[1]
df_ac_log['month_id'] = df_ac_log['Date'].str.split('/').str[0]
df_ac_log['year_id'] = df_ac_log['Date'].str.split('/').str[2].str.split(' ').str[0]

df_ac_log['time'] = df_ac_log['Date'].str.split(' ').str[1]

#df_ac_log.head()

print(len(df_ac_log))

"""Converting Date to add Day Of the Week in Access Log"""

df_ac_log['Dt'] = df_ac_log['Date'].str.split(' ').str[0]

# Convert the 'date' column to datetime
df_ac_log['date'] = pd.to_datetime(df_ac_log['Dt'])

# Extract the day of the week
df_ac_log['day_of_week'] = df_ac_log['date'].dt.day_name()
df_ac_log['weekday'] = df_ac_log['date'].dt.weekday
#df_ac_log.head()

"""**Data Sorting and Grouping**

Sort the access log by IP address, date to understand the sequence of web clicks
"""

df_ac_log.sort_values(['ip'], ascending=[True])  #, 'date', 'time'

#df_ac_log.head(30)

#df_ac_log.groupby(['ip']).count()

"""Let's seperate Numerical and Categorical variables"""

cat_cols = df_ac_log.select_dtypes(include=['object']).columns.tolist()
num_cols = df_ac_log.select_dtypes(include=np.number).columns.tolist()
print("Categorical Variables:")
print(cat_cols)
print("Numerical Variables:")
print(num_cols)

"""Converting string number values to integers"""

# Convert 'numbers' column to integer
df_ac_log['date_id'] = df_ac_log['date_id'].astype(int)
df_ac_log['month_id'] = df_ac_log['month_id'].astype(int)
df_ac_log['year_id'] = df_ac_log['year_id'].astype(int)
num_cols = df_ac_log.select_dtypes(include=np.number).columns.tolist()
print("Numerical Variables:")
print(num_cols)

"""# EDA Univariate Analysis

Below are some histogram and box plots showing the pattern of variables using the Access Log information
"""

for col in num_cols:
  print(col)
  print('Skew:', round(df_ac_log[col].skew(),2))
  print('Kurtosis:', round(df_ac_log[col].kurtosis(),2))

  plt.figure(figsize=(15,4))
  plt.subplot(1,2,1)
  sns.histplot(data=df_ac_log, x=col, kde=True)
  plt.subplot(1,2,2)
  sns.boxplot(data=df_ac_log, x=col)
  plt.show()

"""The month value is left skewed, the access traffic spikes during mid of the week and mid of the month."""

fig, axes = plt.subplots(6,1, figsize=(36,36))
fig.suptitle('Bar plot for all categorical variables in the dataset')

sns.countplot(ax = axes[0], x = 'Product', data = df_ac_log, color='blue',
              order=df_ac_log['Product'].value_counts().index);
sns.countplot(ax = axes[1], x = 'Category', data = df_ac_log, color='blue',
              order=df_ac_log['Category'].value_counts().index);
sns.countplot(ax = axes[2], x = 'Department', data = df_ac_log, color='blue',
              order=df_ac_log['Department'].value_counts().index);
sns.countplot(ax = axes[3], x = 'City', data = df_ac_log, color='blue',
              order=df_ac_log['City'].value_counts().index);
sns.countplot(ax = axes[4], x = 'State', data = df_ac_log, color='blue',
              order=df_ac_log['State'].value_counts().index);
sns.countplot(ax = axes[5], x = 'Country', data = df_ac_log, color='blue',
              order=df_ac_log['Country'].value_counts().index);

axes[1].tick_params(labelrotation=45);
axes[2].tick_params(labelrotation=90);
axes[3].tick_params(labelrotation=90);

axes[4].tick_params(labelrotation=90);
axes[5].tick_params(labelrotation=90);
plt.show()

"""Looking at the chart above, Product Category, Department, State are relatively fewer unique values, so worth looking into further.

**Data Transformation**

"""

## 'Hour', 'date_id', 'month_id', 'year_id', 'weekday'
## The numerical values are skewed, log transformation can help in normalization

# Function for log transformation of the column
def log_transform(data, col):
  for colname in col:
    if (data[colname] == 1.0).all():
      data[colname + '_log'] = np.log(data[colname] + 1)
    else:
      data[colname + '_log'] = np.log(data[colname])
  #data.info()

log_transform(df_ac_log, ['Hour', 'date_id', 'month_id', 'weekday'])


# Replacing inf with a large finite value and -inf with a small finite value
df_ac_log.replace([np.inf, -np.inf], [np.finfo(np.float64).max, np.finfo(np.float64).min], inplace=True)


#Log transformation of the features
#sns.distplot(df_ac_log['Hour_log'], axlabel='Hour_log')
sns.distplot(df_ac_log['date_id_log'], axlabel='date_id_log')
sns.distplot(df_ac_log['month_id_log'], axlabel='month_id_log')
#sns.distplot(df_ac_log['weekday_log'], axlabel='weekday_log')
plt.show()

# Find outlier by Date - IP, if any

from datetime import datetime, timedelta
print(type(df_ac_log['date']))
df_ac_log['date1']= pd.to_datetime(df_ac_log['date'])

# group dataframe by date1 and ip and get counts per day
date_counts = df_ac_log.groupby(['date1', 'ip']).size().reset_index(name='count')
#date_counts = df_ac_log.groupby('date1').size()
print(date_counts)

# find max count in panda series date_count
max_count = date_counts['count'].max()
print(max_count)

min_count = date_counts['count'].min()
print(min_count)

# Reassign the IP address to IDs (make it easier later for creating edges)

ipaddrs = df_ac_log['ip'].unique()
new_ip_ids = list(range(len(df_ac_log['ip'].unique())))
map_ip = dict(zip(ipaddrs, new_ip_ids))
print(type(map_ip))

df_ac_log['ip_id'] = df_ac_log['ip'].map(map_ip)

print(len(ipaddrs))
df_ac_log.head()

#df_ac_log.head()

# group by date1 and ip to create unique session ids
df_ac_log['Session_ID'] = df_ac_log.groupby(['date1', 'ip_id']).ngroup()
df_ac_log.head()
df_ac_log.info()

# group by date1 to create unique date1_ID
df_ac_log['Date1_ID'] = df_ac_log.groupby(['date1']).ngroup()
df_ac_log.head()

"""**Encoding the categorical variables**

Unique Elements in Categorical Columns
"""

print(len(df_ac_log))

df_ac_log.info()

##df_ac_US_log= df_ac_log

"""
print('Product: ' + df_ac_US_log['Product'].unique())
print('Category: ' + df_ac_US_log['Category'].unique())
print('Department: ' + df_ac_US_log['Department'].unique())
print('City: ' + df_ac_US_log['City'].unique())
print('State: ' + df_ac_US_log['State'].unique())
print('Country: ' + df_ac_US_log['Country'].unique())
"""

df_ac_log['ip'].value_counts()

#
# Access Log of Top 5 IP address to be selected

top_ips = df_ac_log['ip'].value_counts().head(5)

#print(top_ips.index.tolist())

# Top IP dataframe to study
#df_top_ip_access = df_ac_log[df_ac_log.ip.isin(top_ips.index.tolist())]

#df_top_ip_access.head(10)

"""IP address access breakdown by date and the buying intent"""

print(len(df_ac_log))
df_ac_log.info()

#df_top_ip_access.value_counts(['ip', 'date','AddToCart'])

df_ac_log['Product'].value_counts()

df_ac_log['Department'].value_counts()

#df_ac_US_log['State'].value_counts()

print(min(df_ac_log['Dt']))
print(max(df_ac_log['Dt']))

"""As there are a lot of categorical columns with many uniques values, let's subset the dataframe"""

df_ac_cat_subset = df_ac_log[["Category", "Department"]]

df_ac_cat_subset['Department'].value_counts()

print(len(df_ac_cat_subset))

#one hot encoding using OneHotEncoder of Scikit-Learn

from sklearn.preprocessing import OneHotEncoder


#Extract categorical columns from the dataframe
#Here we extract the columns with object datatype as they are the categorical columns
categorical_columns = df_ac_cat_subset.select_dtypes(include=['object']).columns.tolist()

#Initialize OneHotEncoder
encoder = OneHotEncoder(sparse_output=False)

# Apply one-hot encoding to the categorical columns
one_hot_encoded = encoder.fit_transform(df_ac_cat_subset[categorical_columns])

#Create a DataFrame with the one-hot encoded columns
#We use get_feature_names_out() to get the column names for the encoded data
one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(categorical_columns))

# Concatenate the one-hot encoded dataframe with the original dataframe
df_encoded = pd.concat([df_ac_cat_subset, one_hot_df], axis=1)

# Drop the original categorical columns
df_encoded = df_encoded.drop(categorical_columns, axis=1)

# Display the resulting dataframe
print(f"Encoded data : \n{df_encoded}")

"""**Tabular data -> Heterogeneous Graph Data**

Identify:
Nodes (Product, User, Location ...)
Edges (Interactions)
Node features(Attributes)
Label (Node level, Edge level, Graph level)
Timesteps (Interval time)
Temporal graph shape: static or dynamic? What is changing over time?

Identifying buying intent in the access log by searching for 'add_to_cart' related url links. This will be an edge feature in the graph.
"""

df_ac_log["AddToCart"] = df_ac_log["url"].str.contains("add_to_cart").astype(int) # str.extract("(add_to_cart)")
#df_ac_log.head()

# Reassign the Product to IDs (make it easier later for creating edges)

products = df_ac_log['Product'].unique()
new_prod_ids = list(range(len(df_ac_log['Product'].unique())))
map_prod = dict(zip(products, new_prod_ids))
print(type(map_prod))

df_ac_log['Product_Id'] = df_ac_log['Product'].map(map_prod)

df_ac_log.head()

# Reassign the Category to IDs (make it easier later for creating labels)

cats = df_ac_log['Category'].unique()
new_cat_ids = list(range(len(df_ac_log['Category'].unique())))
map_cat = dict(zip(cats, new_cat_ids))
print(type(map_cat))

df_ac_log['Category_Id'] = df_ac_log['Category'].map(map_cat)

df_ac_log.head()

# Reassign the Dept to IDs (make it easier later for creating labels)

depts = df_ac_log['Department'].unique()
new_dept_ids = list(range(len(df_ac_log['Department'].unique())))
map_dept = dict(zip(depts, new_dept_ids))
print(type(map_dept))

df_ac_log['Department_Id'] = df_ac_log['Department'].map(map_dept)

df_ac_log.head()

df_ac_log.info()

"""**Based on the dataset, this will be a Heterogenous Graph comprising of Users(IP address) nodes and Product nodes, the edges will be represented as the buy intend (add_to_cart attribute)**


"""

#Let's start with the Product Node, create a subset dataframe.
#It will have Product_Id, Category, Department, url

df_product = df_ac_log[['Product_Id', 'Category_Id']] #,, 'Department' 'url']]
df_product.head()

# select Product node features
df_product = df_product.drop_duplicates()

df_product = df_product.reset_index(drop=True)
df_product.head()
print(len(df_product))

df_product.head()

# Create a dictionary to store node features
node_features = {}

# Iterate through the rows of the dataframe
for index, row in df_product.iterrows():
    # Get the product ID
    product_id = row['Product_Id']
    # Create a feature vector for the node
    features = {
        'Category': row['Category_Id']
    }
    # Store the feature vector in the dictionary
    node_features[product_id] = features

# Print the node features
#print(node_features)

# Access the features for a specific product ID
print(len(node_features))
print(node_features)

#one hot encoding the node feature using OneHotEncoder of Scikit-Learn

from sklearn.preprocessing import OneHotEncoder


#Extract categorical columns from the dataframe
#Here we extract the columns with object datatype as they are the categorical columns
categorical_columns = df_product.select_dtypes(include=['object']).columns.tolist()

#Initialize OneHotEncoder
encoder = OneHotEncoder(sparse_output=False)

# Apply one-hot encoding to the categorical columns
one_hot_encoded = encoder.fit_transform(df_product[categorical_columns])

#Create a DataFrame with the one-hot encoded columns
#We use get_feature_names_out() to get the column names for the encoded data
one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(categorical_columns))

# Concatenate the one-hot encoded dataframe with the original dataframe
df_encoded = pd.concat([df_product, one_hot_df], axis=1)

# Drop the original categorical columns
df_product_1hotfeatures = df_encoded.drop(categorical_columns, axis=1)

# Display the resulting dataframe
print(f"Encoded data : \n{df_product_1hotfeatures}")

df_product_1hotfeatures.head()

!pip install torch # Install torch if you haven't already

import torch # import torch module

# Convert to numpy
x = df_product['Category_Id']
print(x)
print(x.shape)

print(type(x))

# convert x into tensor
product_features = torch.tensor(x, dtype=torch.float).view(-1,1)
print(product_features)
print(product_features.shape)

"""As there are lot of Categories, one-hot encoding will not be effecient. Switching to Label encoding for Category
Extending the logic to encode Department too
"""

import torch
from sklearn.preprocessing import LabelEncoder

# Label encode the 'Category' column
label_encoder = LabelEncoder()

#Extract categorical columns from the dataframe
#Here we extract the columns with object datatype as they are the categorical columns
categorical_columns = df_ac_cat_subset.select_dtypes(include=['object']).columns.tolist()

#print(categorical_columns)

# get unique category names
category_labels = label_encoder.fit_transform(df_ac_cat_subset['Category'].unique())
#print(category_labels)
print(len(category_labels))
category_labels_tensor = torch.tensor(category_labels, dtype=torch.long).view(-1, 1)

product_cat_features = category_labels_tensor
#print(product_cat_features)

# get unique department names

df_product_depts = df_ac_log[['Product_Id', 'Department_Id']]
df_product_depts = df_product_depts.drop_duplicates()
df_product_depts.head()
#dept_labels = label_encoder.fit_transform(df_product_depts['Department_Id'])

dept_labels = df_product_depts['Department_Id']
#print(dept_labels)
#print(len(dept_labels))

dept_labels_tensor = torch.tensor(dept_labels, dtype=torch.float).view(-1, 1)
product_y = dept_labels_tensor

#print(product_y)
print(product_y.shape)
#data['product'].x = product_features

df_product_4features = df_ac_log[['Product','Product_Id','Department', 'Department_Id']]
# drop duplicates
df_product_4features = df_product_4features.drop_duplicates()
df_product_4features.head()

# torch cat the features
product_4features = torch.cat([product_features, product_y, product_features, product_y], dim=1)
#print(product_4features)
print(product_4features.shape)

# torch cat the product.Y features
product_Y4features = torch.cat([product_y, product_y, product_y, product_y], dim=1)
#print(product_4features)
print(product_Y4features.shape)

# subset df_product_4features using Product_Id
# subset based  Product_Id 16, 17
df_sel_products = df_product_4features[df_product_4features['Product_Id'].isin([16, 17,11,23,7,60,31,2,15,31,43,62,9])]
df_sel_products.head(15)

"""# **Create User Node and Features**"""

# group by ip_id and city
#df_ac_log.groupby(["ip_id", "city"]).count()

access_count = df_ac_log.groupby("ip_id")["date_id"].count().rename("access_count")
print(access_count)

access_count2 = df_ac_log.groupby("ip_id")["ip_id"].count().rename("access_count2")
print(access_count2.sum())

#Let's create the User Node, create a subset dataframe.
# Add user city, access count and buy intent count

access_count = df_ac_log.groupby("ip_id")["ip_id"].count().rename("access_count")
buy_count = df_ac_log[df_ac_log["AddToCart"] == 1].groupby("ip_id")["AddToCart"].count().rename("buy_count")
user_node_features = pd.concat([access_count, buy_count], axis=1)

# Remap user ID
user_node_features = user_node_features.reset_index(drop=False)
user_node_features.head()
user_id_mapping = user_node_features['ip_id']

# Only keep user features
#user_node_features = user_node_features.drop('ip_id', axis=1)
user_node_features.head()

print(access_count)
#print(len(user_node_features))

"""Create a subset dataframe of ip_id and City"""

df_ac_log.info()

df_ip_city = df_ac_log[['ip_id', 'City', 'State', 'Country']]
df_ip_city = df_ip_city.drop_duplicates()
df_ip_city.head()
print(len(df_ip_city))

# concat df_ip_city to user_node_features
# join user_node_features and df_ip_city by ip_id

df_user_features = pd.merge(user_node_features, df_ip_city, on='ip_id')

#df_user_features = pd.concat([user_node_features, df_ip_city], axis=1)

# subset df_user_features with unique ip_id
df_user_features = df_user_features.drop_duplicates()
df_user_features.head()
print(len(df_user_features))

#user_ids
user_ids = torch.tensor(df_user_features['ip_id'].values, dtype=torch.long)
print(user_ids)
print(user_ids.shape)

# Label encode City in df_user_features
label_encoder = LabelEncoder()
df_user_features['State'] = label_encoder.fit_transform(df_user_features['State'])
print(df_user_features['State'])
print(df_user_features['State'].shape)

# One-hot encode 'City'
city_one_hot = pd.get_dummies(df_user_features['City'])

# Convert access_count to a tensor
access_count_tensor = torch.tensor(df_user_features['access_count'].values, dtype=torch.float).view(-1, 1)

# Convert buy_count to a tensor
buy_count_tensor = torch.tensor(df_user_features['buy_count'].values, dtype=torch.float).view(-1, 1)

# Convert one-hot encoded features to tensors
city_tensor = torch.tensor(city_one_hot.values, dtype=torch.float)

# Concatenate access_count, buy_count and city tensors to form user features
user_features = torch.cat([access_count_tensor, buy_count_tensor, city_tensor], dim=1)

# Concatenate access_count, buy_count and city tensors to form user features
user_features = torch.cat([access_count_tensor], dim=1)

# label encoded 'State' from above

# convert it to a tensor
state_tensor = torch.tensor(df_user_features['State'].values, dtype=torch.float).view(-1,1)
print(state_tensor)
print(state_tensor.shape)

# convert it to a tensor
buy_count_tensor = torch.tensor(df_user_features['buy_count'].values, dtype=torch.float).view(-1,1)


unknown_class = -1  # Define a class for unknown labels

user_y = torch.cat([buy_count_tensor], dim=1)

#check isnan on user_y and replace by unknown_class
user_y[torch.isnan(user_y)] = unknown_class
print(user_y)
print(user_y.shape)

print(user_y)

#print(user_features)
print(user_features.shape)
#print(product_features)
print(product_features.shape)

### Additional product features

#print(len(df_product_1hotfeatures))
print(len(user_y))
print(user_y.shape)
print(user_y)

print(len(product_y))
print(product_y.shape)
#print(product_y)

"""# Initialize the HeteroData object"""

!pip install torch-geometric torch-sparse torch-scatter
# Import the necessary library
from torch_geometric.data import HeteroData

hdata = HeteroData()
hdata['user'].num_nodes = len(user_node_features)
hdata['user'].x = user_features
hdata['user'].ids = user_ids
hdata['user'].y = user_y


hdata['product'].num_nodes = len(df_product_1hotfeatures)
hdata['product'].x = product_features
product_ids = torch.tensor(df_product_1hotfeatures['Product_Id'].values, dtype=torch.long)
hdata['product'].ids = product_ids
hdata['product'].y = product_y

print(hdata)

# Convert to numpy
x = user_node_features.to_numpy()
print(x)
print(x.shape)

"""
import torch # Import the torch library

# Convert to PyTorch tensors
user_features = user_node_features.values.transpose()
user_features = torch.tensor(user_features, dtype=torch.long)

product_features = df_product_features.values.transpose()
product_features = torch.tensor(product_features, dtype=torch.long)

print(user_node_features)
#print(user_features)
print(user_features.shape)

#print(product_features)
#print(product_features.shape)
"""

user_node_features["buy_count"].hist()

# group by Dt and AddToCart sum to create View and Buy ratio by Day

# Merge date and ip columns to create new column in df_ac_log
df_ac_log['date_ip_id'] = df_ac_log['date'].astype(str) + '_' + df_ac_log['ip_id'].astype(str)
#df_ac_log.head()

df_ac_log.groupby(["Dt", "AddToCart"]).count()

# Add ratio of counts by AddToCart flag in a new column

df_ac_log['view_ratio'] = (
    df_ac_log.groupby(["date_ip_id", "AddToCart"])['Product'].transform('count') /
    df_ac_log.groupby(["date_ip_id"])['Product'].transform('count')
)

df_ac_log['buy_ratio'] = 1 - df_ac_log['view_ratio']
df_ac_log.head()

df_ac_log.info()
print(df_ac_log['Date'])

"""### Scratch work to understand data
## edge_index where AddToCart == 1
df_buy_edge= df_ac_log[df_ac_log["AddToCart"] == 1]
#df_buy_edge.head()

edge_index = df_buy_edge[["ip_id", "Product_Id", "Dt","AddToCart"]]
#edge_index.head()

# edge_index group by Dt count
edge_index.groupby("Dt").count()

#len(edge_index)


## edge_index where AddToCart == 0
df_view_edge= df_ac_log[df_ac_log["AddToCart"] == 0]
len(df_view_edge)

# edge_index group by Dt count where Dt is 9/1/2017 and AddToCart is 1

df_ac_log[(df_ac_log["Dt"] == "9/1/2017") & (df_ac_log["AddToCart"] == 0)].groupby("Dt").count()

Define and create the Edge relationships and interactions
Attributes used are ip_id(user), Product_Id, view_ratio, buy_ratio, AddToCart, Date
"""

# !pip install torch # Install the PyTorch library
import torch # Import the torch module

# Seperate by buy vs view using AddToCart flag
buy_edge_index=[]
buy_timestamp=[]

view_edge_index=[]
view_timestamp=[]

ratio_edge_index=[]

#Iterate the dataframe
for index, row in df_ac_log.iterrows():

  timestamp = datetime.strptime(row["Date"],'%m/%d/%Y %H:%M')
  ts_unix = int(timestamp.timestamp())
  ratio_edge_index.append([row["ip_id"], row["Product_Id"]])

  if row["AddToCart"] == 1:
    buy_edge_index.append([row["ip_id"], row["Product_Id"]])
    buy_timestamp.append(ts_unix)
  else:
    view_edge_index.append([row["ip_id"], row["Product_Id"]])
    view_timestamp.append(ts_unix)

# Convert to tensor and add to HeteroData
buy_edge_index = torch.tensor(buy_edge_index, dtype=torch.long).t().contiguous()
buy_timestamp = torch.tensor(buy_timestamp, dtype=torch.long)
hdata['user', 'buy', 'product'].edge_index = buy_edge_index
### add edge_type attribute
hdata['user', 'buy', 'product'].edge_type = 'buy'

hdata['user', 'buy', 'product'].edge_attr = buy_timestamp

view_edge_index = torch.tensor(view_edge_index, dtype=torch.long).t().contiguous()
view_timestamp = torch.tensor(view_timestamp, dtype=torch.long)
hdata['user', 'view', 'product'].edge_index = view_edge_index
### add edge_type attribute
hdata['user', 'view', 'product'].edge_type = 'view'
hdata['user', 'view', 'product'].edge_attr = view_timestamp

"""
buy_ratio = torch.tensor(df_ac_log['buy_ratio'].values, dtype=torch.float)
view_ratio = torch.tensor(df_ac_log['view_ratio'].values, dtype=torch.float)

# add edge index to user buy_ratio product
ratio_edge_index = torch.tensor(ratio_edge_index, dtype=torch.long).t().contiguous()

#add tensor for ratio_edge_index
ratio_edge_index = torch.tensor(ratio_edge_index, dtype=torch.long).t().contiguous()

hdata['user', 'buy_ratio', 'product'].edge_index = ratio_edge_index
# add edge_type
hdata['user', 'buy_ratio', 'product'].edge_type = 'buy_ratio'
hdata['user', 'buy_ratio', 'product'].edge_attr = buy_ratio

hdata['user', 'view_ratio', 'product'].edge_index = ratio_edge_index
# add edge_type
hdata['user', 'view_ratio', 'product'].edge_type = 'view_ratio'
hdata['user', 'view_ratio', 'product'].edge_attr = view_ratio
"""

print(hdata)
print(buy_edge_index)
print(buy_edge_index.shape)



"""
## edge_index where AddToCart == 1
df_buy_edge= df_ac_log[df_ac_log["AddToCart"] == 1]
df_buy_edge.head()

edge_index = df_buy_edge[["ip_id", "Product_Id"]].values.transpose()
edge_index = torch.tensor(edge_index, dtype=torch.long)
print(edge_index)
print(edge_index.shape)
"""

print(len(df_ac_log))

"""# Create the edge_types and edge_index
Splitting the Heterogeneous Graph into test and train
"""

edge_types = [('user', 'buy', 'product'), ('user', 'view', 'product')]

edge_index = torch.cat([hdata['user', 'buy', 'product'].edge_index, hdata['user', 'view', 'product'].edge_index], dim=1)

# Create a mapping from edge type to numerical index
edge_type_mapping = {
    ('user', 'buy', 'product'): 0,
    ('user', 'view', 'product'): 1,
}

# Create edge_type tensor based on the mapping
edge_type = torch.tensor([edge_type_mapping[tuple(et)] for et in edge_types], dtype=torch.long)

print(edge_type)
print(edge_index)
print(edge_index.shape)

"""Split the Graph Edges for link prediction"""

from torch_geometric.transforms import RandomLinkSplit
from torch_geometric.data import HeteroData

"""
edge_types = [('user', 'buy', 'product'), ('user', 'view', 'product')]
rev_edge_types = [('product', 'rev_buy', 'user'), ('product', 'rev_view', 'user')]  # Reverse edge types

# Perform a graph-aware split into train/val/test sets
transform = RandomLinkSplit(
    is_undirected=True,
    add_negative_train_samples=True,
    edge_types=edge_types,
    rev_edge_types=rev_edge_types  # Added rev_edge_types
)

train_data, val_data, test_data = transform(hdata)

#print number of nodes and edges in the graph
# Finding the number of nodes for each node type
for node_type in test_data.node_types:
    num_nodes = test_data[node_type].num_nodes
    print(f"Number of nodes in {node_type}: {num_nodes}")

# Iterate over the edge types in the training data and handle potential missing edge indices.
for edge_type in train_data.edge_types:
    train_edge_index = train_data[edge_type].get('edge_index')
    # Check if the train_edge_index is present for this edge type
    if train_edge_index is not None:
        train_edge_count = train_edge_index.size(1)
        print(f"Number of edges in train graph for {edge_type}: {train_edge_count}")
    else:
        print(f"Edge index is not available for edge type: {edge_type} in training data")

    # Similarly, handle missing edge indices for the testing data
    test_edge_index = test_data[edge_type].get('edge_index')
    if test_edge_index is not None:
        test_edge_count = test_edge_index.size(1)
        print(f"Number of edges in test graph for {edge_type}: {test_edge_count}")
    else:
        print(f"Edge index is not available for edge type: {edge_type} in testing data")
"""

"""Split the graph by node index for node classification"""

print(hdata['user'])

from torch_geometric.transforms import RandomNodeSplit
from torch_geometric.data import HeteroData
import torch

# Split the nodes for the "user" node type for a node classification task
transform = RandomNodeSplit(split="train_rest")  # num_classes is just an example
htdata = transform(hdata)

# Access the train_mask for the specific node type (e.g., 'user')
train_count = htdata['user'].train_mask.sum().item()
val_count = htdata['user'].val_mask.sum().item()
test_count = htdata['user'].test_mask.sum().item()

print(f"Training nodes: {train_count}")
print(f"Validation nodes: {val_count}")
print(f"Testing nodes: {test_count}")

# Access the train_mask for the specific node type (e.g., 'product')
train_product_count = htdata['product'].train_mask.sum().item()
val_product_count = htdata['product'].val_mask.sum().item()
test_product_count = htdata['product'].test_mask.sum().item()

print(f"Training nodes: {train_product_count}")
print(f"Validation nodes: {val_product_count}")
print(f"Testing nodes: {test_product_count}")

print(edge_index)
print(edge_index.shape)

"""# **Hurray ! We have created the Heterogeneous Graph, verified the node counts for User & Product which matches the inital dataframe unique count values**

The edge relationships are now defined, can be expanded upon later if needed
Also we have split the Graph for testing, training and evaluation purposes

Now let's define the GNN layers using our HeteroData
"""

print(edge_types)
print(type(edge_index))
print(edge_index.shape)

edge_index = torch.cat([hdata['user', 'buy', 'product'].edge_index, hdata['user', 'view', 'product'].edge_index], dim=1)

print(type(edge_index))
print(edge_index.shape)

# Create a mapping from edge type to numerical index
edge_type_mapping = {
    ('user', 'buy', 'product'): 0,
    ('user', 'view', 'product'): 1,
}

# Create edge_type tensor based on the mapping
edge_type = torch.tensor([edge_type_mapping[tuple(et)] for et in edge_types], dtype=torch.long)

print(edge_type)
print(edge_index)
print(edge_index.shape)

#print(user_features)
print(user_features.shape)

#print(hdata['user'].x)
print(hdata['user'].x.shape)

#print(hdata['product'].x)
print(hdata['product'].x.shape)

"""**RGCNConv (Relational Graph Convolutional Network)**

RGCNConv is designed to handle graphs with multiple edge types by parameterizing transformations based on relations. This layer aggregates neighborhood information based on the edge type, making it suitable for relational data.
"""

edge_index_dict = {
    ('user', 'buy', 'product'): hdata['user', 'buy', 'product'].edge_index,
    ('user', 'view', 'product'): hdata['user', 'view', 'product'].edge_index
}
print(edge_index_dict)
print(type(edge_index_dict))
print(edge_index_dict.keys())

# get size of dict
# iterate dict values
for key, value in edge_index_dict.items():
    print(key, value.shape)

# get values from dict using key
print(edge_index_dict[('user', 'buy', 'product')])

#print(edge_index_dict.shape)

print(type(edge_index_dict))
x_dict = {
    'user': hdata['user'].x,
    'product': hdata['product'].x
}


print(edge_index_dict)

# get the first element from edge_index_dict
edge_index = edge_index_dict[('user', 'buy', 'product')]
print(edge_index)
print(edge_index.shape)

import torch
import torch.nn.functional as F
from torch_geometric.nn import RGCNConv
from torch_geometric.data import HeteroData

class RGCN_HeteroGNN(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, num_relations):
        super(RGCN_HeteroGNN, self).__init__()

        # First RGCN layer
        self.conv1 = RGCNConv(in_channels, hidden_channels, num_relations=num_relations)

        # Second RGCN layer
        self.conv2 = RGCNConv(hidden_channels, out_channels, num_relations=num_relations)


    def forward(self, x_dict, edge_index_dict, edge_type):
        # Get the user node features
        # x = x_dict['user']

        # First convolutional layer with activation
        # Pass edge_index_dict and edge_type to RGCNConv
        # Slice edge_type to match the number of edges in the current relation
      #  num_edges = edge_index_dict[('user', 'buy', 'product')].shape[1]
       # edge_index = edge_index_dict[('user', 'buy', 'product')] # Get the specific edge_index for 'buy' relation

        # Access edge_index_dict using the correct key
        edge_index = edge_index_dict  # Use edge_index_dict directly

        # Get the number of edges from the edge index tensor
        # num_edges = edge_index.shape[1]  # No need to get num_edges here

        # edge_type_sliced = edge_type[:num_edges]  # No need to slice edge_type


        x = self.conv1(x_dict['user'], edge_index, edge_type)  # Pass edge_index_dict instead of edge_index
        x = F.relu(x)
        # print("edge_index:", edge_index)
        #print("num_edges:", num_edges)
        #print("edge_type_sliced =", edge_type_sliced)
        #print("Output after first RGCNConv layer:", x.shape)

        # Second convolutional layer
        # Pass edge_index_dict and edge_type to RGCNConv
        # Slice edge_type to match the number of edges in the current relation (assuming same as 'buy')
        x = self.conv2(x, edge_index, edge_type)  # Pass edge_index_dict instead of edge_index
        print("Output after second RGCNConv layer:", x.shape)

        return x


# Model parameters
# Assuming `data` is your HeteroData object with x_dict and edge_index_dict
in_channels = hdata['user'].x.size(-1)       # Input channels for user nodes

print("in_channels count =" , in_channels)
print("in_channels =" , hdata['user'].x)

hidden_channels = 16
out_channels = 8
num_relations = 1  # Two relations: viewed and bought

edge_type = []
for i in range(edge_index.shape[1]):  # Iterate through all edges
    if i < hdata['user', 'buy', 'product'].edge_index.shape[1]:
        edge_type.append(edge_type_mapping[('user', 'buy', 'product')])  # Type 0 for 'buy'
    else:
        edge_type.append(edge_type_mapping[('user', 'view', 'product')])  # Type 1 for 'view'
edge_type = torch.tensor(edge_type, dtype=torch.long)
print(edge_type)
print(edge_type.shape)

# Initialize model
model = RGCN_HeteroGNN(in_channels, hidden_channels, out_channels, num_relations)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
# Define the loss function
criterion = torch.nn.CrossEntropyLoss()

# Forward pass
model.train()
out = model(x_dict, edge_index, edge_type)
print("Final output:", out)
print(out.shape)
print("User node embeddings:" , out[0])
#print("Product node embeddings:" , out[1])


# evaluate out

edge_index_dict = {
    ('user', 'buy', 'product'): hdata['user', 'buy', 'product'].edge_index,
    ('user', 'view', 'product'): hdata['user', 'view', 'product'].edge_index
}
print(edge_index_dict)
print(type(edge_index_dict))
print(edge_index_dict.keys())

# get size of dict
# iterate dict values
for key, value in edge_index_dict.items():
    print(key, value.shape)

# get values from dict using key
print(edge_index_dict[('user', 'buy', 'product')])

#print(edge_index_dict.shape)

print(hdata['user'].y.shape)

"""# RGCN_HeteroGNN model training loop"""

# Define the number of epochs
num_epochs = 100


# Training loop
for epoch in range(num_epochs):
    model.train()  # Set the model to training mode

    # Zero the gradients
    optimizer.zero_grad()

    # Forward pass
    out = model(x_dict, edge_index, edge_type)  # Predict on 'user' nodes or specific node type

    # Select the target labels and nodes (for user nodes here)
    y_true = hdata['user'].y  # True labels for user nodes

    y_true = torch.clamp(y_true, 0, 7)

    # **Reshape the target tensor to 1D**
    y_true = y_true.squeeze(1)  # Remove the extra dimension


    # Calculate loss
    loss = criterion(out, y_true.long())  # Apply .long() to ensure target is LongTensor

    # Backward pass
    loss.backward()
    optimizer.step()

    #print accuracy
    _, predicted = torch.max(out, 1)
    accuracy = (predicted == y_true).sum().item() / len(y_true)
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}")

"""predictions contains the predicted class labels for each node."""

print(type(out))

assert isinstance(out, torch.Tensor), "Output should be a Tensor!"
predictions = out.argmax(dim=1)  # Get the class with the highest score for each node

print(predictions)
print(predictions.shape)

#max value in predictions
print(predictions.max())
print(predictions.min())

"""Edge Classification

edge_logit is the score for buy edge indicating the likelihood of the buy relation between the nodes.
"""

# Example for edge classification

# Assume you have the embeddings `out` from the trained model and the list of edges for which you want predictions:
edge_embeddings = out[edge_index[0]] * out[edge_index[1]]  # Dot product or other combination
edge_logits = torch.sigmoid(edge_embeddings.sum(dim=1))    # Sigmoid if binary classification

print(edge_logits)
print(edge_logits.shape)
# If using a more complex setup, you might pass `edge_embeddings` to another neural network layer

"""# F1 score calculation

from sklearn.metrics import precision_score, recall_score, f1_score

precision = precision_score(y_true, y_pred, average='weighted')
recall = recall_score(y_true, y_pred, average='weighted')
f1 = f1_score(y_true, y_pred, average='weighted')
print(f"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}")
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import torch

# Define the device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Map edge types to numerical indices
edge_type_mapping = {edge: idx for idx, edge in enumerate(hdata.edge_index_dict.keys())}

# Flatten edge indices and assign edge types
"""
edge_index_list = []
edge_type_list = []

for edge_type, edge_index in hdata.edge_index_dict.items():
    edge_index_list.append(edge_index)
    edge_type_list.append(torch.full((edge_index.size(1),), edge_type_mapping[edge_type]))

# Combine all edges and types
edge_index = torch.cat(edge_index_list, dim=1).to(device)
edge_types = torch.cat(edge_type_list, dim=0).to(device)
"""

# Debugging checks
print(f"Unique edge types: {torch.unique(edge_types)}")
print(f"Number of relation types: {len(edge_type_mapping)}")
print(f"Edge types: {edge_types}")

print(f"Edge index shape 111: {edge_index.shape}")
print(f"Edge types shape 111: {edge_types.shape}")

edge_index_list = []
edge_type_list = []

for edge_type, edge_index in hdata.edge_index_dict.items():
    edge_index_list.append(edge_index)
    edge_type_list.append(torch.full((edge_index.size(1),), edge_type_mapping[edge_type]))

# Combine edges and types
edge_index = torch.cat(edge_index_list, dim=1)
edge_types = torch.cat(edge_type_list, dim=0)

print(f"Edge index shape 222: {edge_index.shape}")
print(f"Edge types shape 222: {edge_types.shape}")

print(f"Unique edge types: {torch.unique(edge_types)}")

# Ensure edge types are within valid bounds
edge_types = torch.clamp(edge_types, 0, len(edge_type_mapping) - 1)

# Set model to evaluation mode
model.eval()

with torch.no_grad():
    # Move node features to the device
    x_dict = {key: value.to(device) for key, value in hdata.x_dict.items()}

    # Forward pass
    test_out = model(x_dict, edge_index, edge_types)

    # Predictions
    test_pred = test_out.argmax(dim=1)

# Extract true labels
test_y_true = hdata['user'].y.to(device)

# Filter out NaN labels (if necessary)
if torch.isnan(test_y_true).any():
    mask = ~torch.isnan(test_y_true)
    test_y_true = test_y_true[mask]
    test_pred = test_pred[mask]

# Convert tensors to numpy arrays for scikit-learn metrics
test_pred = test_pred.cpu().numpy()
test_y_true = test_y_true.cpu().numpy()

# Calculate metrics
accuracy = accuracy_score(test_y_true, test_pred)
precision = precision_score(test_y_true, test_pred, average='weighted')
recall = recall_score(test_y_true, test_pred, average='weighted')
f1 = f1_score(test_y_true, test_pred, average='weighted')

# Print metrics
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")

"""Link Prediction

Link scores contain predictions for likelihood of edges between selected pairs

"""

# Select pairs of nodes you want to evaluate for potential edges
## There are 5832820 links created; commenting the code below to save time

"""
node_pairs = [(i, j) for i in range(len(out)) for j in range(i+1, len(out))]  # Example pairs

print(len(node_pairs))
print(type(node_pairs))

# Compute link prediction scores
link_scores = []
for i, j in node_pairs:
    score = torch.dot(out[i], out[j])  # Dot product similarity for link prediction
    link_scores.append(score)

print(link_scores)
print(len(link_scores))
"""

print(out[0])

"""# Another HeteroGNN implementation, using SAGE convolution"""

# Import the necessary library
from torch_geometric.data import HeteroData

hp4data = HeteroData()
hp4data['user'].num_nodes = len(user_node_features)
hp4data['user'].x = user_features
hp4data['user'].ids = user_ids
hp4data['user'].y = user_y


hp4data['product'].num_nodes = len(df_product_1hotfeatures)
hp4data['product'].x = product_4features
product_ids = torch.tensor(df_product_1hotfeatures['Product_Id'].values, dtype=torch.long)
hp4data['product'].ids = product_ids
hp4data['product'].y =  product_Y4features # product_y

hp4data['user', 'buy', 'product'].edge_index = buy_edge_index

# Edge attribute AddToCart
hp4data['user', 'buy', 'product'].edge_attr = torch.tensor(df_ac_log['AddToCart'].values, dtype=torch.float)



print(hp4data)
print(hp4data['user'].x.size(0))
#print(hp4data.x_dict)

print(edge_index_dict)
print(hp4data)

print(hp4data.x_dict)
print(hp4data.edge_index_dict)

import torch
import torch.nn.functional as F
from torch_geometric.nn import HeteroConv, GCNConv, SAGEConv, GATConv
from torch_geometric.data import HeteroData

class HeteroGNN(torch.nn.Module):
    def __init__(self, user_in_channels, product_in_channels, hidden_channels, out_channels):
        super(HeteroGNN, self).__init__()

        # Define HeteroConv with different convolution layers for each edge type
        # Use SAGEConv for both edge types, as it supports bipartite message passing
        self.conv1 = HeteroConv({
            ('user', 'buy', 'product'): SAGEConv((user_in_channels, product_in_channels), hidden_channels),  # SAGEConv used here
            ('product', 'bought_by', 'user'): SAGEConv((user_in_channels, product_in_channels), hidden_channels)  # SAGEConv used here
        }, aggr='sum')

        self.conv2 = HeteroConv({
            ('user', 'buy', 'product'): SAGEConv((hidden_channels, hidden_channels), out_channels),  # SAGEConv used here
            ('product', 'bought_by', 'user'): SAGEConv((hidden_channels, hidden_channels), out_channels)  # SAGEConv used here
        }, aggr='sum')


    def forward(self, x_dict, edge_index_dict):
        # Apply the first layer of convolutions
        x_dict = self.conv1(x_dict, edge_index_dict)


        # Ensure all node types have features after the first layer
        # If a node type has no outgoing edges, its features might be None
        # In such cases, keep the original features
        for node_type in x_dict:
            if x_dict[node_type] is None:
                print(f"Warning: Features for node type '{node_type}' are None after the first layer.")

        x_dict = {key: F.relu(x) for key, x in x_dict.items()}  # Handle potential None values
        print("Output after first HeteroConv layer:", x_dict)

        # Apply the second layer of convolutions
        filtered_edge_index_dict = {}
        for edge_type, edge_index in edge_index_dict.items():
            src_type, _, dst_type = edge_type
            # Convert node type names to lowercase for consistency:
            src_type = src_type.lower()
            dst_type = dst_type.lower()

            if src_type in x_dict and dst_type in x_dict and x_dict[src_type] is not None and x_dict[dst_type] is not None:
                filtered_edge_index_dict[edge_type] = edge_index


        x_dict = self.conv2(x_dict, filtered_edge_index_dict)
        print("Output after second HeteroConv layer:", x_dict)

        return x_dict


in_channels_dict = {
    'user': hp4data['user'].x.size(1),  # Replace with the actual input feature dimension for 'user'
    'product': hp4data['product'].x.size(1)  # Replace with the actual input feature dimension for 'product' , Take the first dimension of the tensor for product input channels
}

# Assuming you have your HeteroData object prepared as `hdata`
hidden_channels = hp4data['user'].x.size(1)  # 32  # Example value for hidden channels
out_channels = hp4data['user'].y.size(1)  #  8 # For example, if doing 3-class classification

user_in_channels = hp4data['user'].x.size(0)
product_in_channels = hp4data['product'].x.size(0)

print(f"User in_channels: {user_in_channels}")
print(f"Product in_channels: {product_in_channels}")

# Initialize the model with the correct input channels:
model = HeteroGNN(user_in_channels, product_in_channels, hidden_channels=hidden_channels, out_channels=out_channels)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

print(model)

print("input channels =", user_in_channels)
print("product input channels =", product_in_channels)

print("hidden channels =", hidden_channels)
print("out channels =", out_channels)


# Forward pass
model.train()
out = model(hp4data.x_dict, hp4data.edge_index_dict)
print("Model output:", out)

"""**Build the 'Old version' Heterogeneous graph data object; May not be used ---**

!pip install torch_geometric

from torch_geometric.data import HeteroData

data = HeteroData()

# Set the number of nodes for 'user' and 'product'
data['user'].num_nodes = len(user_node_features) # Assuming user_node_features is a list or array of user features
data['product'].num_nodes = len(df_product_features) # Assuming df_product_features is a DataFrame or array of product features


data['user'].x = user_features   # user_node_features
data['product'].x = product_features  #df_product_features

#####
#data['user'].y = torch.tensor(df_ac_log['AddToCart'].values, dtype=torch.long)
#data['user'].train_mask = torch.ones(len(user_node_features), dtype=torch.bool)
#data['user'].test_mask = torch.zeros(len(user_node_features), dtype=torch.bool)
#data['user'].val_mask = torch.zeros(len(user_node_features), dtype=torch.bool)

data['user', 'buy', 'product'].edge_index = edge_index

clear_output = True
"
"""

### Visualize using NetworkX

# Simple example of network x rendering with colored nodes and edges
import matplotlib.pyplot as plt
import networkx as nx
from torch_geometric.utils import to_networkx
from torch_geometric.data import Data  # Import Data

#graph = to_networkx(hdata)

homogeneous_data = Data(
    x=hdata['user'].x,  # Node features for 'node_type_1'
    edge_index=hdata['user', 'buy', 'product'].edge_index,  # Edge index for the specified edge type
    y=None  # Optional: add target labels if available
)

# Convert the homogeneous Data object to NetworkX:
graph = to_networkx(homogeneous_data)

# Define colors for nodes and edges
node_type_colors = {
    "user": "#4599C3",
    "product": "#ED8546",
}

node_colors = []
labels = {}
num_users = hdata['user'].num_nodes
for node in graph.nodes():
    # Assuming nodes 0 to num_users - 1 are users, and the rest are products
    node_type = "user" if node < num_users else "product"
    color = node_type_colors[node_type]
    node_colors.append(color)
    if node_type == "user":
        labels[node] = f"U{node}"
    elif node_type == "product":
        labels[node] = f"P{node}"

# Define colors for the edges
edge_type_colors = {
    ("user", "buy", "product"): "#8B4D9E",
    ("user", "view", "product"): "#DFB825",
}

edge_colors = []
for from_node, to_node, attrs in graph.edges(data=True):
    # Get the edge type from the original HeteroData object
    edge_type = ('user', 'buy', 'product')  # Assuming all edges in homogeneous_data are of this type

    # Access the color for this type of edge:
    color = edge_type_colors[edge_type]

    # Set the color as an attribute within your networkx graph
    graph.edges[from_node, to_node]["color"] = color

    # Append to your list of edge colors
    edge_colors.append(color)


# Draw the graph

""" Commmented : Takes long to run
pos = nx.spring_layout(graph, k=2)
nx.draw_networkx(
    graph,
    pos=pos,
    labels=labels,
    with_labels=True,
    node_color=node_colors,
    edge_color=edge_colors,
    node_size=600,
)
plt.show()
"""

"""**Measuring parameters in a PyTorch Geometric (PyG) HeteroData graph**"""

import torch
import torch.nn as nn
from torch_geometric.nn import SAGEConv, HeteroConv

def count_parameters(model):
    """Counts the total number of trainable parameters in a PyTorch model.
    """
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

class HeteroGNN(nn.Module):
    def __init__(self, data):
        super().__init__()
        self.convs = nn.ModuleDict()
        # Create a mapping from node type names to numerical indices
        self.node_type_to_index = {node_type: i for i, node_type in enumerate(data.node_types)}

        for edge_type in data.edge_types:
            src_node_type, _, dst_node_type = edge_type
            # Convert edge_type to a string
            edge_type_str = ':'.join(edge_type)
            # Use numerical indices for in_channels
            self.convs[edge_type_str] = SAGEConv(
                in_channels={self.node_type_to_index[src_node_type]: data[src_node_type].x.shape[1],
                             self.node_type_to_index[dst_node_type]: data[dst_node_type].x.shape[1]},
                out_channels=64
            )

    def forward(self, data):
        x_dict = data.x_dict
        for edge_type, conv in self.convs.items():
            # Convert edge_type string back to tuple for data access
            edge_type_tuple = tuple(edge_type.split(':'))
            x_dict = conv(x_dict, data[edge_type_tuple].edge_index)
        return x_dict


model_HGNN_Sage = HeteroGNN(hdata)

# Count parameters
# Assuming you have a function 'count_parameters' defined
total_params = count_parameters(model_HGNN_Sage)
print(f"Total parameters: {total_params}")


# Iterate Over Edge Types
for edge_type in hdata.edge_types:
    # Access edge features for this type
    edge_index = hdata[edge_type].edge_index

    # Check if edge attributes exist before accessing them
    if 'edge_attr' in hdata[edge_type]:  # Check if edge_attr is present
        edge_features = hdata[edge_type].edge_attr
        print(f"Edge features for {edge_type}: {edge_features.shape}")
    else:
        print(f"No edge features found for {edge_type}")

    print(f"Edge index for {edge_type}: {edge_index.shape}")

"""# **Now our tabular data is ready to be used for Heterogenous GNN processing.**

**Preparing Data to create the  Heterogeneous Temporal Graph (Earlier approach) **
"""

# Order by date
df_ac_log = df_ac_log.sort_values(by='date')

# max date
max_date = df_ac_log['date'].max()
print('Max date: ' + str(max_date))

#min date
min_date = df_ac_log['date'].min()
print('Min date: ' + str(min_date))

# Split the data into daily access buckets
from datetime import datetime, timedelta
print(type(df_ac_log['date']))
df_ac_log['date']= pd.to_datetime(df_ac_log['date'])

start_date = pd.to_datetime(min_date)
end_date = pd.to_datetime(max_date)

interval = timedelta(days=1)
bucket_elements=[]

while start_date <= end_date:
  bucket_elements.append(df_ac_log[(start_date + interval) == df_ac_log["date"]].shape[0])
  start_date += interval

print(bucket_elements)

sns.scatterplot(x="index", y="access per day", data=pd.DataFrame(bucket_elements, columns=["access per day"]).reset_index())
plt.show()

#df_ac_log.head()

# Merge date and ip columns to create new column in df_ac_log
df_ac_log['date_ip'] = df_ac_log['date'].astype(str) + '_' + df_ac_log['ip'].astype(str)
#df_ac_log.head()

df_ac_log.info()

#df_ac_log.info()

# create a subset dataframe with IP, ip_id, date1, Date1_ID, Session_ID, date_ip_id, date_ip, AddToCart
df_session_log = df_ac_log[[ 'ip_id', 'Product', 'Product_Id', 'Category', 'Category_Id', 'Department', 'Department_Id', 'City','State','Country' ,'date', 'time', 'date_ip', 'AddToCart', 'Session_ID', 'date_ip_id', 'Date1_ID']]

df_session_log.head()

##max session_ID
#print("User node feature shape:", hdata['user'].x.shape)
print("session_id, max = ", df_session_log['Session_ID'].max())
### min session_ID
print("session_id, min = ",df_session_log['Session_ID'].min())

##max ip_id
print(df_session_log['ip_id'].max())
### min ip_id
print(df_session_log['ip_id'].min())

##max date_ip_id
print("date_ip_id, max = ", df_session_log['date_ip_id'].max())
### min ip_id
print("session_id, min = ", df_session_log['date_ip_id'].min())


# group date into week ids in df_session_log
df_session_log['week_id'] = df_session_log['date'].dt.isocalendar().week
##max week_id
print("week_id, max = ", df_session_log['week_id'].max())
### min week_id
print("week_id, min = ", df_session_log['week_id'].min())

"""Create Edge information with timestamps. The discrete time interval is set by date"""

# Split the dataframe by session id, date and ip address simulating temporal behaviour of the users on the website

""" DON'T DELETE will uncomment later  """
def split_dataframe_by_column(df, column_name):
    dataframes = []
    for value in df[column_name].unique():
        dataframes.append(df[df[column_name] == value])
    return dataframes

split_dfs = split_dataframe_by_column(df_session_log, 'date_ip_id')
print(len(split_dfs))
#print(split_dfs[1].info())

"""# Represent the HeteroData with session_ids, using User and Product information"""

print(len(df_product))
#print(df_product)

df_product_nodes = df_ac_log[['Product_Id', 'Category', 'Department']]
df_product_nodes = df_product_nodes.drop_duplicates()
df_product_nodes = df_product_nodes.reset_index(drop=True)
print(len(df_product_nodes))

#one hot encoding the node feature using OneHotEncoder of Scikit-Learn

from sklearn.preprocessing import OneHotEncoder


#Extract categorical columns from the dataframe
#Here we extract the columns with object datatype as they are the categorical columns
categorical_columns = df_product_nodes.select_dtypes(include=['object']).columns.tolist()

#Initialize OneHotEncoder
encoder = OneHotEncoder(sparse_output=False)

# Apply one-hot encoding to the categorical columns
one_hot_encoded = encoder.fit_transform(df_product_nodes[categorical_columns])

#Create a DataFrame with the one-hot encoded columns
#We use get_feature_names_out() to get the column names for the encoded data
one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(categorical_columns))

# Concatenate the one-hot encoded dataframe with the original dataframe
df_encoded = pd.concat([df_product_nodes, one_hot_df], axis=1)

# Drop the original categorical columns
df_product_features = df_encoded.drop(categorical_columns, axis=1)

# Display the resulting dataframe
print(f"Encoded data : \n{df_product_features}")

df_product_features.head()

# Convert to numpy
x = df_product_features.to_numpy()
print(x)
print(x.shape)

df_split_user_node = split_dfs[35]
df_split_user_node_col = split_dfs[35][['ip_id']]

#print(df_split_user_node)
print(len(split_dfs))

for i in range(441,444):
#for i in   range(len(split_dfs)):
  #split_dfs[i] = split_dfs[i].sort_values(by='time')
  # Convert 'time' column to datetime objects
  split_dfs[i]['time'] = pd.to_datetime(split_dfs[i]['time'])
  print(i)
  print(split_dfs[i]['ip_id'].unique())
  #print(split_dfs[i]['Session_ID'])

"""Create array of HeteroData objects based on Sesson_IDs"""

#### Commented for now: Might use later

split_dataframes = []

# Iterate the Split_df list
#for i in range(len(split_dfs)):
for i in range(1,100):

  split_dfs[i] = split_dfs[i].sort_values(by='time')
  # Convert 'time' column to datetime objects
  split_dfs[i]['time'] = pd.to_datetime(split_dfs[i]['time'])
  split_dfs[i]['avg_req_duration'] = (max(split_dfs[i]['time']) - min(split_dfs[i]['time'])) / len(split_dfs[i])
  split_dfs[i]['num_requests'] = len(split_dfs[i])
  # num buys based on AddToCart
  split_dfs[i]['num_buys'] = split_dfs[i]['AddToCart'].sum()
  # num views is num_requests minus num_buys
  split_dfs[i]['num_views'] = split_dfs[i]['num_requests'] - split_dfs[i]['num_buys']

  split_dfs[i]['exit_product_id'] = split_dfs[i]['Product_Id'].tail(1).values[0]
  split_dfs[i]['exit_buy'] = split_dfs[i]['AddToCart'].tail(1).values[0]
  split_dfs[i]['entry_product_id'] = split_dfs[i]['Product_Id'].head(1).values[0]
  split_dfs[i]['entry_buy'] = split_dfs[i]['AddToCart'].head(1).values[0]

  #### Let's start with the Product Node, create a subset dataframe.
  # It will have Product_Id, Category, Department, url
  df_temp_product_nodes = split_dfs[i][['Product_Id', 'Category']]
  df_temp_product_nodes = df_temp_product_nodes.drop_duplicates()
  df_temp_product_nodes = df_temp_product_nodes.reset_index(drop=True)
  #print("!!! df_temp_product_nodes !!!")
  #print(df_temp_product_nodes)

  # Filter df_product_features by Product_Id
  df_temp_product_features = df_product_features[df_product_features['Product_Id'].isin(df_temp_product_nodes['Product_Id'])]
  #print(df_temp_product_features)

  # Create the user node and features
  df_temp_user_node = split_dfs[i][['Session_ID']]
  df_temp_user_node = df_temp_user_node.drop_duplicates()
  df_temp_user_node = df_temp_user_node.reset_index(drop=True)
 # print("!!!! df_temp_user_node !!!")
 # print(df_temp_user_node)


  avg_req_duration = split_dfs[i]['avg_req_duration'].mean()
  num_requests = split_dfs[i]['num_requests'].mean()
  num_buys = split_dfs[i]['num_buys'].mean()
  num_views = split_dfs[i]['num_views'].mean()

  exit_product_id = split_dfs[i]['exit_product_id'].mean()

  # Convert avg_req_duration to a numeric representation before concatenation
  avg_req_duration_seconds = avg_req_duration.total_seconds()

  # Create a DataFrame from the individual features
  df_temp_user_features = pd.DataFrame({
    'avg_req_duration': [avg_req_duration_seconds],
    'num_requests': [num_requests],
    'num_buys': [num_buys],
    'num_views': [num_views],
    'exit_product_id': [exit_product_id]
  })
  #print("!!!! df_temp_user_features !!!")
  #print(df_temp_user_features)


  # Edge / edge index
  temp_edge_index = split_dfs[i][["AddToCart"]].values.transpose()
  temp_edge_index = torch.tensor(temp_edge_index, dtype=torch.float)
  print("!!!! temp_edge_index !!!")
  print(temp_edge_index)
  print(temp_edge_index.shape)


  ##### Build the dataset
  temp_data = HeteroData()

  # Set the number of nodes for 'user' and 'product'
  temp_data.date_id = split_dfs[i]['Date1_ID'].unique() # df_temp_user_node['ip_id']
  temp_data['user'].num_nodes = len(df_temp_user_features) # Assuming user_node_features is a list or array of user features
  temp_data['user'].id = split_dfs[i]['ip_id'].unique() # df_temp_user_node['ip_id']
  temp_data['user'].session_id = split_dfs[i]['Session_ID'].unique() # df_temp_user_node['ip_id']
  temp_data['user'].x = df_temp_user_features


  temp_data['product'].x = df_temp_product_features
  temp_data['product'].id = split_dfs[i]['Product_Id'].unique()
  temp_data['product'].num_nodes = len(df_temp_product_features) # Assuming df_product_features is a DataFrame or array of product features
  temp_data['user', 'buy', 'product'].edge_index = temp_edge_index

  split_dataframes.append(temp_data)
  print("~~~ tempdata ~~~")
  print(temp_data)
  print("~~~~")

"""# Sequence of Graph Snapshots
 behavior changes over time and capture long-term temporal patterns
"""

!pip install torch-geometric-temporal

"""# TGN model implementation using Temporal data structure"""

#!pip install --upgrade torch-geometric torch-sparse torch-scatter

!pip install torch-geometric-temporal

"""
import torch

def format_pytorch_version(version):
  return version.split('+')[0]

TORCH_version = torch.__version__
TORCH = format_pytorch_version(TORCH_version)

def format_cuda_version(version):
  return 'cu' + version.replace('.', '')

CUDA_version = torch.version.cuda
CUDA = format_cuda_version(CUDA_version)

!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html
!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html
#!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html
#!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html
!pip install torch-geometric-temporal -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html
!pip install torch-geometric

"""

!pip install torch-geometric-temporal

from google.colab import files
uploaded = files.upload()  # Upload the file from your local machine

!mv tsagcn.py /usr/local/lib/python3.10/dist-packages/torch_geometric_temporal/nn/attention

import torch
import torch.nn.functional as F


# Correct import statement
from torch_geometric_temporal.signal import temporal_signal_split
from torch_geometric_temporal.nn import TGN
#from torch_geometric_temporal.nn import  TGN #TGNMemory #,
from torch_geometric.nn import TransformerConv

# Define a custom module with the out_channels attribute
class MessageModule(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super(MessageModule, self).__init__()
        self.out_channels = out_channels  # Define the out_channels attribute
        self.sequential = torch.nn.Sequential(
            torch.nn.Linear(in_channels, out_channels),
            # ... other layers in your message function
        )

    def forward(self, x):
        return self.sequential(x)

class TGNModel(torch.nn.Module):
    def __init__(self, num_nodes, embedding_dim, memory_dim, num_layers, out_channels):
        super(TGNModel, self).__init__()

        # Initialize memory for each node
        # Providing values for raw_msg_dim, time_dim, message_module, and aggregator_module.
        # Please replace these with appropriate values based on your data and model requirements.
        self.memory = TGNMemory(
            num_nodes=num_nodes,
            memory_dim=memory_dim,
            raw_msg_dim=embedding_dim,  # Example: Assuming raw message dimension is the same as embedding dimension
            time_dim=1,  # Example: Assuming time is represented as a single scalar value
            message_module=MessageModule(embedding_dim, embedding_dim),  # Pass embedding_dim as in_channels and out_channels
            aggregator_module=lambda x: torch.mean(x, dim=0)
        )

        # Instead of trying to access and modify the internal message_module's out_channels,
        # store your message module as an attribute of the TGNModel class
        self.message_module = MessageModule(embedding_dim, embedding_dim)

        # Transformer-based message-passing layer
        self.message_passing = TransformerConv(
            in_channels=memory_dim,
            out_channels=embedding_dim,
            heads=4
        )

        # TGN layer
        # Assuming 'TGN' refers to a custom or third-party implementation, replace if needed.
        # Replace with appropriate TGN implementation if necessary.
        # Assuming this custom TGN layer can handle the memory, message passing, and number of layers.
        self.tgn = TGN(
            self.memory,
            self.message_passing,
            num_layers=num_layers
        )

        # Fully connected output layer
        self.fc = torch.nn.Linear(embedding_dim, out_channels)

    def forward(self, snapshots):
        # Process each graph snapshot in sequence
        out = []
        for i, snapshot in enumerate(snapshots):
            # Run the TGN model on each snapshot
            node_embeddings = self.tgn(snapshot.x, snapshot.edge_index, snapshot.edge_attr, snapshot.time)
            out.append(self.fc(node_embeddings))

        return out  # List of outputs for each snapshot

""" Train and Evaluate the Model"""

# Initialize model, optimizer, and loss function
model = TGNModel(2, embedding_dim=32, memory_dim=64, num_layers=2, out_channels=16)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
loss_fn = torch.nn.BCEWithLogitsLoss()
#criterion = torch.nn.MSELoss()
num_epochs = 100

# Training loop
for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for session in split_dataframes:
      optimizer.zero_grad()

      # Forward pass
      out = model(session)

      # Assume buy action
      labels = torch.randint(0, 2, (session['user'].x.size(0), 1)).float()

      # Compute loss
      loss = loss_fn(out, labels)
      loss.backward()
      optimizer.step()

      total_loss += loss.item()

    #Print loss
    print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(split_dataframes)}")

#Print
print("Training complete!")

print(split_dfs[1])

"""We have the dataset into user session datasets.

Convert the HeteroData object into NetworkX object to visualize the graphs

###### Draw the graph
  graph = to_networkx(temp_data)  # Convert to undirected graph

  # Networkx seems to create extra nodes from our heterogeneous graph, so I remove them
  #isolated_nodes = [node for node in graph.nodes() if graph.out_degree(node) == 0]
  #[graph.remove_node(i_n) for i_n in isolated_nodes]

  print(graph.nodes(data=True))
  print(graph.edges(data=True))

  print(graph.number_of_edges())


  print("######")

  # Define colors for nodes and edges
  node_type_colors = {
      "user": "#4599C3",
      "product": "#ED8546",
  }

  node_colors = []
  labels = {}
  ###
  for node, attrs in graph.nodes(data=True):
    print(graph.out_degree(node))

    print(node)
    #print(attrs)
    #print(type(attrs))


    node_type = attrs.get("type")

    print(node_type)

  #  color = node_type_colors.get(node_type, 'gray') # Use get with default color
   # color = node_type_colors[node_type]
   # node_colors.append(color)
  #  if attrs.get("type") == "user":
   #     labels[node] = f"U{node}"
   # elif attrs.get("type") == "product":
    #    labels[node] = f"P{node}"


    # Instead of temp_data.node_type(node), check for node type based on node ID
    if node < temp_data['user'].num_nodes: # Assuming user nodes are numbered first
        node_type = "user"
    else:
        node_type = "product"

    color = node_type_colors.get(node_type, 'gray') # Use get with default color
    node_colors.append(color)
    if node_type == "user":
        labels[node] = f"U{node}"
    elif node_type == "product":
        labels[node] = f"P{node}"

  ###
  # Define colors for the edges
  edge_type_colors = {
      ("user", "buy", "product"): "#8B4D9E",
      ("user", "view", "product"): "#DFB825",
  }

  edge_colors = []
  for from_node, to_node, attrs in graph.edges(data=True):
      edge_type = attrs["type"]
      color = edge_type_colors[edge_type]

      graph.edges[from_node, to_node]["color"] = color
      edge_colors.append(color)


  # Draw the graph
  pos = nx.spring_layout(graph, k=2)
  nx.draw_networkx(
      graph,
      pos=pos,
      labels=labels,
      with_labels=True,
      node_color=node_colors,
      edge_color=edge_colors,
      node_size=600,
  )
  plt.show()
"""

# Verify the HeteroData object
print(hdata.metadata())

#print(data['user'])

#hdata.x_dict

x_dict = hdata.x_dict
print("x_dict:", {key: value.shape for key, value in x_dict.items()})

"""# Create a sample GCN layers in Pytorch Geometric"""

import torch
import torch.nn as nn
from torch.nn import Linear
from torch_geometric.nn import MessagePassing
from torch_geometric.utils import add_self_loops, degree

class GCNConv(MessagePassing):
  def __init__(self, dim_in, dim_h):
    super().__init__(aggr='add')
    self.linear = Linear(dim_in, dim_h, bias=False)

  def forward(self, x, edge_index):
    edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))

    # apply linear transformation
    x= self.linear(x)

    # compute the normalization factor
    row, col = edge_index
    deg = degree(col, x.size(0), dtype=x.dtype)
    deg_inv_sqrt = deg.pow(-0.5)
    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0
    norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]

    # start propogating
    out = self.propagate(edge_index, x=x, norm=norm)
    return out

  def message(self, x, norm):
    return norm.view(-1, 1) * x

# Initialize this object as a GCN layer
conv = GCNConv(16, 32)

print(conv)
print(conv.aggr)

"""# Create a one-layer GAT model using hdata

"""

!pip install class_resolver

print(htdata)

import torch
import torch.nn.functional as F
from torch_geometric.nn import GAT
from torch_geometric.utils import to_undirected
from torch_geometric.transforms import RandomNodeSplit

model_GAT = GAT(in_channels=-1, hidden_channels=64, out_channels=4, num_layers=1)

optimizer = torch.optim.Adam(model_GAT.parameters(), lr=0.01, weight_decay=0.001)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
htdata = htdata.to(device)
model_GAT = model_GAT.to(device)

# Get the maximum node index for 'user' nodes only
num_user_nodes = htdata['user'].x.shape[0]

# Initialize target labels for 'user' nodes
# Replace with your actual target labels later if available
target_labels = torch.zeros(num_user_nodes, dtype=torch.long).to(device)

htdata['user'].y = htdata['user'].y.clamp(0, 3)

# Assign target labels to the 'user' data
if not hasattr(htdata['user'], 'y'):
    htdata['user'].y = target_labels

# test function
@torch.no_grad()
def test():
    model_GAT.eval()
    pred = model_GAT(htdata['user'].x, htdata['user', 'buy', 'product'].edge_index).argmax(dim=1)

    # Apply the test mask to both pred and htdata['user'].y using user indices
    user_indices = torch.arange(htdata['user'].num_nodes, device=device)  # Get indices of user nodes
    test_user_indices = user_indices[htdata['user'].test_mask.nonzero(as_tuple=True)[0]]

    # Ensure pred and target labels have the same shape for comparison
    pred_test = pred[test_user_indices]
    pred_test = pred_test.reshape(-1, 1)
    target_test = htdata['user'].y[test_user_indices]

    # Check if target_test is 0-dimensional and reshape if necessary
    if target_test.dim() == 0:
        target_test = target_test.reshape(1)  # Reshape to 1D

     # Check if pred_test and target_test have mismatched shapes
    if pred_test.shape != target_test.shape:
        # Print shapes for debugging
        print(f"pred_test shape: {pred_test.shape}, target_test shape: {target_test.shape}")

    # Perform element-wise comparison only if shapes match
    if pred_test.shape == target_test.shape:
        print("pred_test, target_test: " , pred_test, target_test)

        correct = (pred_test == target_test).sum()
        acc = int(correct) / int(htdata['user'].test_mask.sum())
        return float(acc)
    else:
        #print("pred_test.shape, target_test.shape : ", pred_test.shape, target_test.shape)
        print("Shape mismatch, returning 0 accuracy.")
        return 0.0  # Or handle the mismatch appropriately



# Create training loop
for epoch in range(num_user_nodes - 3335):
    model_GAT.train()
    optimizer.zero_grad()
    out = model_GAT(htdata['user'].x, htdata['user', 'buy', 'product'].edge_index)

    # Extract output for user nodes and select training samples
    out_user = out[:htdata['user'].x.shape[0]]
    train_mask = htdata['user'].train_mask

    target_labels = htdata['user'].y[train_mask]
    #If target_labels is not 1D, reshape to 1D
    if target_labels.dim() != 1:
        target_labels = target_labels.argmax(dim=1) # Reshape to 1D if necessary

    target_labels = target_labels.clamp(0, 3) # Clamp values to be between 0 and 3


    loss = F.cross_entropy(out_user[train_mask], target_labels)
    loss.backward()
    optimizer.step()
    print(f'Epoch: {epoch}, Loss: {loss.item()}, Accuracy: {test()}')

"""Implementation of Heterogeneous GAT (HAN)"""

print(hdata.edge_index_dict)
print(type(hdata.edge_index_dict))
print(hdata.edge_index_dict.items())

print(hdata.metadata())

"""Heterogeneous Graph Transformer (HGT) Implementation"""

!pip install --upgrade torch-geometric

import torch
from torch_geometric.utils import remove_self_loops
from torch_geometric.nn import HGTConv, Linear  # Import necessary modules
#from torch_geometric.nn import SAGEConv, to_hetero # Import for SAGEConv
#from torch_geometric.nn import HGTModel  # Import HGTModel

# Get the edge types from the edge_index_dict
edge_types = list(hdata.edge_index_dict.keys())

# Process each edge type to ensure consistency
for edge_type in edge_types:
    # Remove self-loops to potentially fix the issue
    edge_index = hdata.edge_index_dict[edge_type]
    edge_index, _ = remove_self_loops(edge_index)

    # Update edge_index_dict with the modified edge_index
    hdata.edge_index_dict[edge_type] = edge_index

# Check if 'user' is present in x_dict, and add if it's missing
if 'user' not in hdata.x_dict:
    # Assuming you have some user features in a variable 'user_features'
    # Make sure user_features is a PyTorch tensor of the correct shape
    hdata.x_dict['user'] = user_features
    print("Warning: 'user' features were missing and have been added.")


# Define the model using HeteroGNN and HGTConv instead of HGTModel
class HGT(torch.nn.Module):
    def __init__(self, hidden_channels, out_channels, metadata):
        super().__init__()
        self.conv1 = HGTConv(hidden_channels, hidden_channels, metadata, heads=8)
        self.conv2 = HGTConv(hidden_channels, out_channels, metadata, heads=8)

    def forward(self, x_dict, edge_index_dict):
        # x_dict, edge_index_dict = data.x_dict, data.edge_index_dict

        # 1. Obtain node embeddings
        x_dict = self.conv1(x_dict, edge_index_dict)

        # 2. Transform node embeddings using Linear layers
        x_dict = {key: Linear(x_dict[key].shape[1], 64)(x_dict[key]) for key in x_dict}  # Apply Linear to each node type

        # 3. Obtain node embeddings
        x_dict = self.conv2(x_dict, edge_index_dict)

        # 4. Print shapes for debugging
        #print("Shape of x_dict['user'] after conv2:", x_dict['user'].shape)

        # Return the final node embeddings
        return x_dict

# Create the model instance
metadata = hdata.metadata()
model = HGT(hidden_channels=64, out_channels=32, metadata=metadata)

# Forward pass through the model
output = model(hdata.x_dict, hdata.edge_index_dict)

print("Output shape:", output['user'].shape)

!pip install --upgrade torch-geometric

import torch
import torch.nn as nn
from torch_geometric.utils import remove_self_loops
from torch_geometric.nn import HGTConv, Linear

# Ensure 'user' is present in x_dict
if 'user' not in hdata.x_dict:
    # Assuming you have some user features in a variable 'user_features'
    # Make sure user_features is a PyTorch tensor of the correct shape
    hdata.x_dict['user'] = user_features
    print("Warning: 'user' features were missing and have been added.")

# Ensure metadata is defined correctly and passed to HGTConv
metadata = hdata.metadata()
print("Metadata:", metadata) # Print metadata to check before HGTConv is called

class HGTModel(nn.Module):
    def __init__(self, in_channels_dict, hidden_channels, out_channels, num_heads, num_layers, metadata):
        super(HGTModel, self).__init__()
        self.convs = nn.ModuleList()

        # Add HGT layers
        self.convs.append(HGTConv(
            in_channels=in_channels_dict,
            out_channels=hidden_channels,
            heads=num_heads,
            metadata=metadata # Passing metadata for the first layer
        ))

        # Subsequent layers use hidden_channels
        for _ in range(num_layers - 1):
            self.convs.append(HGTConv(
                in_channels=hidden_channels,
                out_channels=hidden_channels,
                heads=num_heads,
                metadata=metadata # Passing metadata for subsequent layers
            ))

        # Fully connected output layer
        self.fc = nn.Linear(hidden_channels, out_channels)

    def forward(self, x_dict, edge_index_dict):
        # Forward pass through each HGT layer
        for conv in self.convs:
            x_dict = conv(x_dict, edge_index_dict)

        # Output layer for user nodes
        return self.fc(x_dict['user'])

# Initialize the model
# Pass metadata to HGTModel during initialization
model = HGTModel(
    in_channels_dict=in_channels_dict,
    hidden_channels=64,
    out_channels=32,
    num_heads=8,
    num_layers=3,
    metadata=metadata
)

# Forward pass through the model
output = model(hdata.x_dict, hdata.edge_index_dict)

# Expected output shape for user nodes
print("Output shape:", output.shape)  # Expected shape: [num_users, out_channels]

"""HAN Model Implementation 1"""

import torch
import torch.nn as nn
from torch_geometric.nn import HANConv

class HANModel(nn.Module):
    def __init__(self, in_channels_dict, hidden_channels, out_channels, metadata, num_heads=8):
        super(HANModel, self).__init__()

        # Initialize HAN convolution layer with the relevant meta-paths
        self.han_conv = HANConv(
            in_channels_dict, hidden_channels, heads=num_heads,
            metadata=metadata, dropout=0.5
        )

        # Fully connected layer for output
        self.fc = nn.Linear(hidden_channels * num_heads, out_channels)

    def forward(self, x_dict, edge_index_dict):
        # Pass through HAN layer for all edge types at once
        # The change: using edge_index_dict directly instead of indexing it
        x_dict = self.han_conv(x_dict, edge_index_dict)

        # Process user node embeddings only (if focusing on user-level predictions)
        x = x_dict['user']
        return self.fc(x)

# Example of model initialization and forward pass
# Assume `data` is your HeteroData object with attributes x_dict and edge_index_dict
in_channels_dict = {
    'user': hdata['user'].x.size(-1),
    'product': hdata['product'].x.size(-1)
}
hidden_channels = 64
out_channels = 32
metadata = hdata.metadata()

print(hdata.x_dict)
print(hdata.edge_index_dict)

# Initialize the model
model = HANModel(in_channels_dict, hidden_channels, out_channels, metadata, num_heads=8)

# Forward pass through the model
output = model(hdata.x_dict, hdata.edge_index_dict)

# Example: output contains user-level predictions if the output layer is applied on 'user' nodes
print("Output shape:", output.shape)  # Expected shape: [num_users, out_channels]

import torch
import torch.nn as nn
from torch_geometric.nn import HANConv

class HANModel(nn.Module):
    def __init__(self, in_channels_dict, hidden_channels, out_channels, metadata, num_heads=8):  # Add metadata here
        super(HANModel, self).__init__()

        # Initialize HAN convolution layer with the relevant meta-paths
        self.han_conv = HANConv(
            in_channels_dict, hidden_channels, heads=num_heads,
            metadata=metadata, dropout=0.5 # Pass metadata to HANConv
        )
        # Fully connected layer for output
        self.fc = nn.Linear(hidden_channels * num_heads, out_channels)

    def forward(self, x_dict, edge_index_dict):
        # Ensure edge_index_dict is a dictionary
        if isinstance(edge_index_dict, torch.Tensor):
          edge_index_dict = {('user', 'buy', 'product'): edge_index_dict}  # Replace with actual edge type if known

        # Pass through HAN layer
        # Iterate through each edge type and apply the HANConv layer
        # Accumulate the output for each edge type
        out_dict = {}

        # Check if edge_index_dict is a dictionary before calling .items()
        if isinstance(edge_index_dict, dict):
            # If it's a dictionary, check if it contains a single tensor or multiple edge types
            if len(edge_index_dict) == 1 and isinstance(next(iter(edge_index_dict.values())), torch.Tensor):
                # If it's a single tensor, assume it's a single edge index and create a dummy edge type
                # Replace ('user', 'buy', 'product') with the actual edge type if known
                iterable = [(('user', 'buy', 'product'), next(iter(edge_index_dict.values())))]
            else:
                # If it's a dictionary with multiple edge types, iterate as usual
                iterable = edge_index_dict.items()
        else:
            raise TypeError("edge_index_dict should be a dictionary")


        for edge_type, edge_index in iterable:
            # Get source and destination node types from edge_type
            src_type, _, dst_type = edge_type

            # Apply HANConv to this edge type
            edge_out = self.han_conv(
                {src_type: x_dict[src_type], dst_type: x_dict[dst_type]},
                edge_index
            )
            # Accumulate output (if multiple edge types) - can be adjusted as needed
            if dst_type in out_dict:
                out_dict[dst_type] = out_dict[dst_type] + edge_out[dst_type]
            else:
                out_dict[dst_type] = edge_out[dst_type]

        x_dict = out_dict  # Update x_dict with the results from HANConv

        # Process user node embeddings only (if focusing on user-level predictions)
        x = x_dict['user']
        return self.fc(x)

# Example of model initialization and forward pass
# Assume `data` is your HeteroData object with attributes x_dict and edge_index_dict
in_channels_dict = {
    'user': hdata['user'].x.size(-1),
    'product': hdata['product'].x.size(-1)
}
hidden_channels = 64
out_channels = 32
metadata = hdata.metadata()

#print(hdata.x_dict)
#print(hdata.edge_index_dict)

# Initialize the model
model = HANModel(in_channels_dict, hidden_channels, out_channels, metadata, num_heads=8)

# Forward pass through the model
output = model(hdata.x_dict, hdata.edge_index_dict)

# Example: output contains user-level predictions if the output layer is applied on 'user' nodes
print("Output shape:", output.shape)  # Expected shape: [num_users, out_channels]

"""Define the input dimensions and Initilaze the model

Forward pass thru the model
"""

x_dict = hdata.x_dict  # Dictionary of node features by type (e.g., {'user': tensor, 'product': tensor})
edge_index_dict = hdata.edge_index_dict  # Dictionary of edge indices by type (e.g., {('user', 'buy', 'product'): tensor})


# Example: Define input feature sizes for each node type
in_channels_dict = {
    'user': hdata['user'].x.size(-1),  # Input feature size for 'user' nodes
    'product': hdata['product'].x.size(-1)  # Input feature size for 'product' nodes
}
hidden_channels = 64
out_channels = 32  # Or the number of classes if doing classification

# Initialize the model
model = HANModel(
    in_channels_dict=in_channels_dict,
    hidden_channels=hidden_channels,
    out_channels=out_channels,
    metadata=hdata.metadata(),
    num_heads=8
)

output = model(x_dict, edge_index_dict)

# `output` contains predictions for 'user' nodes (if focusing on user-level prediction)
print("Output shape:", output.shape)  # Expected shape: [num_users, out_channels]

"""
from torch_geometric.nn import GATConv, Linear, to_hetero
from torch.fx import symbolic_trace  # Import symbolic_trace

class GAT(torch.nn.Module):
    def __init__(self, dim_h, dim_out, metadata):  # Add metadata to the constructor
        super().__init__()
        self.conv1 = GATConv((-1, -1), dim_h, add_self_loops=False)
        self.linear = Linear(dim_h, dim_out)
        self.metadata = metadata  # Store metadata

    def forward(self, x_dict, edge_index_dict):  # Modify forward to handle dictionaries
        # Create a new dictionary to store the outputs
        x_dict_out = {}
        # Iterate over node types and apply conv1 only to 'user'
        # --> Fix: Apply a dummy operation to all node types <---
        for node_type in self.metadata[0]:
            if node_type == 'user':
                x_dict_out[node_type] = self.conv1(x_dict[node_type], edge_index_dict[('user', 'buy', 'product')]).relu()
                x_dict_out[node_type] = self.linear(x_dict_out[node_type])  # Apply linear layer inside 'user' condition
            else:
                # --> Dummy operation: Apply a linear transformation and ReLU to all other node types <--
                # This ensures they are part of the computation graph.
                # You can use an identity transformation (no change to features) if you prefer.
                x_dict_out[node_type] = self.linear(x_dict[node_type]).relu()  # Replace with identity or other operation if needed

        return x_dict_out  # Return the new dictionary


model = GAT(dim_h=64, dim_out=4, metadata=data.metadata()) # Pass metadata when creating the model

# Check if the model has already been traced
if not hasattr(model, '_is_traced'):  # Check for _is_traced attribute on the model
    model = to_hetero(model, data.metadata(), aggr='sum') # Convert to hetero before tracing
    model.forward = symbolic_trace(model.forward) # Then trace the forward method
    model._is_traced = True # Add attribute to indicate it's been traced

optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
"""

print("x_dict:", {key: value.shape for key, value in x_dict.items()})

"""Heterogeneous GNN model"""

print(model_HGNN_Sage)
#print(out)

print(model_GAT)

"""HeteroGNN implementation using GCNConv"""

import torch
import torch.nn as nn
import torch_geometric.nn as pyg_nn
from torch_geometric.nn import HeteroConv, GCNConv
from torch.fx import symbolic_trace

class HeteroGNN2(torch.nn.Module):
    def __init__(self, metadata, hidden_channels, out_channels):
        super().__init__()
        # Get the input dimensions from x_dict for the 'user' node type
        user_input_dim = next(iter(x_dict.values())).shape[1]  # Assuming all nodes have the same feature dimension

        # Define the convolutional layers as separate attributes
        self.user_buy_product_conv1 = GCNConv(user_input_dim, hidden_channels, add_self_loops=False)
        self.user_buy_product_conv2 = GCNConv(hidden_channels, out_channels, add_self_loops=False)


        self.conv1 = HeteroConv({
            ('user', 'buy', 'product'): self.user_buy_product_conv1
        }, aggr='sum')

        self.conv2 = HeteroConv({
            ('user', 'buy', 'product'): self.user_buy_product_conv2
        }, aggr='sum')

        self.traced = False  # Flag for tracing
        # Instead of storing the original forward, we store a copy for tracing
        self._forward_for_tracing = self.forward.__func__

    def forward(self, x_dict, edge_index_dict):
        print("x_dict:", x_dict)
        #print("edge_index_dict:", edge_index_dict)
        x_dict = self.conv1(x_dict, edge_index_dict)

        # Modify x_dict using a copy to avoid in-place assignment on Proxy objects
        x_dict_copy = x_dict.copy()  # Create a copy of the dictionary
        try:
            x_dict_copy['user'] = x_dict['user'].relu()  # Apply relu on the copy
        except KeyError:
            pass  # If 'user' key is not present, do nothing
        x_dict = x_dict_copy  # Assign the copy back to x_dict

        x_dict = self.conv2(x_dict, edge_index_dict)
        return x_dict

    def trace(self, x_dict, edge_index_dict): # Pass sample inputs to trace
        if not self.traced:
            # Use Interpreter to execute the traced graph with sample inputs
            # Trace the stored copy of the forward function
            traced_graph = symbolic_trace(self._forward_for_tracing, concrete_args={'x_dict': x_dict, 'edge_index_dict': edge_index_dict})
            # Create a callable using Interpreter and assign to self.forward
            self.forward = lambda x_dict, edge_index_dict: Interpreter(traced_graph).run(x_dict=x_dict, edge_index_dict=edge_index_dict)
            self.traced = True

# Sample input from a HeteroData object
x_dict = hdata.x_dict  # Ensure this is correctly formatted
edge_index_dict = hdata.edge_index_dict  # Ensure this is correctly formatted

# Instantiate the model
hidden_channels = 64  # Example value
out_channels = 32  # Example value
model_HGNN = HeteroGNN2(metadata=None, hidden_channels=hidden_channels, out_channels=out_channels)

# Trace the model before the forward pass
model_HGNN.trace(x_dict, edge_index_dict)

# Forward pass
out = model_HGNN(x_dict, edge_index_dict)