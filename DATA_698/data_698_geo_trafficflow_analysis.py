# -*- coding: utf-8 -*-
"""DATA_698_Geo_TrafficFlow_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11-SIfq78G1m8qLGY1Yfi9QmWCG2ymShE
"""

# Install PyTorch Geometric
!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.0.1+cu118.html
!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.0.1+cu118.html
!pip install torch-cluster -f https://data.pyg.org/whl/torch-2.0.1+cu118.html
!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.1+cu118.html
!pip install torch-geometric

# Install PyTorch Geometric Temporal
!pip install torch-geometric-temporal

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
#to ignore warnings
import warnings
warnings.filterwarnings('ignore')

## In the interest of saving time, calling the API multiple times, as the information is of static nature
##   the generated file with added columns is saved and read directly to save compute time

access_log_location_url = 'https://raw.githubusercontent.com/baruab/baruab/refs/heads/main/DATA_698/tokenized_access_logs_global.csv'
df_ac_log = pd.read_csv(access_log_location_url)

# Drop uplicates
df_ac_log = df_ac_log.drop_duplicates()

df_ac_log['Dt'] = df_ac_log['Date'].str.split(' ').str[0]

# Convert the 'date' column to datetime
df_ac_log['date'] = pd.to_datetime(df_ac_log['Dt'])

# Extract the day of the week
df_ac_log['day_of_week'] = df_ac_log['date'].dt.day_name()
df_ac_log['weekday'] = df_ac_log['date'].dt.weekday

df_ac_log["AddToCart"] = df_ac_log["url"].str.contains("add_to_cart").astype(int) # str.extract("(add_to_cart)")
#df_ac_log.head()

# Reassign the IP address to IDs (make it easier later for creating edges)

ipaddrs = df_ac_log['ip'].unique()
new_ip_ids = list(range(len(df_ac_log['ip'].unique())))
map_ip = dict(zip(ipaddrs, new_ip_ids))
print(type(map_ip))

df_ac_log['ip_id'] = df_ac_log['ip'].map(map_ip)

print(len(ipaddrs))
#df_ac_log.head()



# Reassign the Product to IDs (make it easier later for creating edges)

products = df_ac_log['Product'].unique()
new_prod_ids = list(range(len(df_ac_log['Product'].unique())))
map_prod = dict(zip(products, new_prod_ids))
print(type(map_prod))

df_ac_log['Product_Id'] = df_ac_log['Product'].map(map_prod)

#### Category
# Reassign the Category to IDs

cats = df_ac_log['Category'].unique()
new_cat_ids = list(range(len(df_ac_log['Category'].unique())))
map_cat = dict(zip(cats, new_cat_ids))

df_ac_log['Category_Id'] = df_ac_log['Category'].map(map_cat)

# Reassign the Dept to IDs

depts = df_ac_log['Department'].unique()
new_dept_ids = list(range(len(df_ac_log['Department'].unique())))
map_dept = dict(zip(depts, new_dept_ids))

df_ac_log['Department_Id'] = df_ac_log['Department'].map(map_dept)

df_ac_log.info()

#Split date

df_ac_log['date_id'] = df_ac_log['Date'].str.split('/').str[1]
df_ac_log['month_id'] = df_ac_log['Date'].str.split('/').str[0]
df_ac_log['year_id'] = df_ac_log['Date'].str.split('/').str[2].str.split(' ').str[0]

df_ac_log['time'] = df_ac_log['Date'].str.split(' ').str[1]


df_ac_log['Dt'] = df_ac_log['Date'].str.split(' ').str[0]

# Convert the 'date' column to datetime
df_ac_log['date'] = pd.to_datetime(df_ac_log['Dt'])

# Extract the day of the week
df_ac_log['day_of_week'] = df_ac_log['date'].dt.day_name()
df_ac_log['weekday'] = df_ac_log['date'].dt.weekday

# Convert 'numbers' column to integer
df_ac_log['date_id'] = df_ac_log['date_id'].astype(int)
df_ac_log['month_id'] = df_ac_log['month_id'].astype(int)
df_ac_log['year_id'] = df_ac_log['year_id'].astype(int)

# Find outlier by Date - IP, if any

from datetime import datetime, timedelta
print(type(df_ac_log['date']))
df_ac_log['date1']= pd.to_datetime(df_ac_log['date'])

# group dataframe by date1 and get counts per day
date_counts = df_ac_log.groupby(['date1']).size().reset_index(name='count')
#date_counts = df_ac_log.groupby('date1').size()
print(date_counts.head(50))

# find max count in panda series date_count
max_count = date_counts['count'].max()
print(max_count)

min_count = date_counts['count'].min()
print(min_count)

# group date into week ids in df_session_log
df_ac_log['week_id'] = df_ac_log['date'].dt.isocalendar().week

# create year_month_id based on year_id and month_id
df_ac_log['year_month_id'] = df_ac_log['year_id'].astype(int)*100 + df_ac_log['month_id'].astype(int)

# remove rows with 20170914 date1 from df_ac_log due to unusual spike
df_ac_log = df_ac_log[df_ac_log['date1'] != '2017-09-14']

print(df_ac_log.shape)

# Reassign the year_month_id to snapshot_ID (make it easier later for splitting graphs)

year_months = df_ac_log['year_month_id'].unique()
new_year_months = list(range(len(df_ac_log['year_month_id'].unique())))
map_snapshot = dict(zip(year_months, new_year_months))
print(type(map_snapshot))

df_ac_log['snapshot_id'] = df_ac_log['year_month_id'].map(map_snapshot)

# Merge date and ip columns to create new column in df_ac_log
df_ac_log['date_ip_id'] = df_ac_log['date'].astype(str) + '_' + df_ac_log['ip_id'].astype(str)

access_count = df_ac_log.groupby("ip_id")["date_id"].count().rename("access_count")
print(access_count)

"""Encoding data"""

!pip install sentence-transformers

#install torch
!pip install torch

import torch
from sentence_transformers import SentenceTransformer

class SequenceEncoder:
     def __init__(self, model_name='all-MiniLM-L6-v2', device=None):
         self.device = device
         self.model = SentenceTransformer(model_name, device=device)

     @torch.no_grad()
     def __call__(self, df):
         x = self.model.encode(df.values, show_progress_bar=True,
                               convert_to_tensor=True, device=self.device)
         return x.cpu()

encoder = SequenceEncoder() # Create an instance of the SequenceEncoder class

# subset the dataframe
df_cat = df_ac_log[['Category']]
df_cat = df_cat.drop_duplicates()
print(df_cat.shape)


category_xs = [encoder(df_cat['Category'])] # Encode the 'Category' column
category_x = torch.cat(category_xs, dim=-1) # Concatenate the encoded results
print(category_x.shape)

# subset the dataframe
df_prod = df_ac_log[['Product']]
df_prod = df_prod.drop_duplicates()
print(df_prod.shape)

product_xs = [encoder(df_prod['Product'])] # Encode the 'Category' column
product_x = torch.cat(product_xs, dim=-1) # Concatenate the encoded results
print(product_x.shape)
print(product_x)

# group by date1 and ip to create unique session ids
df_ac_log['Session_ID'] = df_ac_log.groupby(['date1', 'ip_id']).ngroup()

#Sort the dataframe by Session_ID and time
df_ac_log = df_ac_log.sort_values(by=['Session_ID', 'time'])

df_ac_log.head()
df_ac_log.info()



df_ac_log.head()

"""# User based analysis"""

import pandas as pd

# subset df_ac_log by Session_id, time, AddToCart For Product based analysis
df_session_log = df_ac_log[['Session_ID', 'snapshot_id', 'ip_id','date', 'time', 'AddToCart', 'Product', 'Product_Id', 'Department', 'Department_Id', 'City', 'State', 'Country']]

# add interaction column based on AddToCart , if 0 then 'view' else 1 then 'buy'
df_session_log['interaction'] = df_session_log['AddToCart'].apply(lambda x: 'view' if x == 0 else 'buy')

# sort dataframe by Session_ID and time
df_session_log = df_session_log.sort_values(by=['Session_ID', 'time'])

# add timestamp based on date and time
df_session_log['timestamp'] = pd.to_datetime(df_session_log['date'].astype(str) + ' ' + df_session_log['time'].astype(str))

# create timestamp using  pd.to_datetime
df_session_log['timestamp'] = pd.to_datetime(df_session_log['timestamp'])

# add time_spend based on time difference for same Session_Id
# Calculate time differences within each session
# Move the Timespent up by 1 row during calculation

df_session_log['PrevTimeSpent'] = df_session_log.groupby('Session_ID')['timestamp'].diff().dt.total_seconds()
df_session_log['TimeSpent'] = df_session_log.groupby('Session_ID')['PrevTimeSpent'].shift(-1)

# Calculate user transitions
df_session_log['NextProduct_Id'] = df_session_log.groupby('Session_ID')['Product_Id'].shift(-1)
df_session_log['NextProduct'] = df_session_log.groupby('Session_ID')['Product'].shift(-1)


# Aggregate interaction sequences for each session
df_session_log['InteractionSequence'] = df_session_log.groupby('Session_ID')['interaction'].transform(lambda x: ' -> '.join(x))

df_session_log.head(5)

# Drop rows where there is no transition
transitions = df_session_log.dropna(subset=['NextProduct'])

"""# Prepare subset for geographic graph"""

# Create a subset of df_geo_subset with User_ID  Product_ID Interaction
df_geo_subset = df_session_log[['ip_id', 'Session_ID','City', 'Country', 'Product_Id', 'AddToCart',  'interaction']]
df_geo_subset.head()

# Group by Session_ID and aggregate
geo_df = df_session_log.groupby('Session_ID').agg(
    ip_id = ('ip_id', 'first'),
    City = ('City', 'first'),
    Country = ('Country', 'first'),

    num_buy=('AddToCart', 'sum'),          # Sum of AddToCart per session
    InteractionCount=('Session_ID', 'count')
).reset_index()

"""Create a Geographic graph"""

import networkx as nx

# Group by City, State, or Country to aggregate interactions
traffic_data = geo_df.groupby(['City', 'Country']).agg(
    TotalInteractions=('InteractionCount', 'sum')
).reset_index()

print(traffic_data.shape)

# Create graph
G = nx.Graph()

# Add nodes for each location
for _, row in traffic_data.iterrows():
    location = f"{row['City']}, {row['Country']}"
    G.add_node(location, interactions=row['TotalInteractions'])

# Add edges (example: between highly connected locations)
for i, loc1 in traffic_data.iterrows():
    for j, loc2 in traffic_data.iterrows():
        if i < j:
            # Example: Create an edge if total interactions > threshold
            weight = loc1['TotalInteractions'] + loc2['TotalInteractions']
            if weight > 10:  # Adjust threshold as needed
                loc1_name = f"{loc1['City']}, {loc1['Country']}"
                loc2_name = f"{loc2['City']}, {loc2['Country']}"
                G.add_edge(loc1_name, loc2_name, weight=weight)

# Print graph info
print(f"Number of nodes: {G.number_of_nodes()}")
print(f"Number of edges: {G.number_of_edges()}")

"""Visualize Graph

Commented due to RAM constraint
import matplotlib.pyplot as plt
# Import the networkx library
import networkx as nx

# Position nodes using spring layout
pos = nx.spring_layout(G, seed=42)

# Draw the graph
plt.figure(figsize=(12, 8))
nx.draw(
    G, pos, with_labels=True, node_size=1000, node_color="skyblue",
    edge_color="gray", font_size=10, font_weight="bold", alpha=0.8
)
labels = nx.get_edge_attributes(G, 'weight')
nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)
plt.title("Geographic Traffic Graph")
plt.show()

Analyze Traffic Patterns
"""

# Import the networkx library
import networkx as nx

# Identify high-traffic locations (degree centrality)
centrality = nx.degree_centrality(G)
sorted_centrality = sorted(centrality.items(), key=lambda x: x[1], reverse=True)
print("High-Traffic Locations:", sorted_centrality)

# Calculate edge weights (total interactions between locations)
edge_weights = nx.get_edge_attributes(G, 'weight')

edge_weights = dict(sorted(edge_weights.items(), key=lambda x: x[1], reverse=True))

print("Traffic Between Locations:", edge_weights)

print(f"Number of nodes: {G.number_of_nodes()}")
print(f"Number of edges: {G.number_of_edges()}")

!pip show torch
!pip show torch-geometric
!pip show torch-geometric-temporal
!pip show torch-sparse

!pip install --upgrade torch  # Ensure PyTorch is up-to-date
!pip uninstall torch_scatter torch_sparse  # Remove existing installations
!pip install torch-scatter torch-sparse --no-index --find-links=https://data.pyg.org/whl/torch-{torch.__version__}.html # Reinstall from PyG website

from google.colab import files
uploaded = files.upload()  # Upload the file from your local machine

!mv tsagcn.py /usr/local/lib/python3.10/dist-packages/torch_geometric_temporal/nn/attention

from torch_geometric_temporal.signal import DynamicGraphTemporalSignal
import torch

# Example temporal data
edge_indices = []  # List of edge indices per snapshot
edge_weights = []  # List of edge weights per snapshot
node_features = []  # Node features per snapshot

# Create snapshots for each time window
for time_window in sorted(geo_df['Session_ID'].unique()):
    snapshot = geo_df[geo_df['Session_ID'] == time_window]
    # Ensure 'City' and 'Country' columns are numeric (e.g., using label encoding)
    # Example using pandas factorize:
    city_labels, _ = pd.factorize(snapshot['City'])
    country_labels, _ = pd.factorize(snapshot['Country'])
    edges = np.stack([city_labels, country_labels], axis=0)  # Edge indices as numeric array

    weights = snapshot['InteractionCount'].tolist()  # Edge weights
    edge_indices.append(edges)
    edge_weights.append(weights)
    node_features.append([])  # Add node features if available


class MyDynamicGraphTemporalSignal(DynamicGraphTemporalSignal):
    def _check_temporal_consistency(self):
        # Remove or modify the check if targets are None
        if self.targets is not None:
            assert len(self.features) == len(self.targets), "Temporal dimension inconsistency."
        # or just pass if targets are None
        # pass

    def _get_target(self, time_index: int):
        # Check if self.targets is None before accessing it
        if self.targets is None:
            return None  # Or any default value you want to return when targets are None
        elif self.targets[time_index] is None:
            return self.targets[time_index]
        else:
            return torch.tensor(self.targets[time_index], dtype=torch.float)


temporal_graph = MyDynamicGraphTemporalSignal(
    edge_indices=edge_indices,
    edge_weights=edge_weights,
    features=node_features,
    targets=None
)

# Print details about the DynamicGraphTemporalSignal
print("Number of time steps:", len(temporal_graph.features))

# Example: Inspect the first snapshot
print("Edge Index at t=0:", temporal_graph.edge_indices[0])
print("Edge Weights at t=0:", temporal_graph.edge_weights[0])
print("Node Features at t=0:", temporal_graph.features[0])
print("Targets at t=0:", temporal_graph.targets[0] if temporal_graph.targets else "No targets")

"""Iterate over snapshots"""

for t, snapshot in enumerate(temporal_graph):
    x_t = snapshot.x  # Node features at time t
    edge_index_t = snapshot.edge_index  # Edge indices at time t
    edge_weight_t = snapshot.edge_weight  # Edge weights at time t
    y_t = snapshot.y if hasattr(snapshot, 'y') else None  # Targets at time t (if available)

    print(f"Time step {t}:")
    print(f"  Node Features: {x_t.shape}")
    print(f"  Edge Indices: {edge_index_t.shape}")
    if edge_weight_t is not None:
        print(f"  Edge Weights: {edge_weight_t.shape}")
    if y_t is not None:
        print(f"  Targets: {y_t.shape}")

"""Temporal Node Classification"""

from torch_geometric_temporal.nn.recurrent import GConvLSTM
import torch
import torch.nn.functional as F

class TemporalGNN(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super(TemporalGNN, self).__init__()
        self.recurrent = GConvLSTM(in_channels, hidden_channels, 1)  # 1 = single temporal layer
        self.linear = torch.nn.Linear(hidden_channels, out_channels)

    def forward(self, x, edge_index, edge_weight, h=None):
        h = self.recurrent(x, edge_index, edge_weight, h)  # Recurrent temporal processing
        h = F.relu(h)
        return self.linear(h)

# Initialize the model
try:
    in_channels = torch.tensor(temporal_graph.features[0]).shape[1]  # If features are 2D
except IndexError:
    in_channels = 1

out_channels = 2  # Example: Binary classification
model = TemporalGNN(in_channels=in_channels, hidden_channels=16, out_channels=out_channels)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Training loop
model.train()
for epoch in range(50):  # Number of epochs
    loss = 0
    h = None  # Initialize hidden state
    for t, snapshot in enumerate(temporal_graph):
        x_t = snapshot.x
        # Skip snapshots with empty feature tensors
        if x_t.nelement() == 0:  # Check if x_t is empty
            print(f"Skipping snapshot {t} due to empty node features.")
            continue  # Skip to the next snapshot

        edge_index_t = snapshot.edge_index
        edge_weight_t = snapshot.edge_weight
        y_t = snapshot.y

        optimizer.zero_grad()
        out = model(x_t, edge_index_t, edge_weight_t, h)
        loss_t = F.cross_entropy(out, y_t)
        loss += loss_t
        loss_t.backward()
        optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss.item()}")

# Create a subset of df_geo_subset with#Session_ID | User_ID | Product_ID | InteractionType | Timestamp
df_flow_subset = df_session_log[['ip_id', 'Session_ID','City', 'Country', 'Product_Id', 'timestamp',  'interaction']]
df_flow_subset.head()

# Sort by Session_ID and Timestamp
df_flow_subset = df_flow_subset.sort_values(by=['Session_ID', 'timestamp'])

# Group by Session_ID to get product pathways
session_paths = df_flow_subset.groupby('Session_ID')['Product_Id'].apply(list)
print(session_paths)

"""Construct Product Flow Graph"""

import networkx as nx

# Initialize directed graph
G = nx.DiGraph()

# Add edges based on session pathways
for path in session_paths:
    for i in range(len(path) - 1):
        source, target = path[i], path[i + 1]
        if G.has_edge(source, target):
            G[source][target]['weight'] += 1  # Increment weight if edge exists
        else:
            G.add_edge(source, target, weight=1)

# Print graph info
print(f"Number of nodes: {G.number_of_nodes()}")
print(f"Number of edges: {G.number_of_edges()}")

# Print edges with weights
for edge in G.edges(data=True):
    print(edge)

# Sort edges by weight
common_paths = sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)
print("Common Pathways:", common_paths)



"""# Predict the Next Product"""

def predict_next_product(current_product, G):
    # Get all neighbors of the current product
    neighbors = list(G[current_product])
    if not neighbors:
        return None  # No prediction if no outgoing edges
    # Predict the neighbor with the highest weight
    next_product = max(neighbors, key=lambda neighbor: G[current_product][neighbor]['weight'])
    return next_product

# Example prediction
current_product = 28
next_product = predict_next_product(current_product, G)
print(f"Predicted next product after {current_product}: {next_product}")

from sklearn.model_selection import train_test_split
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense

# Prepare sequences
all_paths = session_paths.tolist()
vocab = {product: idx for idx, product in enumerate(set(df_flow_subset['Product_Id']))}
encoded_paths = [[vocab[product] for product in path] for path in all_paths]

# Prepare training data
X, y = [], []
for path in encoded_paths:
    for i in range(1, len(path)):
        X.append(path[:i])  # Input sequence
        y.append(path[i])   # Next product
X = pad_sequences(X, padding='pre')
y = to_categorical(y, num_classes=len(vocab))

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define LSTM model
model = Sequential([
    Embedding(input_dim=len(vocab), output_dim=16, input_length=X.shape[1]),
    LSTM(32),
    Dense(len(vocab), activation='softmax')
])
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

import matplotlib.pyplot as plt
history = model.fit(X_train, y_train, epochs=3, batch_size=32, validation_data=(X_test, y_test))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.title("Loss Over Epochs")
plt.show()

"""Predict next Product"""

def predict_next_product(sequence, model, vocab, reverse_vocab):
    # Encode the input sequence
    encoded_sequence = [vocab[product] for product in sequence]
    padded_sequence = pad_sequences([encoded_sequence], padding='pre', maxlen=X.shape[1])
    # Predict the next product
    prediction = model.predict(padded_sequence)
    predicted_index = prediction.argmax(axis=1)[0]
    return reverse_vocab[predicted_index]

reverse_vocab = {idx: product for product, idx in vocab.items()}
print(predict_next_product([16, 17], model, vocab, reverse_vocab))

import matplotlib.pyplot as plt

pos = nx.spring_layout(G)  # Layout for better visualization
plt.figure(figsize=(8, 6))
nx.draw(G, pos, with_labels=True, node_size=3000, node_color='skyblue', font_size=10, font_weight='bold', edge_color='gray')
edge_labels = nx.get_edge_attributes(G, 'weight')
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)
plt.title("Product Traffic Flow")
plt.show()