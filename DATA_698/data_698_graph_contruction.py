# -*- coding: utf-8 -*-
"""DATA_698_Graph_contruction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yUFVfvHcCCu9K5PO8jsz_p4DK1OyVqL-

Construct multiple views of graph
"""



"""# Load the dataset"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
#to ignore warnings
import warnings
warnings.filterwarnings('ignore')

## In the interest of saving time, calling the API multiple times, as the information is of static nature
##   the generated file with added columns is saved and read directly to save compute time

access_log_location_url = 'https://raw.githubusercontent.com/baruab/baruab/refs/heads/main/DATA_698/tokenized_access_logs_global.csv'
df_ac_log = pd.read_csv(access_log_location_url)

# Drop uplicates
df_ac_log = df_ac_log.drop_duplicates()

df_ac_log.nunique()

"""# Adding Features to the data"""

df_ac_log['Dt'] = df_ac_log['Date'].str.split(' ').str[0]

# Convert the 'date' column to datetime
df_ac_log['date'] = pd.to_datetime(df_ac_log['Dt'])

# Extract the day of the week
df_ac_log['day_of_week'] = df_ac_log['date'].dt.day_name()
df_ac_log['weekday'] = df_ac_log['date'].dt.weekday

df_ac_log["AddToCart"] = df_ac_log["url"].str.contains("add_to_cart").astype(int) # str.extract("(add_to_cart)")
#df_ac_log.head()

# Reassign the IP address to IDs (make it easier later for creating edges)

ipaddrs = df_ac_log['ip'].unique()
new_ip_ids = list(range(len(df_ac_log['ip'].unique())))
map_ip = dict(zip(ipaddrs, new_ip_ids))
print(type(map_ip))

df_ac_log['ip_id'] = df_ac_log['ip'].map(map_ip)

print(len(ipaddrs))
#df_ac_log.head()

# Reassign the Product to IDs (make it easier later for creating edges)

products = df_ac_log['Product'].unique()
new_prod_ids = list(range(len(df_ac_log['Product'].unique())))
map_prod = dict(zip(products, new_prod_ids))
print(type(map_prod))

df_ac_log['Product_Id'] = df_ac_log['Product'].map(map_prod)

#### Category
# Reassign the Category to IDs

cats = df_ac_log['Category'].unique()
new_cat_ids = list(range(len(df_ac_log['Category'].unique())))
map_cat = dict(zip(cats, new_cat_ids))

df_ac_log['Category_Id'] = df_ac_log['Category'].map(map_cat)

# Reassign the Dept to IDs

depts = df_ac_log['Department'].unique()
new_dept_ids = list(range(len(df_ac_log['Department'].unique())))
map_dept = dict(zip(depts, new_dept_ids))

df_ac_log['Department_Id'] = df_ac_log['Department'].map(map_dept)
df_ac_log.head()

#mapping = {index: i for i, index in enumerate(df_ac_cat_subset.index.unique())}

"""**Split the date time string into date & time represented as numbers**"""

df_ac_log['date_id'] = df_ac_log['Date'].str.split('/').str[1]
df_ac_log['month_id'] = df_ac_log['Date'].str.split('/').str[0]
df_ac_log['year_id'] = df_ac_log['Date'].str.split('/').str[2].str.split(' ').str[0]

df_ac_log['time'] = df_ac_log['Date'].str.split(' ').str[1]


df_ac_log['Dt'] = df_ac_log['Date'].str.split(' ').str[0]

# Convert the 'date' column to datetime
df_ac_log['date'] = pd.to_datetime(df_ac_log['Dt'])

# Extract the day of the week
df_ac_log['day_of_week'] = df_ac_log['date'].dt.day_name()
df_ac_log['weekday'] = df_ac_log['date'].dt.weekday

# Convert 'numbers' column to integer
df_ac_log['date_id'] = df_ac_log['date_id'].astype(int)
df_ac_log['month_id'] = df_ac_log['month_id'].astype(int)
df_ac_log['year_id'] = df_ac_log['year_id'].astype(int)

access_count = df_ac_log.groupby("ip_id")["date_id"].count().rename("access_count")
print(access_count)

"""Create Encoders"""

!pip install sentence-transformers

#install torch
!pip install torch

import torch
from sentence_transformers import SentenceTransformer

class SequenceEncoder:
     def __init__(self, model_name='all-MiniLM-L6-v2', device=None):
         self.device = device
         self.model = SentenceTransformer(model_name, device=device)

     @torch.no_grad()
     def __call__(self, df):
         x = self.model.encode(df.values, show_progress_bar=True,
                               convert_to_tensor=True, device=self.device)
         return x.cpu()

df_ac_log.head()

encoder = SequenceEncoder() # Create an instance of the SequenceEncoder class

# subset the dataframe
df_cat = df_ac_log[['Category']]
df_cat = df_cat.drop_duplicates()
print(df_cat.shape)


category_xs = [encoder(df_cat['Category'])] # Encode the 'Category' column
category_x = torch.cat(category_xs, dim=-1) # Concatenate the encoded results
print(category_x.shape)

# subset the dataframe
df_prod = df_ac_log[['Product']]
df_prod = df_prod.drop_duplicates()
print(df_prod.shape)

product_xs = [encoder(df_prod['Product'])] # Encode the 'Category' column
product_x = torch.cat(product_xs, dim=-1) # Concatenate the encoded results
print(product_x.shape)
print(product_x)

print(category_x)
print(category_x.shape)

"""**Alternate Label encoder**"""

from sklearn.preprocessing import LabelEncoder

# Label encode the 'Category' column
label_encoder = LabelEncoder()

# get unique category names
category_labels = label_encoder.fit_transform(df_ac_log['Category'].unique())
#print(category_labels)
print(len(category_labels))
category_labels_tensor = torch.tensor(category_labels, dtype=torch.long).view(-1, 1)
print(category_labels_tensor)

"""# Define the node features"""

#Let's create the User Node, create a subset dataframe.
# Add user city, access count and buy intent count

access_count = df_ac_log.groupby("ip_id")["ip_id"].count().rename("access_count")
buy_count = df_ac_log[df_ac_log["AddToCart"] == 1].groupby("ip_id")["AddToCart"].count().rename("buy_count")
user_node_features = pd.concat([access_count, buy_count], axis=1)

# Remap user ID
user_node_features = user_node_features.reset_index(drop=False)
user_node_features.head()
user_id_mapping = user_node_features['ip_id']

# Only keep user features
#user_node_features = user_node_features.drop('ip_id', axis=1)
user_node_features.head()


df_ip_city = df_ac_log[['ip_id', 'City', 'State', 'Country']]
df_ip_city = df_ip_city.drop_duplicates()
df_ip_city.head()

# merge user_node_features and df_ip_city by ip_id
df_user_features = pd.merge(user_node_features, df_ip_city, on='ip_id')
df_user_features = df_user_features.drop_duplicates()

# Only keep user features
#df_user_features = df_user_features.drop('ip_id', axis=1)
#df_user_features.head()
print(df_user_features.shape)

# torch cat the features
#torch access_count
access_cnt = df_user_features['access_count']
access_cnt = torch.tensor(access_cnt, dtype=torch.float).view(-1, 1)
print(access_cnt.shape)

#torch city
df_user_features['City'] = df_user_features['City'].astype(str) # Convert cities to a list of strings
df_user_features['State'] = df_user_features['State'].astype(str) # Convert cities to a list of strings

# torch cities
city_xs =  [encoder(df_user_features['City'])]
city_x = torch.cat(city_xs, dim=-1) # Concatenate the encoded results
print(city_x.shape)

# torch states
state_xs =  [encoder(df_user_features['State'])]
state_x = torch.cat(state_xs, dim=-1) # Concatenate the encoded results
print(state_x.shape)

user_features = torch.cat([city_x, state_x], dim=1)
print(user_features.shape)

df_user_features.head()

"""Create Product features"""

# get unique department names

df_product_depts = df_ac_log[['Product_Id', 'Department_Id']]
df_product_depts = df_product_depts.drop_duplicates()
df_product_depts.head()

# convert x into tensor
product_features = torch.tensor(df_product_depts['Product_Id'], dtype=torch.float).view(-1,1)
print(product_features.shape)

#dept_labels = label_encoder.fit_transform(df_product_depts['Department_Id'])

dept_labels = df_product_depts['Department_Id']
#print(dept_labels)
#print(len(dept_labels))

dept_labels_tensor = torch.tensor(dept_labels, dtype=torch.float).view(-1, 1)
product_y = dept_labels_tensor

#print(product_y)
print(product_y.shape)

!pip install torch-geometric torch-sparse torch-scatter

# Import the necessary library
from torch_geometric.data import HeteroData

user_ids = torch.tensor(df_user_features['ip_id'].values, dtype=torch.long)
user_y = torch.tensor(df_user_features['buy_count'].values, dtype=torch.long).view(-1,1)

hdata = HeteroData()
hdata['user'].num_nodes = len(user_node_features)
hdata['user'].x = city_x
hdata['user'].ids = user_ids
hdata['user'].y = user_y   # buy_count

product_ids = torch.tensor(df_product_depts['Product_Id'].values, dtype=torch.long)
hdata['product'].num_nodes = len(df_product_depts)
hdata['product'].x = product_x
hdata['product'].ids = product_ids
hdata['product'].y = product_y   # dept label

print(hdata)

import torch # Import the torch module
from datetime import datetime # Import the datetime module from the datetime library

# Seperate by buy vs view using AddToCart flag
buy_edge_index=[]
buy_timestamp=[]

#Iterate the dataframe
for index, row in df_ac_log.iterrows():

  timestamp = datetime.strptime(row["Date"],'%m/%d/%Y %H:%M')
  ts_unix = int(timestamp.timestamp())
  buy_edge_index.append([row["ip_id"], row["Product_Id"]])
  buy_timestamp.append(ts_unix)

# Convert to tensor and add to HeteroData
buy_edge_index = torch.tensor(buy_edge_index, dtype=torch.long).t().contiguous()
buy_timestamp = torch.tensor(buy_timestamp, dtype=torch.long).view(-1, 1)
hdata['user', 'buy', 'product'].edge_index = buy_edge_index
### add edge_type attribute
hdata['user', 'buy', 'product'].edge_type = 'buy'
hdata['user', 'buy', 'product'].edge_attr = buy_timestamp

# torch AddToCart from df_ac_log
add_to_cart_labels = torch.tensor(df_ac_log['AddToCart'].values, dtype=torch.long)
hdata['user', 'buy', 'product'].edge_label = add_to_cart_labels


# Add reverse relation edge index for 'product' -> 'user'
hdata['product', 'rev_buy', 'user'].edge_index = hdata['user', 'buy', 'product'].edge_index.flip(0)

# Optionally, add edge attributes for the reverse relation
# If `edge_attr` is present for 'buys' edges, you can reuse it for 'rev_buys'
if 'edge_attr' in hdata['user', 'buy', 'product']:
    hdata['product', 'rev_buy', 'user'].edge_attr = hdata['user', 'buy', 'product'].edge_attr

print(hdata)
print(buy_edge_index)
print(buy_edge_index.shape)

import torch_geometric.transforms as T
from torch_geometric.nn import SAGEConv, to_hetero

hdata.num_classes = 2

class GNN(torch.nn.Module):
    def __init__(self, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = SAGEConv((-1, -1), hidden_channels)
        self.conv2 = SAGEConv((-1, -1), out_channels)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index).relu()
        x = self.conv2(x, edge_index)
        return x


model = GNN(hidden_channels=64, out_channels=hdata.num_classes)
model = to_hetero(model, hdata.metadata(), aggr='sum')

#next training
# Initialize model
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
# Define the loss function
criterion = torch.nn.CrossEntropyLoss()

# Forward pass
model.train()

# Ensure edge indices in edge_index_dict are of type torch.long
for edge_type in hdata.edge_index_dict:
    hdata.edge_index_dict[edge_type] = hdata.edge_index_dict[edge_type].type(torch.long)

out = model(hdata.x_dict, hdata.edge_index_dict)

#print("Final output:", out)
# print user node embedding
print("User node embeddings:", out[('user')])

# print product node embedding
print("Product node embeddings:", out[('product')])

#print("User node embeddings:" , out[0])

# Add training loop
def train():
    model.train()
    optimizer.zero_grad()
    out = model(hdata.x_dict, hdata.edge_index_dict)

# Select the target labels and nodes




# loop thru 100 epoch