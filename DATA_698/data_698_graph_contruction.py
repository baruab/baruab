# -*- coding: utf-8 -*-
"""DATA_698_Graph_contruction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yUFVfvHcCCu9K5PO8jsz_p4DK1OyVqL-

Construct multiple views of graph
"""



"""# Load the dataset"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
#to ignore warnings
import warnings
warnings.filterwarnings('ignore')

## In the interest of saving time, calling the API multiple times, as the information is of static nature
##   the generated file with added columns is saved and read directly to save compute time

access_log_location_url = 'https://raw.githubusercontent.com/baruab/baruab/refs/heads/main/DATA_698/tokenized_access_logs_global.csv'
df_ac_log = pd.read_csv(access_log_location_url)

# Drop uplicates
df_ac_log = df_ac_log.drop_duplicates()

df_ac_log.nunique()

"""# Adding Features to the data"""

df_ac_log['Dt'] = df_ac_log['Date'].str.split(' ').str[0]

# Convert the 'date' column to datetime
df_ac_log['date'] = pd.to_datetime(df_ac_log['Dt'])

# Extract the day of the week
df_ac_log['day_of_week'] = df_ac_log['date'].dt.day_name()
df_ac_log['weekday'] = df_ac_log['date'].dt.weekday

df_ac_log["AddToCart"] = df_ac_log["url"].str.contains("add_to_cart").astype(int) # str.extract("(add_to_cart)")
#df_ac_log.head()

# Reassign the IP address to IDs (make it easier later for creating edges)

ipaddrs = df_ac_log['ip'].unique()
new_ip_ids = list(range(len(df_ac_log['ip'].unique())))
map_ip = dict(zip(ipaddrs, new_ip_ids))
print(type(map_ip))

df_ac_log['ip_id'] = df_ac_log['ip'].map(map_ip)

print(len(ipaddrs))
#df_ac_log.head()

# Reassign the Product to IDs (make it easier later for creating edges)

products = df_ac_log['Product'].unique()
new_prod_ids = list(range(len(df_ac_log['Product'].unique())))
map_prod = dict(zip(products, new_prod_ids))
print(type(map_prod))

df_ac_log['Product_Id'] = df_ac_log['Product'].map(map_prod)

#### Category
# Reassign the Category to IDs

cats = df_ac_log['Category'].unique()
new_cat_ids = list(range(len(df_ac_log['Category'].unique())))
map_cat = dict(zip(cats, new_cat_ids))

df_ac_log['Category_Id'] = df_ac_log['Category'].map(map_cat)

# Reassign the Dept to IDs

depts = df_ac_log['Department'].unique()
new_dept_ids = list(range(len(df_ac_log['Department'].unique())))
map_dept = dict(zip(depts, new_dept_ids))

df_ac_log['Department_Id'] = df_ac_log['Department'].map(map_dept)
df_ac_log.head()

#mapping = {index: i for i, index in enumerate(df_ac_cat_subset.index.unique())}

"""**Split the date time string into date & time represented as numbers**"""

df_ac_log['date_id'] = df_ac_log['Date'].str.split('/').str[1]
df_ac_log['month_id'] = df_ac_log['Date'].str.split('/').str[0]
df_ac_log['year_id'] = df_ac_log['Date'].str.split('/').str[2].str.split(' ').str[0]

df_ac_log['time'] = df_ac_log['Date'].str.split(' ').str[1]


df_ac_log['Dt'] = df_ac_log['Date'].str.split(' ').str[0]

# Convert the 'date' column to datetime
df_ac_log['date'] = pd.to_datetime(df_ac_log['Dt'])

# Extract the day of the week
df_ac_log['day_of_week'] = df_ac_log['date'].dt.day_name()
df_ac_log['weekday'] = df_ac_log['date'].dt.weekday

# Convert 'numbers' column to integer
df_ac_log['date_id'] = df_ac_log['date_id'].astype(int)
df_ac_log['month_id'] = df_ac_log['month_id'].astype(int)
df_ac_log['year_id'] = df_ac_log['year_id'].astype(int)

access_count = df_ac_log.groupby("ip_id")["date_id"].count().rename("access_count")
print(access_count)

"""Create Encoders"""

!pip install sentence-transformers

#install torch
!pip install torch

import torch
from sentence_transformers import SentenceTransformer

class SequenceEncoder:
     def __init__(self, model_name='all-MiniLM-L6-v2', device=None):
         self.device = device
         self.model = SentenceTransformer(model_name, device=device)

     @torch.no_grad()
     def __call__(self, df):
         x = self.model.encode(df.values, show_progress_bar=True,
                               convert_to_tensor=True, device=self.device)
         return x.cpu()

df_ac_log.head()

encoder = SequenceEncoder() # Create an instance of the SequenceEncoder class

# subset the dataframe
df_cat = df_ac_log[['Category']]
df_cat = df_cat.drop_duplicates()
print(df_cat.shape)


category_xs = [encoder(df_cat['Category'])] # Encode the 'Category' column
category_x = torch.cat(category_xs, dim=-1) # Concatenate the encoded results
print(category_x.shape)

# subset the dataframe
df_prod = df_ac_log[['Product']]
df_prod = df_prod.drop_duplicates()
print(df_prod.shape)

product_xs = [encoder(df_prod['Product'])] # Encode the 'Category' column
product_x = torch.cat(product_xs, dim=-1) # Concatenate the encoded results
print(product_x.shape)
print(product_x)

print(category_x)
print(category_x.shape)

"""**Alternate Label encoder**"""

from sklearn.preprocessing import LabelEncoder

# Label encode the 'Category' column
label_encoder = LabelEncoder()

# get unique category names
category_labels = label_encoder.fit_transform(df_ac_log['Category'].unique())
#print(category_labels)
print(len(category_labels))
category_labels_tensor = torch.tensor(category_labels, dtype=torch.long).view(-1, 1)
print(category_labels_tensor)

"""# Define the node features"""

#Let's create the User Node, create a subset dataframe.
# Add user city, access count and buy intent count

access_count = df_ac_log.groupby("ip_id")["ip_id"].count().rename("access_count")
buy_count = df_ac_log[df_ac_log["AddToCart"] == 1].groupby("ip_id")["AddToCart"].count().rename("buy_count")
user_node_features = pd.concat([access_count, buy_count], axis=1)

# Remap user ID
user_node_features = user_node_features.reset_index(drop=False)
user_node_features.head()
user_id_mapping = user_node_features['ip_id']

# Only keep user features
#user_node_features = user_node_features.drop('ip_id', axis=1)
user_node_features.head()


df_ip_city = df_ac_log[['ip_id', 'City', 'State', 'Country']]
df_ip_city = df_ip_city.drop_duplicates()
df_ip_city.head()

# merge user_node_features and df_ip_city by ip_id
df_user_features = pd.merge(user_node_features, df_ip_city, on='ip_id')
df_user_features = df_user_features.drop_duplicates()

# Only keep user features
#df_user_features = df_user_features.drop('ip_id', axis=1)
#df_user_features.head()
print(df_user_features.shape)

# torch cat the features
#torch access_count
access_cnt = df_user_features['access_count']
access_cnt = torch.tensor(access_cnt, dtype=torch.float).view(-1, 1)
print(access_cnt.shape)

#torch city
df_user_features['City'] = df_user_features['City'].astype(str) # Convert cities to a list of strings
df_user_features['State'] = df_user_features['State'].astype(str) # Convert cities to a list of strings

# torch cities
city_xs =  [encoder(df_user_features['City'])]
city_x = torch.cat(city_xs, dim=-1) # Concatenate the encoded results
print(city_x.shape)

# torch states
state_xs =  [encoder(df_user_features['State'])]
state_x = torch.cat(state_xs, dim=-1) # Concatenate the encoded results
print(state_x.shape)

user_features = torch.cat([city_x, state_x], dim=1)
print(user_features.shape)

df_user_features.head()

"""Create Product features"""

# get unique department names

df_product_depts = df_ac_log[['Product_Id', 'Department_Id']]
df_product_depts = df_product_depts.drop_duplicates()
df_product_depts.head()

# convert x into tensor
product_features = torch.tensor(df_product_depts['Product_Id'], dtype=torch.float).view(-1,1)
print(product_features.shape)

#dept_labels = label_encoder.fit_transform(df_product_depts['Department_Id'])

dept_labels = df_product_depts['Department_Id']
#print(dept_labels)
#print(len(dept_labels))

dept_labels_tensor = torch.tensor(dept_labels, dtype=torch.float).view(-1, 1)
product_y = dept_labels_tensor

#print(product_y)
print(product_y.shape)

!pip install torch-geometric torch-sparse torch-scatter

# Import the necessary library
from torch_geometric.data import HeteroData

user_ids = torch.tensor(df_user_features['ip_id'].values, dtype=torch.long)
user_y = torch.tensor(df_user_features['buy_count'].values, dtype=torch.long).view(-1,1)

hdata = HeteroData()
hdata['user'].num_nodes = len(user_node_features)
hdata['user'].x = city_x
hdata['user'].ids = user_ids
hdata['user'].y = user_y   # buy_count

product_ids = torch.tensor(df_product_depts['Product_Id'].values, dtype=torch.long)
hdata['product'].num_nodes = len(df_product_depts)
hdata['product'].x = product_x
hdata['product'].ids = product_ids
hdata['product'].y = product_y   # dept label

print(hdata)

import torch # Import the torch module
from datetime import datetime # Import the datetime module from the datetime library

# Seperate by buy vs view using AddToCart flag
buy_edge_index=[]
buy_timestamp=[]

#Iterate the dataframe
for index, row in df_ac_log.iterrows():

  timestamp = datetime.strptime(row["Date"],'%m/%d/%Y %H:%M')
  ts_unix = int(timestamp.timestamp())
  buy_edge_index.append([row["ip_id"], row["Product_Id"]])
  buy_timestamp.append(ts_unix)

# Convert to tensor and add to HeteroData
buy_edge_index = torch.tensor(buy_edge_index, dtype=torch.long).t().contiguous()
buy_timestamp = torch.tensor(buy_timestamp, dtype=torch.long).view(-1, 1)
hdata['user', 'buy', 'product'].edge_index = buy_edge_index
### add edge_type attribute
hdata['user', 'buy', 'product'].edge_type = 'buy'
hdata['user', 'buy', 'product'].edge_attr = buy_timestamp

# torch AddToCart from df_ac_log
add_to_cart_labels = torch.tensor(df_ac_log['AddToCart'].values, dtype=torch.long)
hdata['user', 'buy', 'product'].edge_label = add_to_cart_labels


# Add reverse relation edge index for 'product' -> 'user'
hdata['product', 'rev_buy', 'user'].edge_index = hdata['user', 'buy', 'product'].edge_index.flip(0)

# Optionally, add edge attributes for the reverse relation
# If `edge_attr` is present for 'buys' edges, you can reuse it for 'rev_buys'
if 'edge_attr' in hdata['user', 'buy', 'product']:
    hdata['product', 'rev_buy', 'user'].edge_attr = hdata['user', 'buy', 'product'].edge_attr

print(hdata)
print(buy_edge_index)
print(buy_edge_index.shape)

"""# GNN model with 2 layers of SAGEConv using to_hetero function"""

import torch_geometric.transforms as T
from torch_geometric.nn import SAGEConv, to_hetero

hdata.num_classes = 2

class GNN(torch.nn.Module):
    def __init__(self, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = SAGEConv((-1, -1), hidden_channels)
        self.conv2 = SAGEConv((-1, -1), out_channels)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index).relu()
        x = self.conv2(x, edge_index)
        return x


model = GNN(hidden_channels=64, out_channels=hdata.num_classes)
model = to_hetero(model, hdata.metadata(), aggr='sum')

#next training
# Initialize model
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
# Define the loss function
criterion = torch.nn.CrossEntropyLoss()

# Forward pass
model.train()

# Ensure edge indices in edge_index_dict are of type torch.long
for edge_type in hdata.edge_index_dict:
    hdata.edge_index_dict[edge_type] = hdata.edge_index_dict[edge_type].type(torch.long)

out = model(hdata.x_dict, hdata.edge_index_dict)

#print("Final output:", out)
# print user node embedding
print("User node embeddings:", out[('user')])

# print product node embedding
print("Product node embeddings:", out[('product')])

#print("User node embeddings:" , out[0])

"""Move the model to appropriate device"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
hdata = hdata.to(device)

"""Calculate the edge embedding"""

import torch

# Suppose `out` contains node embeddings after the forward pass
# out['user'] and out['product'] have the embeddings for 'user' and 'product' nodes

# Get edge index for the edge type you are interested in
edge_index = hdata.edge_index_dict[("user", "buy", "product")].to(device)


# Move edge_index to the same device as out['user'] (likely CPU)
edge_index = edge_index.to(out['user'].device)

# Get the source (user) and target (product) node embeddings for each edge
user_embeddings = out['user'][edge_index[0]]  # Source node embeddings (user)
product_embeddings = out['product'][edge_index[1]]  # Target node embeddings (product)

# Define edge embeddings, here using concatenation as an example
edge_embeddings = torch.cat([user_embeddings, product_embeddings], dim=1)

# Now `edge_embeddings` contains an embedding for each ("user", "buy", "product") edge
print("Edge embeddings shape:", edge_embeddings.shape)

"""Define Training and Evaluation Functions"""

# Add training loop
def train():
    model.train()
    optimizer.zero_grad()
    out = model(hdata.x_dict, hdata.edge_index_dict)

    # classifying 'buy' edges
    # Extract 'user' -> 'buy' -> 'product' predictions
    # Ensure edge_index is on the same device as the model
    edge_index = hdata.edge_index_dict[("user", "buy", "product")].to(device)

    # Get the source (user) and target (product) node embeddings for each edge
    user_embeddings = out['user'][edge_index[0]]  # Source node embeddings (user)
    product_embeddings = out['product'][edge_index[1]]  # Target node embeddings (product)

    # Define edge embeddings, here using concatenation as an example
    buy_out = torch.cat([user_embeddings, product_embeddings], dim=1)

    buy_labels = hdata['user', 'buy', 'product'].edge_label  # Shape: [num_buys]

    # Move buy_labels to the same device as buy_out
    buy_labels = buy_labels.to(device)
    loss = criterion(buy_out, buy_labels)
    loss.backward()
    optimizer.step()
    return loss.item()

def evaluate():
    model.eval()
    with torch.no_grad():
        out = model(hdata.x_dict, hdata.edge_index_dict)
        # Ensure edge_index is on the same device as the model
        edge_index = hdata.edge_index_dict[("user", "buy", "product")].to(device)

        # Get the source (user) and target (product) node embeddings for each edge
        user_embeddings = out['user'][edge_index[0]]  # Source node embeddings (user)
        product_embeddings = out['product'][edge_index[1]]  # Target node embeddings (product)

        # Define edge embeddings, here using concatenation as an example
        buy_out = torch.cat([user_embeddings, product_embeddings], dim=1)

        buy_labels = hdata['user', 'buy', 'product'].edge_label  # Shape: [num_buys]

        # Move buy_labels to the same device as buy_out
        buy_labels = buy_labels.to(device)


        # Compute accuracy
        pred = buy_out.argmax(dim=1)
        correct = (pred == buy_labels).sum().item()
        acc = correct / buy_labels.size(0)
        return acc

# Training loop
num_epochs = 50
for epoch in range(1, num_epochs + 1):
    loss = train()
    if epoch % 5 == 0 or epoch == 1:
        acc = evaluate()
        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Accuracy: {acc:.4f}')

"""# Create Train/Validation/Test Splits"""

from sklearn.model_selection import train_test_split

# Split edge indices and labels
edge_indices = hdata['user', 'buy', 'product'].edge_index
labels = hdata['user', 'buy', 'product'].edge_label

# Split indices
train_idx, test_idx = train_test_split(range(edge_indices.size(1)), test_size=0.2, random_state=42)
train_idx, val_idx = train_test_split(train_idx, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2

# Create masks
train_mask = torch.zeros(edge_indices.size(1), dtype=torch.bool)
val_mask = torch.zeros(edge_indices.size(1), dtype=torch.bool)
test_mask = torch.zeros(edge_indices.size(1), dtype=torch.bool)

train_mask[train_idx] = True
val_mask[val_idx] = True
test_mask[test_idx] = True

# Assign masks and split labels
hdata['user', 'buy', 'product'].train_mask = train_mask
hdata['user', 'buy', 'product'].val_mask = val_mask
hdata['user', 'buy', 'product'].test_mask = test_mask

"""Modify the training using the masks"""

def train():
    model.train()
    optimizer.zero_grad()
    out = model(hdata.x_dict, hdata.edge_index_dict)

    # classifying 'buy' edges
    # Extract 'user' -> 'buy' -> 'product' predictions
    # Ensure edge_index is on the same device as the model
    edge_index = hdata.edge_index_dict[("user", "buy", "product")].to(device)

    # Get the source (user) and target (product) node embeddings for each edge
    user_embeddings = out['user'][edge_index[0]]  # Source node embeddings (user)
    product_embeddings = out['product'][edge_index[1]]  # Target node embeddings (product)

    # Define edge embeddings, here using concatenation as an example
    buy_out = torch.cat([user_embeddings, product_embeddings], dim=1)

    buy_labels = hdata['user', 'buy', 'product'].edge_label  # Shape: [num_buys]
    # Move buy_labels to the same device as buy_out
    buy_labels = buy_labels.to(device)


    # Use train_mask
    train_mask = hdata['user', 'buy', 'product'].train_mask
    loss = criterion(buy_out[train_mask], buy_labels[train_mask])
    loss.backward()
    optimizer.step()
    return loss.item()

def evaluate(mask):
    model.eval()
    with torch.no_grad():
        out = model(hdata.x_dict, hdata.edge_index_dict)

        # classifying 'buy' edges
        # Extract 'user' -> 'buy' -> 'product' predictions
        # Ensure edge_index is on the same device as the model
        edge_index = hdata.edge_index_dict[("user", "buy", "product")].to(device)

        # Get the source (user) and target (product) node embeddings for each edge
        user_embeddings = out['user'][edge_index[0]]  # Source node embeddings (user)
        product_embeddings = out['product'][edge_index[1]]  # Target node embeddings (product)

        # Define edge embeddings, here using concatenation as an example
        buy_out = torch.cat([user_embeddings, product_embeddings], dim=1)

        buy_labels = hdata['user', 'buy', 'product'].edge_label  # Shape: [num_buys]
        # Move buy_labels to the same device as buy_out
        buy_labels = buy_labels.to(device)

        masked_out = buy_out[mask]
        masked_labels = buy_labels[mask]

        pred = masked_out.argmax(dim=1)
        correct = (pred == masked_labels).sum().item()
        acc = correct / masked_labels.size(0)
        return acc

# Training loop with validation
num_epochs = 50
for epoch in range(1, num_epochs + 1):
    loss = train()
    if epoch % 5 == 0 or epoch == 1:
        val_acc = evaluate(hdata['user', 'buy', 'product'].val_mask)
        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Accuracy: {val_acc:.4f}')

"""Implementation of GAT Layer with PyTorch"""

import torch
import torch.nn as nn
from torch_geometric.nn import GATConv

class GATModel(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, num_heads):
        super(GATModel, self).__init__()
        # Define GAT layers
        self.conv1 = GATConv(in_channels, hidden_channels, heads=num_heads, concat=True)
        self.conv2 = GATConv(hidden_channels * num_heads, out_channels, heads=1, concat=False)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index).relu()
        x = self.conv2(x, edge_index)
        return x

# model initialization
num_input_features = hdata.num_features['user']
#print(num_input_features)

in_channels =   num_input_features  # Number of input features
hidden_channels = 32
out_channels = 2  # Number of classes for classification
# number of heads
num_heads = 4

model = GATModel(in_channels, hidden_channels, out_channels, num_heads)

"""Implementation of HGT Layer with PyTorch"""

# tensor dims seperate

print(hdata['user'].x.size(dim=1))
print(hdata['product'].x.size(dim=1))

import torch
import torch.nn as nn
from torch_geometric.nn import HGTConv

class HGTModel(nn.Module):
    def __init__(self, in_channels_dict, hidden_channels, out_channels, num_heads, num_layers, metadata):
        super(HGTModel, self).__init__()
        self.convs = nn.ModuleList()

        # Define multiple HGT layers
        for _ in range(num_layers):
            self.convs.append(HGTConv(
                hidden_channels, hidden_channels,
                metadata=metadata,
                heads=num_heads
            ))

        # Fully connected output layer
        self.fc = nn.Linear(hidden_channels, out_channels)

    def forward(self, x_dict, edge_index_dict):
        # Forward pass through each HGT layer
        for conv in self.convs:
            x_dict = conv(x_dict, edge_index_dict)

        # Output layer for user nodes (or product nodes, if applicable)
        return self.fc(x_dict['user'])

# Example model initialization

num_user_features = hdata['user'].x.size(dim=1)
num_product_features =hdata['product'].x.size(dim=1)

in_channels_dict = {'user': num_user_features, 'product': num_product_features}  # Input feature sizes per node type
hidden_channels = 32
out_channels = 2
num_heads = 4
num_layers = 2
metadata = hdata.metadata()  # Metadata from your HeteroData object

model = HGTModel(in_channels_dict, hidden_channels, out_channels, num_heads, num_layers, metadata)

"""Implement HAN layer"""

import torch
import torch.nn as nn
from torch_geometric.nn import HANConv

class HANModel(nn.Module):
    def __init__(self, in_channels_dict, hidden_channels, out_channels, metadata, num_heads=8):
        super(HANModel, self).__init__()
        # HAN layer with meta-paths
        self.han_conv = HANConv(
            in_channels_dict, hidden_channels, heads=num_heads,
            metadata=metadata, dropout=0.5
        )
        # Output layer
        self.fc = nn.Linear(hidden_channels, out_channels)

    def forward(self, x_dict, edge_index_dict):
        # Pass through HAN layer
        x_dict = self.han_conv(x_dict, edge_index_dict)
        x = x_dict['user']  # Focusing on user nodes if predicting user-related labels
        return self.fc(x)

# model initialization
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')  # Use GPU if available

# Reduce hidden_channels and num_heads to reduce model size
hidden_channels = 16
num_heads = 4

model = HANModel(in_channels_dict, hidden_channels, out_channels, metadata, num_heads).to(device) # Move model to device

# Initialize optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = nn.CrossEntropyLoss()  # For classification tasks

num_epochs = 5
# Training loop
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()

    # Move data to the same device as the model
    out = model(hdata.x_dict, hdata.edge_index_dict)  # Forward pass

    #print(f"hdata['user'].y.view(-1) size : {hdata['user'].y.view(-1).size(dim=0)}")
    target = hdata['user'].y.view(-1)
    # target = target - target.min() # Shift labels to start from 0 if needed
    # *** This is useful is your labels are in the range of [min_val, max_val],
    # *** where min_val is not 0, but you have out_channels total number of classes.
    target = torch.clamp(target, 0, out_channels - 1) # Clamp to the expected range
    target = target.type(torch.LongTensor).to(device)

    # **Check unique values in target and compare to num_classes:**
    unique_targets = torch.unique(target)

    loss = criterion(out, target)  # Assuming labels are for 'user' nodes
    loss.backward()
    optimizer.step()

    print(f'out (embedding)  , {out[edge_index[0]]}')
    print(f'Epoch {epoch}, Loss: {loss.item()}')

def train():
    model.train()
    optimizer.zero_grad()
    out = model(hdata.x_dict, hdata.edge_index_dict)

    # classifying 'buy' edges
    # Extract 'user' -> 'buy' -> 'product' predictions
    # Ensure edge_index is on the same device as the model
    edge_index = hdata.edge_index_dict[("user", "buy", "product")].to(device)

    # Define edge embeddings, here using concatenation as an example
    buy_out = out[edge_index[0]]

    buy_labels = hdata['user', 'buy', 'product'].edge_label  # Shape: [num_buys]
    # Move buy_labels to the same device as buy_out
    buy_labels = buy_labels.to(device)


    # Use train_mask
    train_mask = hdata['user', 'buy', 'product'].train_mask
    loss = criterion(buy_out[train_mask], buy_labels[train_mask])
    loss.backward()
    optimizer.step()
    return loss.item()

def evaluate(mask):
    model.eval()
    with torch.no_grad():
        out = model(hdata.x_dict, hdata.edge_index_dict)

        # classifying 'buy' edges
        # Extract 'user' -> 'buy' -> 'product' predictions
        # Ensure edge_index is on the same device as the model
        edge_index = hdata.edge_index_dict[("user", "buy", "product")].to(device)


        # Define edge embeddings, here using concatenation as an example
        buy_out = out[edge_index[0]]

        buy_labels = hdata['user', 'buy', 'product'].edge_label  # Shape: [num_buys]
        # Move buy_labels to the same device as buy_out
        buy_labels = buy_labels.to(device)

        masked_out = buy_out[mask]
        masked_labels = buy_labels[mask]

        pred = masked_out.argmax(dim=1)
        correct = (pred == masked_labels).sum().item()
        acc = correct / masked_labels.size(0)
        return acc

# Training loop with validation
num_epochs = 50
for epoch in range(1, num_epochs + 1):
    loss = train()
    if epoch % 5 == 0 or epoch == 1:
        val_acc = evaluate(hdata['user', 'buy', 'product'].val_mask)
        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Accuracy: {val_acc:.4f}')